% DATASETS

% d9f18e2 - DATASET FOR UNBOUNDED
% 2860d6fe - DATASET FOR HIGHSTART
% 53f6b74 - four levels for p_* - needed to examine lower bounds

<<setup, include=FALSE>>=
library(knitr)
library(cowplot) # styling of plots, extension of ggplot2
library(gridExtra) # grid layouts for ggplot2
library(lattice) # needed for bwplot etc
library(english) # convert numbers to words
opts_chunk$set(fig.path='generated_figures/')
knit_hooks$set(pdfcrop = hook_pdfcrop)

load.results.simple <- function(t) {
	colClasses <- c("numeric","numeric","integer","integer","numeric","numeric","factor","factor","factor","factor","factor","factor","factor","factor")
	read.csv(t, colClasses=colClasses)
}

load.results <- function(t) {
	colClasses <- c("integer","integer","numeric","numeric","integer","integer","numeric","numeric","factor","factor","factor","factor","factor","factor","factor","factor")
	df <- read.csv(t, colClasses=colClasses)
	df$ecf <- factor(df$ecf,levels=c(0,1,5,10)) # place into natural order
	df
}

load.toyworld <- function(f){
	colClasses <- c("NULL", "NULL", "factor","NULL","factor","factor", "factor","factor","integer","integer","integer")
	df <- read.csv(f,colClasses=colClasses)
	df$Partition.Start <- factor(df$Partition.Start, levels = c("4750", "9750", "14750", "19750"))
	df
}

myqqplot <- function(x,y,...) {
sx <- sort(x)
sy <- sort(y)
lenx <- length(sx)
leny <- length(sy)
if (leny < lenx)
 sx <- approx(1L:lenx, sx, n = leny)$y
if (leny > lenx)
 sy <- approx(1L:leny, sy, n = lenx)$y
qplot(sx,sy,...)
}
@

% # Filter(Negate(is.factor),z[[gen]]) # quantile values for each combination of factor levels at gen
% # levels <- unique(interaction(Filter(is.factor,y))) # all unique combinations of factor levels in y

\chapter{Introduction}\label{introduction}

\section{TODO-Motivation}\label{motivation}

\settowidth{\epigraphwidth}{Wonderful life : the Burgess Shale and the nature of history}
\epigraph{%
	Without hesitation or ambiguity, and fully mindful of such paleontological wonders as large dinosaurs andAfrican ape-men, I state that the invertebrates of the Burgess Shale, found high in the Canadian Rockies in YohoNational Park, on the eastern border of British Columbia, are the world's most important animal fossils. Modern multicellular animals make their first uncontested appearance in the fossil record some 570 million years ago--and with a bang, not a protracted crescendo.}%
{\textit{\\Wonderful life : the Burgess Shale and the nature of history}\\\textsc{Stephen Jay Gould}}

Motivation is to understand the onset of evolution; how robust, interesting, adaptive evolution can be started and undergo self-improvement in the non-living world.

Our belief is that the evolution of evolution can and will emerge under certain conditions, and that a bottoms-up approach emergent approach will be more successful than the predominant design-driven top-down approach today.

In the Spencerian sense of progressive improvement, rather than
necessarily any particular mechanism (e.g., evolution by natural
selection)

Simply a change of population distribution over time--but if just
purely distributive (sensu \autocite{Bourrat2015}), rather than transformative, not
very interesting

Abstract idea, different from mechanisms such as ENS (grounded perhaps
inseparably in biology) e.g., ``Darwin's theory of evolution by natural
selection is restricted in scope. One sense in which it is restricted is
that it refers to organisms.'' \autocite{Griesemer2005}

ENS as ``a theory of descent with modification''

Life (see later) provides the canonical example of ``interesting''; approaching life's creativity in an artificial system would be quite something.  Hope to see robustnes,  evolvability, novelty.

Alternatives to ``evolution''? Broad definition\ldots{}:

\begin{itemize}
\item Lower bound of random search--not good enough
\item No upper bound in practice--but ENS is the gold standard
\item Good result would be `interesting' forms
\end{itemize}

Natural selection as mechanism for adaptation (in biological sense). Assume that adaptation in this sense is useful concept for CS

Simple ongoing evolution is not the same as creative or interesting evolution; current state-of-the-art insufficient and unsatisfying.

We show that under further conditions interesting is possible, perhaps inevitable.

\begin{itemize}
\item
	Not same as ENS--that explains natural processes; our goal is to achieve something that is as interesting in a different domain
\item 
	Not same as EAs--EAs have exogenous fitness, not open-ended (search through a fixed space, cannot surprise (e.g., \autocite{Nellis2014})
\item  
	Not same as approach where take ideas from biology and evaluate for different purpose (e.g., in EAs, island populations, GRNs (e.g., L-systems), Lamarkian learning, co-evolution, evolutionary transitions (cooperation and mediation in \autocite{Defaweux:2005fk}\ldots{}). All seem to follow model of currently we use these tools, biology has something we don't have, let's try it\ldots{}
\item
	Not the same as most type of Alife, where evolution is not the subject of the research but rather a tool or mechanism towards some other goal (canonically, creating artificial life.)
\end{itemize}

\subsection{Why important?}\label{why-important}

\begin{itemize}
\item Application to engineering -- alternative to EC approaches -- self-optimizing mechanism -- EvoEvo for EAs
\item In itself--one of grand challenges of Alife \autocite{Bedau:2000mi}
\item Possibly some application to other related fields
\end{itemize}

\subsection{Life, the Gold Standard}

\subsubsection{Living organisms are supremely well suited to their environments, and can adapt to environmental changes}
\label{living-organisms-are-supremely-well-suited-to-their-environments-and-can-adapt-to-environmental-changes}

Adaptation of organisms to their environments occurs in the main on two
different time-scales.

Evolution by \gls{naturalselection} acts over a period of generations on
populations of individual organisms. Changes are therefore relatively
gradual, and many generations can pass before a change such as a
beneficial mutation becomes ubiquitous in a population (\eg 300-500
generations for targeted modifications in lactose processing in
\emph{E. coli} \autocite{Dekel:2005fk}. In contrast, gene regulatory
effects act during the life cycle of a single individual, either during
development to affect morphology, or during the adult lifespan in
reaction to seasonal or other environmental cues. These
regulation-driven changes are not in themselves heritable, but they can
be assimilated back into the population by influencing the organisms
fitness under natural selection (\eg,
\autocite{Baldwin:1896ly,Dennett:2003ve,Paenke:2009xe,Paenke:2007ve}).

Similar effects can be seen by another adaptive mechanism that operates
on individuals during their lifespan: learning, where behavioural
adaptions can also lead to genetic change
(\eg \autocite{Hinton:1987vy}.)

\subsubsection{Natural selection, acting on populations, is the primary driver for long-term adaptation}
\label{natural-selection-acting-on-populations-is-the-primary-driver-for-long-term-adaptation}

The year 2009 saw the celebration of the 150\textsuperscript{th}
anniversary of the publication of \emph{On the Origin of Species}, the
explanation of evolution by natural selection, with extensive coverage
in scientific and popular media. The terms are therefore fairly well
known to many people, but what exactly does \emph{natural selection}
mean? To quote from \autocite{Futuyama:1979tg}, \gls{naturalselection}
is the ``differential survival and reproduction of genotypes''.

Let's examine each of the components in this idea in turn:

Differential survival. Living organisms are constantly engaged in an
intimate relationship with their environments. Indeed, according to the
theory of \emph{autopoiesis} \autocite{Varela:1974qd}, organisms are
defined by this engagement: to be alive means maintaining oneself
against the surrounding environment. In general the more effectively the
organism is able to do this, the more likely it is to survive. However,
in practice survival for an individual may be affected by random events.
Scholarship winning students can be killed by drunk drivers. Sardines in
shoals flash and turn, yet sharks still manage to grab one or two from
the shoal effectively at random. Averaged over a population however
these chance events balance out; a succession of random trials leads to
a skewed distribution of fitness away from the less able.

\footnote{This introduces two significant differences between \glspl{ea}
	and biology: first, \glspl{ea} conduct a series of discrete trials of fitness,
	rather than a continuous evaluation. Second, fitness in an \gls{ea} is
	measured by an explicit \emph{objective function} whereas in biology
	fitness \emph{emerges dynamically} through continuous interaction with
	the environment.}

Reproduction. In this sense, reproduction simply means inheritance. Only
those characteristics that can be passed on from one generation to the
next are relevant. One implication of this is that the only traits of an
organism that matter to natural selection are those that are apparent
while the organism can reproduce. Altruism and kin-selection, where an
individual acts to increase the fitness of a related individual, are
interesting for the light they shed on this implication.

Genotypes. An organism's \gls{genotype} is the heritable information
that defines an individual, of which the great majority is encoded in
DNA (some \emph{epigenetic} information is inherited through
DNA-methylation, maternal protein concentrations and other mechanisms.
However, generally DNA remains the primary source.)

How then does natural selection unfold in practice? Although an organism
is defined by its genotype, its survival is based not on the raw
genotype, but on the expression of the genotype--called the
\gls{phenotype}--that participates in the interaction with the
environment. There is not necessarily a direct one-to-one mapping
between genotype and phenotype; for example, environmental triggers
during development can switch the phenotype in different directions (a
phenomenon called \gls{polyphenism}.)

This indirect mapping enables a number of important mechanisms
significant to the operation of natural selection: first, changes in the
genotype (caused by mutations for example) may build up independent of
phenotype changes--the idea of \emph{neutral mutations}
\autocite{Ohta:1996vn,Ohta:2002ys,Ohta:1973kx}. Examples from studies
of RNA secondary structures (the physical folding of RNA molecules) show
that many closely-related RNA sequences can produce the same RNA folding
\autocite{Fontana:1993zn}. Adjacent changes often have little effect on
structure.

Second, by extension, \autocite{Gavrilets:1997qt} and
\autocite{Gravner:2007yd} have shown that in cases involving many gene
loci under well-defined conditions there is a path between viable
phenotypes that requires only neutral mutations.

Third, behaviours such as learning, rather than purely genetic
mechanisms, can influence the form of this \gls{gpmap} in combination
with natural selection, as illustrated by the Baldwin effect
\autocite{Baldwin:1896ly} and other examples of genetic assimilation
{[}\autocite{Hinton:1987vy};Siegal:2002qn;Waddington:1942jb{]}.

Finally, \glspl{grn} provide another mechanism to modify the \gls{gpmap}
and hence to guide natural selection.

\subsubsection{Novelties often arise from new regulatory connections rather than changes to genes}
\label{novelties-often-arise-from-new-regulatory-connections-rather-than-changes-to-genes}

\autocite{Prudhomme:2007ax} believe that evolutionary novelties more
commonly arise from changes or additions of regulatory
\emph{connections} than from the development of \emph{new} genes or
regulatory elements; that is, from changes to the network topology
rather than from additions to the network elements. The underlying
implication is that novelties are therefore new compositions of
pre-existing elements, rather than being constructed `\emph{de novo}',
and that production of novelties may be relatively rapid. Connection
changes may happen quickly; by comparison, new genes may take many generations.

\section{Relationships with other fields}\label{connections-to-other-fields}

\subsection{Origins of life}
Similar problem--from chemistry to biology. Transition to biology.

Systems Chemistry is tightly related field.
  
Analogous to origin of life, but despite parallels, must resist temptation to extend claims to this--the evolution of life was contingent, and because of lack of evidence from early stages, no way anyway to test or confirm.
  
Work to describe a pathway from plausible conditions on the early abiotic Earth to the first living things. Important to understand that this pathway only describes one possible form of life, and one possible mechanism -- restricted in scope to connecting the two endpoints.
  
The primary postulates of OOL are not our postulates. We do though have concepts and principles in common - meta-ENS, or the workings of evolutionary processes, of which the mechanisms of OOL are one concrete example (a form of existence proof.)
  
\begin{itemize}
	\item
	hypercycles \autocite{Eigen1971}
	\item
	autocatalysis
	\begin{itemize}
		\item
		present in all three elements (\autocite{Ganti:2003hl}--or
		earlier?) of life
		\item
		DNA replicates with enzymatic help
		\item
		some metabolites, such as ATP, exclusively autocatalytic
		\item
		lipids in a membrane enhance addition of other lipids--Formation of
		autocatalytic cycles/sets (\autocite{Hordijk2004})--percolation increases
		likelihood
		\item
		\autocite{Sousa2015} for detection of RAF sets in e coli metabolic
		networks
		\item
		Selection amongst autocatalytic networks (Ganti and Wachtershauser referenced in \autocite{Fernando:2005ly})
		\item
		\autocite{Fernando:2007pf}--selection/liposomes/chemical avalanches
		based on Wachtershauser
		\item
		autocatalytic cores (\autocite{Vasas2012})
		\item
		Kauffman's original Reflexively Autocatalytic Polymer Networks
		(RAPN) \autocite{Kauffman1986,Farmer1986} are not capable of
		non-digital evolution. RAPN stabilise into single network without
		variation
	\end{itemize}
	\item
	GARD (fixed catalysts) \autocite{Segre1998} not capable of evolution -
	lack heredity of variation (mutations overwhelm heredity) \autocite{Vasas2010}
	\item
	Bimolecular rearrangements (\autocite{Fernando:2008xy,Fernando:2007pf})?
	\item
	template replicators--highly unlikely without intermediate steps
	\item
	Must be driven far-from-equilibrium (many references e.g.,
	\autocite{Pascal2015}--continuous supply of energy required (explicit
	modelling unlike say \autocite{Fontana1994} where energy only
	implicitly modelled)--and maintained there (by metabolism--e.g., how
	\quote{living matter evades the decay to equilibrium}{\autocite{Schrodinger1944}})
	\item
	Eigen threshold for replicators--high mutation rates overwhelm
	heredity
	\item
	Biological evolutionary theory
	\item
	Extension beyond biology
	
	\begin{itemize}
		\item
		Previous work on extending evolution--\autocite{Bourrat2015} etc
		\item
		Evolution of culture, language, technology
	\end{itemize}
	\item
	OOL is focussed on plausible explanations for life-as-we-know-it
	\item
	And must be constrained by biological givens, i.e.
	\item
	Starting point consistent with what is known about archaic Earth
	\item
	Must lead to end-point consistent with earliest known life
	\item
	In a reasonable time period
	\item
	single-step astronomically unlikely (single RNA strand probability
	about 10E-60, based on 100 monomers--\autocite{Pascal2013})
	\item
	Requires a series of steps--akin to OOL where single-step
	astronomically unlikely (single RNA strand probability about 10E-60,
	based on 100 monomers--\autocite{Pascal2013})
	\item
	Contingent and specific--constraints
	\item
	starting point compatible with what is known of prebiotic conditions
	(either on earth or extraterrestrially)
	\item
	end point of ENS at something that might be LUCA
	\item
	Must be simplified and abstracted
	\item
	Choosing an artificial chemistry similar to natural chemistry enables
	an argument by analogy
	\item
	Meets known constraints for OEE--and we have OOL as an example of OEE
	from natural chemistry
	\item
	Catalysis/autocatalysis possible through emergence in some AChems
	(\eg \autocite{Virgo2013})
	\item
	Some fundamental differences remain however between our domain and the
	natural domain
	\item
	The sheer size of the Biosphere means we can never duplicate the
	number of individual evolutionary ``trials'' (selection, mutation and
	reproduction) events in our model
	\item
	The Biosphere is underpinned by a phenomenally rich set of physical
	and chemical laws, implicitly constraining each and every action and
	interaction
\end{itemize}

\subsection{What is life?}\label{what-is-life}

Autopoesis

Self-replication

Rosen (M,R) systems

Ganti Chemoton

Autonomy

Or \quote{
	What modifications must be made to this type of
	experiment to allow at least one of the following outcomes:
	‘open-ended evolution’ (Bedau et al., 2000); the origin of
	basic autonomy, i.e. a dissipative system capable of the
	recursive generation of functional constraints (Ruiz-Mirazo
	et al., 2004); a process ultimately capable of the
	production of nucleic acids or other modular replicators
	with unlimited heredity potential (Maynard-Smith and
	Szathmary, 1995; Szathmary, 2000); identification of ‘‘the
	course of evolution by which the determinate order of
	biological metabolism developed out of the chaos of intercrossing
	reactions’’ (Oparin, 1964); the coupled cycling of
	bioelements (Morowitz, 1968, 1971); the maximization of
	entropy production by a biosphere (Kleidon, 2004); the
	minimal unit of life (Ganti, 2003a, b); or an autopoetic unit
	(Maturana and Verela, 1992)?}{\autocite{Fernando:2007pf}}

The arrow of complexity - explored in \autocite{Miconi:2008cy}:
\begin{itemize}
	\item Passive (inevitable result of non-repeating evolution from simple seed)
	\item Active--some drive towards increasing complexity
\end{itemize}

\subsection{Synthetic biology}\label{synthetic-biology}

Provides a perspective on living things - one answer to the question of what is required for something to be alive.

ADD IN NOTES

Minimal cells

\begin{itemize}
	\item
	Minimal cell created either by removing elements from simple cell to
	produce functioning minimal cell or alternatively by synthesising cell
	from bottom-up from a set of hypothesised necessary elements.
	\item
	``chemical reaction networks coupled to containers''--protocell
	definition, from Protocells:Back to the Future workshop materials
	(http://www.unamur.be/en/sci/naxys/pb2f).
	\item
	Containers useful for protection, for concentration of elements, and
	for selection, allowing benefits to accrue to originator.
	\item
	Ganti's observation that contemporary living things always have a
	metabolic subsystem, a heritable control system, and a boundary system
	to contain (in \autocite{Szathmary:2006ty}).
\end{itemize}

\subsection{Artificial Intelligence}\label{ai}
Artificial General Intelligence--one approach to AGI holds that intelligence is an evolutionary adaptation, and therefore that most
promising approach is to follow an evolutionary process
  
Early review of AL approach to AI in http://www.mitpressjournals.org/doi/pdf/10.1162/artl.1993.1.1\_2.75

Rodney Brooks/Maes--actionist approach to robotics. Similar bottoms-up approach to the one we propose.

Intelligence needs environment

Manipulating symbols that are not grounded in the environment doesn't look like it'll result in emergent intelligence

Intelligence arose from interactions with the world

Understanding of intelligence based on humans--replicating this
difficult. And yet, hard to generalize from this one sample. So rather
than trying to recreate human intelligence, let intelligence emerge from
interactions with world

\begin{itemize}
	\item
	Similar approach to emergent Alife
	\item
	The AL approach to AI
\end{itemize}

Similar problems to Alife

\begin{itemize}
	\item
	``Life'' and ``Intelligence'' are both hard to define and measure
	\item
	Both appear emergent--that is, hard to measure progress--either yes
	or no
	\item
	Best measures for both may be Turing-like--if it quacks like a
	duck\ldots{}comparison to existing life or human intelligence
\end{itemize}

\section{Previous work}\label{epistemology}

Previous work coming to consensus on conditions--e.g., rich generative mechanism, unlimited heredity, inexhaustible fitness landscape, emergence \autocite{Vasas2015}, and good genetic representation, ``sufficiently large world for every individual to be evaluated'', and a seed or starting point, (plus four specific conditions
\autocite{Soros2014})

\subsection{Thought experiments--models}
Non-domain specific
\begin{itemize}
	\item DaisyWorld (Gaia Hypothesis)--\autocite{Saunders1994}--regulation as model for evolution (selection a hindrance)
	\item MWUA--meta-model? \autocite{Chastain2014} and \autocite{Barton2014}
\end{itemize}

\subsubsection{Biological Evolution}\label{evolution}

\quote{Evolution is a process that results in heritable changes in a population spread over many generations.}{
``Sandwalk: strolling with a skeptical biochemist'',
\url{http://sandwalk.blogspot.co.nz/2012/10/what-is-evolution.html}}

\quote{
Biological evolution consists of change in the hereditary characteristics of groups of organisms over the course of generations. Groups of organisms, termed populations and species, are formed by the division of ancestral populations or species, and the descendant groups then change independently. Hence, from a long-term perspective, evolution is the descent, with modification, of different lineages from common ancestors.}{
``Evolution, Science, and Society: Evolutionary Biology and the National Research Agenda'', Working Draft, 28 September 1998, \url{http://www.zoology.ubc.ca/~otto/evolution/Evolwhite.pdf}}

Separate outcome or result from process or mechanism (e.g., adaptation)

Blurring in use of OEE

\begin{itemize}
\item Either in sense of quote{an indefinitely large number of structures are each capable of replication.}{\autocite{MaynardSmith1999}}, or
\item Additionally meaning novel, interesting, surprising
\end{itemize}

Ongoing generation of novel forms

Inevitably leads to increasing complexity as without complexity will exhaust new possibilities and cover old ground

\paragraph{Evolution by Natural Selection (ENS))}\label{ens-evolution-by-natural-selection}

ENS is an example of OEE
ENS is strongly tied to life, and mostly presupposes living subjects.

http://www.protevi.com/john/Morality/evolution4dimensions.pdf--outline
of Jablonka, summary of evolutionary ``theory'' as of 2005

\begin{itemize}
\item
Biology based around ENS e.g., \quote{god and natural selection are, after
all, the only workable theories we have of why we exist}{\autocite{Dawkins1982}}
\item
But later biology includes HGT (horizontal gene transmission) which
blurs mechanism somewhat, as does importance of neutral-theory,
without invalidating ENS
\item
Summaries and formal models attempting to either capture definition
(constitutive) or sufficiency (causal) of observed systems
\item
ENS (Evolution by Natural Selection) in biology needs variation,
fitness differences, heritability of fitness (Lewontin) or variation,
multiplication, heritability (Maynard-Smith)
\item
See \autocite{Griesemer2001} for discussion of differences
\item
Review of various formulations in Godfrey-Smith2007; all from starting
point of natural evolution -\textgreater{} summary -\textgreater{}
reconcile, address specific difficulties in particular formulations
against common problem cases
\item
Core is \quote{``combination of variation, heredity, and fitness
differences.}{\autocite{Godfrey-Smith2007}}
\item
Argument that heredity may in fact be a product of evolution rather
than a precursor \autocite{Bourrat2015}
\item
Heredity seen as method to maintain low entropy over much longer time
than possible with non-biological systems: \quote{ Living systems can stay away from maximum entropy
for much longer, indeed arbitrarily long (the biotic time scale is, for all we know, only
limited by the existence of the biosphere). It is then this ability: to persist in a state of
reduced entropy for biotic as opposed to abiotic time scales, that defines a set of molecules
as living, and this set of molecules must achieve that feat via the self-replication of
information.}{\autocite{Adami2015}}
\item
HGT (Horizontal gene transfer)
\item
Thought that biological evolution is Evolution by Natural Selection,
but this is not strictly true:
\item
\quote{We take it as given that biology
instantiates ENS {[}Evolution by Natural Selection{]}. That is, ENS
occurs in biological evolution (there is no need to reiterate the
evidence for this). However, we wish to separate the conclusion that
ENS \textit{occurs in} biological evolution from the conclusion that
the algorithm of adaptive biological evolution \textit{is} ENS}{\autocite{Watson2012}}
(emphasis in original)
\item
LGT complicates this
\item
Exaptations more common than thought, meaning that evolutionary driver
shifting towards neutrality from pure selection \autocite{Barve2013}
\item
Junk DNA/Neutral theory--not a highly selective environment
(http://sandwalk.blogspot.co.nz/2008/02/theme-genomes-junk-dna.html)
\item
Random genetic drift
\item
Problems with ENS beyond classical organisms
\item
Previous work in application of ENS to levels of biological hierarchy
(Griesemer2005)
\item
issue in identifying ``individuals'' or ``entities'' in ENS
formulations--part of the ``unit of selection'' problem
\item
issue in meaning of heredity without genotypes--needs a causal
formulation for pre-cellular evolution akin to that in
\autocite{Bourrat2015} or presented in \autocite{Griesemer2005} for
Weismannian causal relationship between genes and organisms
\end{itemize}

\paragraph{Lamarck}\label{lamarck}

Inheritance of acquired characteristics--e.g., epigenetics, but probably not significant as evolutionary mechanism in biology. But a viable non-biological mechanism

\subsubsection{Evolution without life}\label{evolution-without-life}

Not attempting to understand life, but instead as a guide to achieving similar property in another domain, or in understanding similar processes in another domain (e.g., Arthur)

Examples:
\begin{itemize}
\item
Random search--Random search/walk will explore possibilities, but without meaning\ldots{}
\item
Compositional--\autocite{Watson2002} discusses compositional evolution and building blocks
\item
Technology evolution--by components (Arthur2009)

\begin{itemize}
\item
\autocite{Arthur2009} investigates the evolution of
technology, where evolution is used in the sense of \quote{all objects of
some class are related by ties of common descent from the collection
of earlier objects.}{\autocite{Arthur2009}}

\item
  Evolution in technology occurs by using earlier technologies as
  building blocks in the composition of new technologies, and these new
  technologies then become building blocks for use in later
  technologies, and so on. Arthur calls this ``combinatorial
  evolution.'' But what is the starting point? How is this regression
  grounded? Arthur proposes that the capture and harnessing of natural
  phenomena starts each lineage, and provides new raw components for
  inclusion in later technologies.
\item
Evolution is related to innovation: in fact, Arthur claims that by
understanding the mechanism by which technologies evolve we will
understand how innovations arise. In other words, innovations arise
as the result of an evolutionary process, rather than de novo from
the brain of a designer.
\item
Darwinian evolution, or natural selection, is not appropriate for
technology. Arthur quotes from Samuel Butler's essay ``Darwin Among
the Machines'' : ``{[}t{]}here is nothing which our infatuated race
would desire more than to see a fertile union between two steam
engines\ldots{}'' to illustrate the impossibility of slavish
adoption of biological models.
\item
However, we can clearly see descent of form, in the example given by
Gilfillan in 1935 tracing the development of various elements of the
sailing ship: planking, sails, keels, ribbing and fastenings. In
each case we see a line of gradual improvements leading to the
present day component. But the point is made that this is not
evolution in the full sense, as it lacks both universal scope and an
underlying mechanism.
\item
The first obstacle to a more general scope is the existence of
innovations such as the jet engine, laser, railroad locomotive, or
QuickSort computer algorithm (to name Arthur's examples.)
Innovations seem to appear without obvious parentage; they do not
appear to be the result of gradual changes or adaptations to earlier
technologies.
\item
Arthur's answer is to look inside the innovation and to recognise
that each is made up of recognisable components or modules; the key
lies in the nature of heredity in technology. Technologies are
formed by combining modules of earlier technologies. These groupings
start as loose assemblages to meet some new function, but over time
become fixed into a standard unit (for example, the change in DNA
amplification mechanisms from assemblages of laboratory equipment to
standard off-the-shelf products.)
\item
\autocite{Bourrat2015} comments that distributive evolution (where
distribution of elements changes, as result of selection or drift)
cannot result in novelties
\item
Arthur's response is that novelty comes from incorporating new
phenomena\ldots{}
\end{itemize}
\item
HGT e.g., \autocite{Ochman2000}
\item
\autocite{Pross2011}--Dynamic Kinetic Stability
\end{itemize}

\subsubsection{Evolutionary Computation and Algorithms}\label{evolutionary-computationevolutionary-algorithms}

\begin{itemize}
\item
EAs originally abstracted/inspired by biological ENS
\item
Since specialized, radiated into new areas
\item
Re-unification attempted in works such as \autocite{Paixao2015} but hence not
concerned with extension beyond \quote{models in theoretical population
genetics and in the theory of evolutionary computation}{\autocite{Paixao2015}}
\item
\quote{Some EDAs can be regarded abstractions of evolutionary processes:
instead of generating new solutions through variation and then
selecting from these, EDAs use a more direct approach to refine the
underlying probability distribution. The perspective of updating a
probability distribution is similar to the Wright--Fisher model.}{\autocite{Paixao2015}}
\end{itemize}

Major points of difference:

\begin{itemize}
\item
Mutation combines copying errors with genetic drift (and probably
more)
\item
Search through a fixed space, cannot surprise \eg \autocite{Nellis2014}
\item
Fitness mechanism--implicit vs explicit
\item
top-down for EAs--specific constructs without endogenous evolution
(processes used not subject to evolution)
\end{itemize}

Similar interests in desirable properties:

\begin{itemize}
	\item redundancy and degeneracy -- \autocite{Whitacre:2010qy})
	\item novelty (novelty-search - \autocite{Lehman:2008cr})
\end{itemize}
 
Seems tightly linked to life--requirements for OEE (hypothesised) look
a lot like requirements for Life (which is only known OEE system, so
perhaps not so surprising)

What elements of life are required for OEE, and which are not?

\begin{itemize}
\item
Exact elements (same thing)--e.g., evolution?
\item
Equivalent elements (same concept/function/purpose, different detail)
e.g., compartments/membranes?
\item
Not required at all e.g., specific metabolic cycles
\end{itemize}

\subsubsection{Artificial Life}\label{artificial-life}

``Synthesis and simulation of living systems'' or contemporary
artificial life as \quote{an interdisciplinary study of life and life-like
	processes, whose two most important qualities are that it focuses on the
	essential rather than the contingent features of living systems and that
	it attempts to understand living systems by artificially synthesizing
	simple forms of them.}{\autocite{Bedau:2007ga}}

``life-as-it-could-be'' rather than ``life-as-we-know-it'' \autocite{Langton1989}

Implicit rather than explicit fitness

\begin{itemize}
	\item
	The mapping between representation and fitness must be implicit
	\item
	a property that arises from the representation itself rather than from
	an external measure
	\item
	difficult to imagine how to pre-specify a mapping that remains
	relevant in an open-ended system
\end{itemize}

Life a subset of Alife

Life is the only provided example that we have

Life has non-trivial emergence

Alife doesn't have to, but it is a guide

Hard vs Soft vs Wet forms of Alife

Purposes

\begin{itemize}
	\item
	Insights into life
	\item
	Swimming--\autocite{Terzopoulos1994}
	\item
	Foundations for other fields e.g., AI
	\item
	In own right--life ``de novo''
	\item
	Philosophy--what is living?
\end{itemize}

Themes \autocite{Aguilar2014}

\begin{itemize}
	\item
	Properties of living systems--Origins of life, autonomy,
	self-organization, adaptation (evolution, development, and learning)
	\item
	Life at different scales--Ecology, artificial societies, behaviour,
	computational biology, artificial chemistries
	\item
	Understanding, uses and descriptions of the living--information,
	living technology, art and philosophy
\end{itemize}

History

\begin{itemize}
	\item Prehistory
	\item Various automata
	\item
	Philosophy about automata
	\item
	1818 Mary Shelley ``Frankenstein; or, The Modern Prometheus''
	\item
	Uptick in mentions of ``Artificial Life'' as quoted in \autocite{Aguilar2014}
	\item
	Modern field
	\item
	1951 Von Neumann--first formal model
	\item
	1984 Christopher Langton
	\item
	1987 ``Official'' birth of field--first ``Workshop on the Synthesis
	and Simulation of Living Systems'' in Sante Fe, NM, by Langton
	\item
	Conway Game of Life
	\item
	Cellular Automata
	\item
	Tierra, Avida
	\item
	Dawkin's Biomorphs
	\item
	Bedau Challenges
	\item
	Overlap with Brooks's robotics, AI, etc
\end{itemize}

Some systems claim capable of OEE (e.g., Channon, Avida) but not necessarily creative
\TODO{complete list of relevant previous work in Alife - not so long a list...}

\section{TODO-Approach and methods}\label{approach}

Untangle the biological from the general in previous theory. Emergent, self-improving, actionist (similar to Brooks in AI)

Much previous work aimed at understanding biology--assumes that domain.

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{figures/approach}
\end{center}
\end{figure}

EvoEvo project taking a similar approach, but from a higher level biological starting point (genotype-phenotype mappings)
http://evoevo.liris.cnrs.fr/about-evoevo-project/

\begin{itemize}
\item Presupposes microbial evolution, ``at the level of genomes, biological networks and populations.''
\item Focus on four specific properties of a genotype-phenotype mapping - Variability, Robustness, Evolvability, Open-endedness
\item Later work to remove biological specificity to provide framework for applying EvoEvo to ICT problems
\end{itemize}

\subsection{TODO-The legitimate role of Simulations}\label{the-legitimate-role-of-simulations}

\TODO{rewrite, properly mark quotations + references!}

The field of artificial life is synonymous with simulation
\autocite[chap.2]{Aicardi2010}. In other forms of science however
practitioners make use of a number of tools, including experiments,
mathematical models, thought experiments, and simulations.
Each method is well suited to some types of
questions, and inappropriate for others. Is the use of simulation
justified for our proposed investigation?

Simulations are becoming central to some disciplines in natural history.
Ecology--\glspl{abm} or \glspl{ibm} (reviewed in
\autocite{DeAngelis2005}; also see
\autocite{Grimm:2006fk,Grimm:2005wd,Grimm:1999kf,Hogeweg:1990jz}).
\glspl{ibm} in Microbiolgy are seen as very close to Alife \autocite{Grimm:2009th}.

The value of simulation over experiments for
\gls{ibm} study lies in a reduction in costs; the difficulties in
cultivation of microbial populations (99\% of known species yet to be
cultivated), and significantly, that they form \quote{complex systems only
poorly explained by reduction.}{\autocite{Ferrer:2008hv}} Emergence and
dynamic behaviours are important, and yet they are hard to capture with
mathematical models.

Types of investigation

\subsubsection{Thought experiments}\label{thought-experiments}

non-empirical, clarification, contradictions/dissonance, fast, cheap.

\subsubsection{Models}\label{models}

\quote{
It is seldom the case in biology that a model is derived deductively
from a more fundamental quantitative theory, with the possible exception
of population genetics which has its foundations in evolutionary
theory.}{\autocite{Krakauer2011}}

Models can be ``useful stop-gaps'' towards a theory, by providing a
testable body of data for experiments and predictions
\autocite{Krakauer2011}, and may be constructed either bottom-up (such
as in ABM) or top-down, by the application of constraints
\autocite{Krakauer2011}. Many types in \eg ecology--eleven according to
\autocite{Jorgensen2008}--of which fall into two main groups

Mathematical models: complexity, need for abstraction/assumptions
(\eg Fisher's famous equation describing the changes in allele
distribution under selection assumes independent genes--extending this
to realistic cases remains an open problem \autocite{Schuster2011}),
difficulty in handling dynamism/emergence. Cheap, fast. Non-empirical.

Simulations: bottom-up approaches, holistic, variability so diversity
closer to real systems, adaptive behaviour/changing
\autocite{Ferrer:2008hv}. Non-empirical data.

\subsubsection{Experiments}\label{experiments}

reductionism, conflation (difficulty in removing other factors e.g.,
Heinemann), expense, time (generations). Source of empirical data.

\quote{
Although this may seem a paradox, all exact science is dominated by the
idea of approximation.}{The Scientific Outlook, Bertrand Russell}

\subsubsection{Benefits}\label{benefits}

Unique ability to explore subject = unique object of enquiry e.g., study
of emergence, complex, self-organizing subjects. Biology stands alone in
the importance of emergence \autocite{Bersini:2006ve}, and the
interconnection of levels of analysis, \eg behaviour can influence gene
expression, and genes can affect behaviour \autocite{Krakauer2011}. As
summarized by \autocite{Krakauer2011}, when asking how much of biology
can be predicted bottom-up from the application of basic physical and
chemical laws--``This question is simple to answer: effectively zero.''

Unique method of enquiry, that is, properties that improve on existing
techniques, e.g., by relaxing assumptions

\subsubsection{The epistemological nature of simulations}\label{the-epistemological-nature-of-simulations}

Simulations seem to fall somewhere in between thought experiments or
abstract models, and experiments. They are also relatively novel; common
use has only come with increased access to digital computers.
Consequently the nature of simulation--what can be claimed as a result
of simulation, and what role may be played legitimately by simulation in
scientific discovery--is a hot topic for philosophers of science. As
might not be unexpected, there are two opposing positions taken, plus a
synthesis that claims the middle ground.

\newthought{Simulations are just calculators}\label{simulations-are-just-calculators}

a computational means to solve analytically intractable equations
\autocite[31]{Winsberg2010}, producing nothing new (just consequences
of what is ``fed in''\autocite{DiPaolo2000}), nothing empirical.

\quote{A simulation is ultimately
only a high-speed generator of the consequences that some theory assigns
various antecedent conditions.}{\autocite{Eldridge}, quoting from Dennett 1979 p192}

If a model, then might take many forms--analogy, model, pure exploration \autocite{Webb2009}.

Models are \quote{a purposeful representation. A model needs to have a
purpose because otherwise there would be no way to decide what to
include in it. A model's purpose is a filter: the model should not
include anything not believed essential for explaining the phenomenon
of interest}{\autocite{Grimm:2009th}}

\autocite{MaynardSmith1974} distinguishes between
``practical'' descriptions of ecological systems, or ``simulations'', and
theoretical ones: ``models''.

Simulations are aimed at answering specific questions, or analysing
particular scenarios. The more accurate the simulation however, and
hence the more valuable the results, the harder it is to generalize to
other cases. It is hard to understand the behaviour of complicated
simulations, and the causes of particular behaviours of interest may be
unclear if there are many variables in play.

Instead, \autocite{MaynardSmith1974} prefers the use of simple models, designed
to illuminate the ``causes of differences of behaviour between different
species or systems'' rather than ``assertions which are true of all
systems or of all species.''

Hughes 1999 would argue that simulations have genuinely ``mimetic''
character, particularly ones that present results graphically as
real-world systems do, that goes beyond plain number-crunching. They use
a variety of methods beyond calculation (such as graphics) to draw
inferences from data. They also incorporate approximations and creative
choices to make the problem tractable, which introduces need for
justification. Simulations need interpretation and justification--they
are not self-contained, their own justifications
\autocite[31]{Winsberg2010}

\newthought{Simulations are themselves an instance of the
thing}\label{simulations-are-themselves-an-instance-of-the-thing}

That is, the thing is not a shadow but the object. The Animats are
actually alive, and therefore instances of biology.
\footnote{And this way leads us to the claims of Strong Alife--the simulation is actually alive.}
The simulation is a stand-in for the real world, and you can perform
experiments on it as would any other system
\autocite[31]{Winsberg2010}. As \autocite{Adami2002} says, describing
Avida,

\quote{
These organisms, because they are defined by the sequence of
instructions that constitute their genome, are not simulated. They are
physically present in the computer's memory and live there. The world to
which these creatures adapt, on the other hand, is
simulated\ldots}{\autocite{Adami2002}}

They certainly have elements of uncertainty and error, like experiments.

TODO Paul Humphreys 1994 says Monte Carlo simulations are experiments; Von
Neumann ``replace a computation from an unquestioned theory by direct
measurement'' (quoted in TODO [28]{Winkler et al 1987}).

Norton and Suppe 2001 argue that they are experiments, when proper conditions met:
``Empirical
data about real phenomena are produced under conditions of experimental
control'', although ``lacking other data, we can never evaluate the
information that these experiments provide'' Hughes 1999 p.142 so no inherent epistemological force to his
argument. Validity solely depends on validation.

\newthought{A third-way, neither experimental or theoretical}\label{a-third-way-neither-experimental-or-theoretical}

\autocite[31]{Winsberg2010} or an Opaque Thought Experiment"
\autocite{DiPaolo2000}. A common view, among others Dowling 1999, 264:
simulation is like theory as about ``manipulating equations'' and
``developing ideas'' but like experiments as ``fiddling with machines'',
``trying ideas out'', ``watching to see what happens.'' Simulation is a
form of Kuhn's theory articulation or ``model building''--making
principles apply to local, concrete systems in the real world. In this
view, \quote{Simulation is a process of knowledge creation}{\autocite[6]{Winsberg2010}}

\subsubsection{The legitimate role of simulation in scientific
enquiry}\label{the-legitimate-role-of-simulation-in-scientific-enquiry}

Following this third way, simulations might be seen as a source of new
hypotheses \autocite{Eldridge}. The fundamental question remains
however: what is the exact relationship between world and simulation? If
the simulation exhibits an analogous behaviour, under a set of
assumptions, to the real world, then we can contend that there exist
similar mechanisms in the real world to the assumptions in our model.
TODO Noble 1997, quoted in \autocite{Eldridge}). The example given by
\autocite{Eldridge} is Boids \autocite{Reynolds1987} where flocking behaviour in
birds is very similar to that that results from a set of three simple
rules in a simulation.

However, \autocite{Eldridge} identifies three problems with this
approach: first, logically, similarity does not require congruence
\autocite{Weitzenfeld1984}, second, how is the degree of similarity to
be assessed, and third, the impossibility of proving that a simulation
is an accurate model of a theory. That is, where does attribution lie?
Is the result a result of the underlying theory or a quirk of the
implementation? There may be no way of resolving this absolutely as it
may not even be possible to distinguish between the two
\autocite{DiPaolo2000}--we cannot prove correctness through testing.

TODO On the other hand, Taylor (1989) in \autocite{Webb2009}, argues for
``pure exploration'' or ``exploratory tools'' that do not need
justification, and that may be used to generate ``new questions to ask,
new terms to employ, or different models to construct'' (Taylor 1989,
p122). But in this case you might reasonably argue that any insights are
``insights about a mathematical system'' not necessarily insights into
the real-world (123–124 Taylor 1989).

In practice then, any form of model claiming a significance beyond its
own self must show a correspondence with the thing it claims to be
modelling.

\quote{However, existence proofs clearly do require comparisons
between model results and empirical data. One cannot evaluate the claim
that phenomenon X requires condition Y unless one can show that
phenomenon X is actually produced (with or without Y). And the claim or
proof will be stronger or weaker depending on how well the simulated X
matches the real X; for example, demonstrating successful behaviour in
the same physical situation as the animal.}{\autocite[278]{Webb2009}}
The strength of our belief depends on the degree of similarity.

\subsection{TODO-Alternative approaches}\label{alternative-approaches}

\section{TODO-Guide to this work}\label{guide-to-this-work}

\section{Previous publications}\label{previous-publications}

A version of Reactant and Product Strategies \cref{reactant-and-product-selection-strategies} was published as \cite{Young2015},
and material from \cref{model-validation} in \cite{Young2013}.

ToyWorld is available under an GNU GPL v2 open source licence from GitHub \cite{toyworld}.

\section{TODO-Contributions}\label{contributions}

\begin{enumerate}
	\item A grounding for Artificial Evolution in EvoEvo. Shown:
	\begin{itemize}
		\item Inheritance is inevitable given some form of variable copying mechanism
		\item Variation a result of the variability in copying
		\item Copying mechanism robust and self-tuning
	\end{itemize}
	\item Continue work towards a non-biological perspective on evolution
	\item Progress towards OEE in artificial system--evolution compatible with OEE without necessarily showing OEE (which is hard to measure and prove)
\end{enumerate}

\part{TODO-Evolutionary Potential}

\chapter{TODO-Introduction}

In this section we explore the following research questions:

\vspace{0.3cm}
\begin{minipage}[l]{0.95\textwidth}
\begin{enumerate}[label=RQ\arabic*:]
\item Can inheritance emerge from simple selection and variation?
\item Can evolution act on the inheritance mechanism to tune it for varying environmental conditions?
\end{enumerate}
\end{minipage}
\vspace{0.3cm}

V+S-\textgreater{}I

``Context determines fitness'', where the environment is stable and
affects the development of the entities

Minimal conditions for evolutionary system capable of open-ended (but
not necessarily interesting behaviour)

\begin{itemize}
\item
  Elements that allow for ongoing evolution--necessary, and starting
  point for novelty
\item
  Open-ended evolution can be seen as evolution in an open-ended system
  (\eg Chemistry), where an open-ended system has effectively
  unrestricted representation: the number of possible types must be much
  larger than the number of individuals (ideally without any
  restriction). Without this property all possible types can be
  generated in a finite time, and the system will either reach stasis or
  begin to repeat. Not all open-ended systems necessarily support
  evolution, but in those that do, our intuition suggests that
  open-ended evolution produces increasing complexity, increasing
  diversity, accumulation of novelty and continual adaptation
  \autocite{Lehman2012}.
\end{itemize}

\quote{
by open-ended evolutionary capacity we understand the potential of a
system to reproduce its basic functional-constitutive dynamics, bringing
about an un-limited variety of equivalent systems, of ways of expressing
that dynamics, which are not subject to any predetermined upper bound of
organizational complexity (even if they are, indeed, to the
energetic-material restrictions imposed by a finite environment and by
the universal physico-chemical laws.}
{\autocite{Ruiz-Mirazo2004}}

\begin{itemize}
\item
  Open-ended from \autocite{MaynardSmith1999} definition--\TODO{ size of search space vs population}
\item
  Heritability a challenge--biological organisms employ digital
  heredity; sophisticated mechanism with controlled error rates, but
  exceedingly unlikely to arise spontaneously
\item
  Multiplication/heredity for maintenance of population
\item
  Analog methods possible--\eg:

  \begin{itemize}
  \item
    compositional (where new entity contains some elements of original)
    (as seen in ACS ``core'' inheritance e.g., \autocite{Vasas2015, Watson2012}?)
  \item
    \quote{migrant pools}{\autocite{Watson2015}}
  \item
    Group fissioning \autocite{Watson2015}
  \item
    Attractor based \autocite{Szathmary2000}
  \end{itemize}
\item
  Heredity seen as method to maintain low entropy over much longer time
  than possible with non-''biological'' systems \autocite{Adami2015}
\item
  Argument that heredity may in fact be a product of evolution rather than a precursor \autocite{Bourrat2015}
\item
  \autocite{Kauffman:1993kk} argued that self-organization (RAPN) can replace the genome
\end{itemize}

\section{Previous work}

General difficulties:
\begin{itemize}
\item Somewhat arbitrary choices of elements of description
\item Genotype/Phenotype, Selection,\ldots{} often based on goal of rationalizing existing descriptions, so not a re-examination
\item Lack causality--so hard to use as mechanism
\item Leave options and alternatives for implementer
\item Sheer number of EA algorithms
\end{itemize}

Proofs of effectiveness:
\begin{itemize}
\item Base equations from Malthus and population genetics
\item \autocite{Vose:1999di} for EAs
\item Underlying assumptions should be maintained in our models
\end{itemize}

\autocite{Godfrey-Smith2007}
\begin{itemize}
\item Biological summaries
\item Purpose of summaries as opposed to Formal models
\item Identify the major elements in biology
\end{itemize}

\autocite{Bourrat2015}--Not all of these elements are essential

Other attempts at general models:
\begin{itemize}
\item \autocite{VonNeumann1966} as reviewed in \autocite{Taylor:1999sc} (Lack of environmental emphasis)
\item \autocite{Waddington2008} as reviewed in \autocite{Taylor:1999sc}--Originally published in ``Towards a Theoretical Biology, Vol. 2'' in 1969
\item \autocite{Paixao2015}--reconciliation of EA models with PG models--A synthesis rather than a reformulation--way of describing both A and B using generalizations of each. Not a causal explanation
\end{itemize}

\section{Method}\label{method}

Goal is a minimal description of necessary factors--enough to build a
mechanism

Strategy is to make as many factors as possible endogenous rather than
needing specification by the modeller--more general and hence more
powerful claim

Therefore separate out descriptive and causal descriptions of process

Descriptive--outcome or goal--based on Evolution. Not useful for
construction, but rather for testing

Causal--mechanism

\begin{itemize}
\item
  Want to remove extraneous factors
\item
  More factors introduce complications
\item
  Complicate identification of necessary conditions vs contingent
  factors
\item
  Ideally minimal choices to be made or parameters to be chosen
\item
  Broadest claim
\item
  Worth reflected in proportion of population--core principle of method
\end{itemize}

\section{Mechanism is cumulative adaptive evolution}\label{mechanism-is-cumulative-adaptive-evolution}

From earlier work, attempt a synthesis based on common factors

\section{Assumptions}\label{assumptions}

No learning mechanisms--changes during lifetime

Corollary--addition and removal of elements

\begin{itemize}
\item
  If have no removals then population is either static (contradiction)
  or indefinitely expanding (practical problem)
\item
  If have no additions, then cannot adjust population proportions
\end{itemize}

\section{Contributions}

\begin{itemize}
\item General model for evolutionary processes
\item Experimental support for hypothesis that selection and variation sufficient for evolution
\item Experimental support for ability of a copy mechanism with adjustable fidelity to auto-adjust to changing environmental conditions
\end{itemize}

\chapter{TODO-Synthesis based on first principles}\label{synthesis-based-on-first-principles}

\section{Variation (and Inheritance)}\label{variation-and-inheritance}

Assume that variation occurs only on addition

\begin{itemize}
\item
  If during life then, by definition, a learning mechanism
\end{itemize}

Corollary: no Lamarckian inheritance

Variation is in some element from parent to offspring

\begin{itemize}
\item
  Degree of relationship interesting
\item
  No relationship (no covariance) is as before--no learning
\item
  If not related, then effectively random search--process is not
  `learning'--no information inheritance
\item
  Duplication is effectively the same as just extending the lifespan -
  not interesting
\item
  Somewhere in between (between 0 and 1) means heredity/inheritance
\item
  Earlier work in biology has connected evolvability to mutation
  rate/inheritance rate
\end{itemize}

\section{Selection}\label{selection}

\subsection{Exogenous}\label{exogenous}

External calculation used directly to adjust population

Or guided evolution--external agency adjusts population directly

But as external element shared across population not evolvable

\subsection{Endogenous}\label{endogenous}

Feedback loop between element and everything else (in the sense of Pattee’s ``semantic
closure’’ discussed extensively in \autocite[sect. 3.5]{Taylor2001})

Causally results in a proportional change in population proportions

e.g., resource competition, or some other interaction between element and other things

Could occur on fixed timeframe (fixed generations, as common in
biological modelling) or continuous

More generally, on range from \textasciitilde{}0 (continuous selection)
to 1 (generations), although mid-points likely to be of little
additional value

Affected by element and by everything else, so conceivably could be
guided by modifying the environment in a calculated way

\section{Issues--still parameters and choices}\label{issues -- still-parameters-and-choices}

When do additions happen?

\begin{itemize}
\item
  Degree of correlation between addition events and selection events
\end{itemize}

\autocite{Bourrat2015} introduces a check-for-overcrowding step--is
this necessary? Under endogenous selection shouldn't overcrowding also
be endogenous?

\section{Can a causal reading further simplify the model?}\label{can-a-causal-reading-further-simplify-the-model}

Reproduction follows from replication--no need to choose degree of
similarity, emergent from lower level rules

\chapter{TODO-Synthesis from a general evolutionary model}\label{synthesis-from-a-general-evolutionary-model}

\TODO{Reformulate as causal reading}

\autocite{Bourrat2015} showed that inheritance bias (correlation in trait between parent and offspring) increases over time


TODO
\begin{enumerate}
\item
Fitness independent of inheritance potential--as explored in
\autocite{Bourrat2015}--bias applied only to bias value of offspring, not fitness
\item
fitness dependent on inheritance--more likely. A mechanism that doesn't copy well unlikely to preserve information leading to high
fitness\ldots{}--the parent's fidelity influences the offspring's fidelity, and to offspring's fitness
\end{enumerate}

\begin{itemize}
\item
  Model 4--proportion of high fitness entities rapidly increases to
  near 1.0
\item
  Model 5--both the proportion of offspring that are procreators
  (rather than persistors) and the heritability of ability to procreate
  increases over time to 1.0
\end{itemize}

\section{Hypothesis: Variation and Inheritance and Selection are sufficient for Evolution}\label{h1}

\begin{hypothesis}
Variation and Inheritance and Selection are sufficient for Evolution, or V+S+I-\textgreater{}E
\end{hypothesis}\label{hypothesis-1}

From previous work\ldots{}

For some forms of V, S, I, E\ldots{}what are requirements?

\begin{itemize}
\item
  For I, reasonable to define by degree--\autocite{Bourrat2015} = bias
  or degree of correlation between parent and offspring
\item
  For S, compatible with resource competition--endogenous fitness
\item
  For V, compatible with copy mutation
\end{itemize}

Can we construct a causal diagram from variation and selection that:

\begin{itemize}
\item
  complies with requirement (optimizer, population dominated by best)
\item
  less choice, more inevitable, less arbitrary
\item
  foundation for creativity?
\end{itemize}

\section{Hypothesis: Correlated Variation and Selection are sufficient for Inheritance}\label{h2}

Inheritance comes from a copy mechanism under evolutionary control, such that the fidelity of the copy may
be varied by interaction with the environment.

Inheritance describes the similarity of an offspring to its parent, and depending on the context, can refer to the correlation for either a single trait or to a group of traits shared between offspring and parent (perhaps all of them.)

For the single trait case, when the correlation between the value for a parent's trait and an offspring's trait approaches the upper limit of 1.0 we say we have complete or full inheritance. Conversely, if there is no correlation (near the lower limit of 0) there is no inheritance and the entities are unrelated. We extend the measure to a group of traits simply by taking the average correlation of the group.

\begin{hypothesis}
Variation, where there is some initial correlation between generations for a property, and Selection are sufficient for maximal Inheritance, or $V'+S\rightarrow I$
\end{hypothesis}\label{hypothesis-2}

\TODO{we mean inheritance increases to some optimal level}
Inheritance is defined as any correlation better than 0.5, or in other words, as better than chance. Fidelity is defined as the degree of correlation between parent and offspring values for the same trait.

\TODO{I iff V' (that is, with initial correlation)}
\TODO{Mechanism vs measure for fidelity}
\TODO{Can this be strengthened to include V+S necessary for I, or I-\textgreater{}V+S?}

\section{Predictions}\label{predictions}

In an unchanging environment, where the relative fitness of an entity, with respect to the environment, does not change over time, our expectations are:

\begin{enumerate}
	\item Average inheritance will tend towards perfect inheritance.
		Higher fitness entities will survive longer and reproduce more; higher fidelity reduces variation in fitness and so results in higher fitness being preferred.
	\item Population variance for inheritance will decrease more than chance.
\end{enumerate}

Under changing environmental conditions, where the fitness of an unchanging entity changes in response to environmental changes, we expect:

\begin{enumerate}
	\item The \gls{sd} of final fidelity under changing conditions \textgreater{} that under fixed conditions.
	\item The higher the variability in the environment, the higher the \gls{sd} of \emph{Fidelity}.
	\item \emph{Fidelity} at the end of a run under changing conditions \textless{} that under fixed conditions.
	\item The final \emph{Fitness} will be in the the range described by the distribution applied in the $tweakFitness$ function.
\end{enumerate}

\section{Theory}\label{theory}

Degree of variation between generations important (no correlation means
effectively unguided search, complete correlation means no source of
novelties)

  Problem is how can the optimal degree of variation (that is,
  inheritance) be established endogenously rather than as a model
  parameter?

Inheritance related to variation (between generations). Low variation
implies high inheritance

Selection strengthens degree of inheritance

  high fitness lineages more successful. Inheritance increases
  correlation along lineage, so high fitness more likely to be passed
  down. (Also low fitness, but they will suffer). On average then high
  inheritance increases average fitness over time

\section{Alternative explanations}\label{alternative-explanations-1}

The three main alternatives, examined later in \cref{elimination-of-alternative-explanations} are:

\begin{enumerate}
	\item Variation alone is sufficient for Inheritance, or $V\rightarrow I$.
	\item Selection alone is sufficient for Inheritance, $S\rightarrow I$.
	\item Variation and Selection, without trait or property correlation, is sufficient for Inheritance.
\end{enumerate}

\chapter{Simulation model}

To test our hypothesis, we turn to simulation and experiment. If a simulation that accurately represents the system in the hypothesis behaves in a way that matches our predictions, the hypothesis is supported. On the other hand, if the behaviour doesn't align with the predictions, our hypothesis will be rejected.

Experimental tests are the strongest method we have to examine the claims made in hypothesis \autoref{hypothesis-2}. As explored earlier in \cref{the-legitimate-role-of-simulations}, logical argument or theorem-proving is difficult to apply to model-based systems. Thought experiments lack the strength we hope for, while experimentation is both feasible and, assuming correct design, rigorous.

Note that for those familiar with the design of experiments in the physical world, there are some differences in simulations, with the most significant being the sources and understanding of experimental errors. In simulation, experimental runs are exactly reproduceable, absent any dependency on factors external to the simulation. Variation is explictly introduced usually through a random number generator, which can be seeded to produce the same sequence of numbers again and again. This means that the practice in real-world experiments of ``blocking'' to control external variation is not required in simulation experiments. However, \gls{replicate}s where the same combination of factor values is run several times each with a different random seed value, remains valuable, but in this case less to control for experimental error and more to record the variation across a series of runs and the sensitivity of the model to parameter settings.

\section{Base model}\label{base-model}

All experiments in this part of the work make use of some variant of the same base model (\autoref{base-model-algorithm}) where the key elements of the hypothesis, such as inheritance fidelity, are represented as explicit parameters. The main elements will be quite familiar to anyone from Evolutionary Computation or Evolutionary Biology, although the introduction of fidelity is novel and important to the overall thesis.

We define a population of entities, each with two properties -- \emph{fitness} and \emph{fidelity} -- and a set of population-transforming functions.

\begin{itemize}
	\item \emph{Fitness} represents the probability that an entity will survive and possibly also reproduce, and has the usual range for a probability of $[0,1]$.
	\item \emph{Fidelity} measures the correlation between the child's value for a property and the same property's value in the parent. The range is $[0,1]$ where a value of $0$ means that the value for a child's property has no correlation with its parent's value for that property. High \emph{fidelity} values mean high correlation, and when $fidelity = 1.0$ the child's value is identical to the parent's.  There is a subtle difference between fidelity and inheritance: inheritance is the result, while fidelity is the mechanism, expressed as the degree of correlation between two generations. Inheritance can be thought of as emerging or resulting from fidelity.
\end{itemize}

The specific relationship between parent and child property values is given by a single mapping, represented in the algorithm by the function $Derive$.

The only transformation functions in the base model are for the two core elements from the hypothesis, \emph{Selection} and \emph{Variation}, although in later sections we will add others to examine the sensitivity of the model to various influences.

Each \gls{run} of the model consists of a fixed number of time steps (generations), where at each step the functions are applied in a defined order to the current population to form a replacement population. Colloquially, the replacement population is formed by a combination of parents from the current population, plus their children.

The model is parameterized (see \cref{tbl:parameter_definitions}) so that different combinations of factors can be investigated for their influence on the model's behaviour.

\begin{figure}[htb]
\centering
\includegraphics[width=0.95\linewidth]{figures/model}
\end{figure}

\begin{algorithm}
\For{each generation $\in [1\dots$number of generations]}{
	$population\leftarrow Selection(population)$\;
	$population\leftarrow population \cup Variation(population)$\;
	\BlankLine
	\lIf{$population$ size is too small}{break}
}
\BlankLine
\BlankLine
\Def{Selection(population)}{
	$population_{new}\leftarrow \{\}$\;
	\For{each $entity \in population$} {
		\lIf{$p_{selection} = 0$}{$p\leftarrow $parent's fitness}
		\lElse{$p\leftarrow p_{selection}$}
		\Prob($p$:){
			Add $entity$ to $population_{new}$\;
		}
	}
	\Return $population_{new}$\;
}
\BlankLine
\Def{Variation(population)}{
	$children\leftarrow \{\}$\;
	\For{each $entity$ in $population$}{
		\lIf{$p_{reproduce} = 0$} {$p\leftarrow $parent's fitness}
		\lElse{$p\leftarrow p_{reproduce}$}
		\BlankLine
		\Prob($p$:){
			\For{some number of children $\in \mathcal{U}[0,n_{children}]$} {
				$fitness\leftarrow$ Derive(parent's $fitness$, parent's $fidelity$)\;
				\uIf{Correlate Fidelity} {
					$fidelity\leftarrow$ Derive(parent's $fidelity$, parent's $fidelity$)\;
				}
				\uElse{
					$range\leftarrow$ some $x \in \mathcal{U}[0,1]$\;
					$fidelity\leftarrow$ Derive(parent's $fidelity$, $range$)\;
				}
				Create new $child$ with $fitness$ and $fidelity$\;
				Add $child$ to $children$\;
			}
		}
	}
	\Return $children$\;
}
\caption{Algorithm for the base model}\label{base-model-algorithm}
\end{algorithm}

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\linewidth]{figures/correlation}
\end{figure}

\begin{table}
	\begin{center}
		\caption{Model parameters}\label{tbl:parameter_definitions}
		\begin{tabular}{@{}llp{8cm}@{}}
			\toprule
			Parameter&  Value& 	Description\\
			\midrule
			$p_{reproduction}$& 		$0$&       Probability of reproduction is given by the parent's $fitness$\\
			&								$(0,1]$&    Fixed probability of reproduction. The probability of reproduction for any entity is given by $p_{reproduction}$\\
			$p_{selection}$&    		$0$&        Probability of selection is given by the parent's $fitness$\\
			&          					$(0,1]$&    Probability of selection is unrelated to parent's $fitness$ and given instead by $p_{selection}$ for all entities\\
			$n_{children}$&           	$n_{children}\in \mathbb{Z}_{\ge 0}$& Maximum number of children per parent\\
			%Restriction&			  	$(population, n)\mapsto population$&	Function to take $n$ elements from $population$\\
			Derive&             		$[0,1]\times[0,1]\mapsto[0,1]$&    Function for generating a sample value $x$ from a distribution described by some measure of expected value and range\\
			Correlate Fidelity&       	$\{\mathrm{true}, \mathrm{false}\}$&  Child's fidelity is related to parent's fidelity by parent's fidelity, or if false, by a random value $x\in \mathcal{U}[0,1]$\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\subsection{Claim of generality}

The model as described is general, under the assumptions given below. It is also directly comparable to models from Evolutionary Computation and so we also expect any results and conclusions to be relevant to EC models.

\TODO{Map to standard EA models - mutation only etc}

\section{Initial conditions and settings}\label{initial-conditions}

At the beginning of each run we construct an initial population to some design. Although a randomly chosen population is useful so as not to introduce any bias that may result from a consistent starting point, our interest is also in two other factors. First, under evolution we expect a general increase in fitness over time from any low initial starting point, and second, if a population is already in an evolved state with a reasonably high relative fitness we would not expect to see a decrease over time in fitness. We call these two initial starting points the low-start and High Start cases respectively, and they are described in the sections below.

\subsection{Low-start case}\label{low-start-case}

Entities in the low-start case begin with a low initial fidelity and fitness chosen randomly from a uniform distribution over a particular range (see \cref{tbl:ic}.) This case is of interest for two reasons: first, directly from hypothesis \autoref{hypothesis-2}, and driven by the overall goal of this thesis, we expect to see a population of low fidelity entities  eventually replaced by one of high (or at least higher) fidelity ones. Second, this is analogous to a key step on the path taken in biology from the abiotic world, where early copy mechanisms lacked the capabilities for high-fidelity copying (see earlier discussion in \cref{alternative-approaches}.)

\begin{table}[t]
	\begin{center}
		\caption{Initial conditions for low-start and High Start cases}\label{tbl:ic}
		\begin{tabular}{@{}lll@{}}
			\toprule
			Case&	Property&  Initial range\\
			\midrule
			\multirow{2}{*}{Low-start}&			Fitness&	$\mathcal{U}[0, 0.3]$\\
			&													Fidelity&	$\mathcal{U}[0, 0.3]$\\
			\midrule
			\multirow{2}{*}{High Start}&		Fitness&	$\mathcal{U}[0, 0.9]$\\
			&													Fidelity&	$\mathcal{U}[0.5, 0.9]$\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\subsection{High-start case}\label{high-start-case}

In this case, entities start with a higher initial fidelity and a greater range of fitnesses (again, for specifics see \cref{tbl:ic}). This case examines the ability of the model to preserve advantageous entities, although, in the broader scope of the thesis, it is highly unlikely that any randomly created population would be of this initial form.

%In biological systems, random genetic drift can result in a decreasing fitness trend under certain circumstances \TODO{ref} but as our model omits drift \TODO{ true?} we can eliminate it as an acceptable explanation for any downwards trend.

\chapter{Initial screening}

Translating model parameters into factors in the experiment design results in the factors in the first column of \ref{tbl:factor-levels-for-investigation-into-inheritance-under-low-start-conditions}. As is usual with exploratory experiments with a number of parameters, where each run of the model has some cost in time or other resources, the key problem is to understand the relationship between parameters and response variables at an acceptable cost. In this case, our main cost is time - each run of an evolutionary model is cheap in resources but takes a little time. Exhaustively sampling the entire parameter space is unrealistic. Therefore, we first reduce the search space by limiting the number of values that each parameter can take. By choosing these values appropriately, we can construct an analysis model from the results that is sufficiently accurate for our exploratory purposes at a greatly reduced cost in time.

There are many approaches to this, but they mostly fall into two standard groups. First are response-surface methods which sample from the parameter space in a particular fashion to effectively construct an analysable function, or response-surface, from parameter values to response-variable values that approximates to some degree the behaviour of the model. The emphasis is on the shape of the response-surface; the parameter values are randomly chosen.

The usual alternative is some variant of a factorial design, where each parameter of interest is represented by a factor taking some small number of values, or levels (two levels being most common) and the analysis model constructed from runs that systematically work through a series of combinations of factors at different levels. The emphasis here is on the response given particular factor, and hence parameter, values.

\section{Experimental design}

As our interest is in the behaviour of the model both overall, and under specific conditions (such as the low-start case, or to the \emph{Correlate Fidelity} parameter), a factorial design is preferred.

Now, a full factorial design with seven 2-level factors would require testing $2^{7}$ combinations of factor values, or 128 sets of replicated runs, while a $2^{(7-3)}$ fractional factorial design \footnote{\eg  \url{http://www.itl.nist.gov/div898/handbook/pri/section3/eqns/2to7m3.txt}} can reduce this to 16 sets of replicated runs without loss of validity on the assumption that 3-factor interactions and higher are not significant. In other words, a $2^{(7-3)}$ design is sufficient to separate the main effect from any 2-factor interactions.  This seems a reasonable tradeoff between discriminatory power and the total number of runs required, given our exploratory goals.

The design is complicated a little by the suspicion that two factors - $p_{reproduction}$ and $p_{selection}$ - require more than two levels. Fortunately a $2^n$ fractional design can be extended relatively simply to include 4-level factors (\cite[368]{Montgomery2009}) either by replacement where each 4-level factor is mapped to two 2-level ones, or, as we choose to do, by a hybrid full-fractional design, where use combinations of our four-level factors,  $p_{reproduction}$ and $p_{selection}$, for two of the two-level factors, X1 and X2, of the standard $2^{(7-3)}$ design. This may not be quite as computationally efficient as a complete mixed-levels fractional factorial design but is efficient enough and meets our purposes.

We use a fractional factorial design to reduce the number of experimental runs required when compared to a full factorial design \autocite{Montgomery2009}. Although the number of runs is reduced, as each level of each factor occurs the same number of times in the results, the design remains ``balanced'' in the statistical sense, greatly easing analysis.

The number of \glspl{replicate} required for a particular statistical power is related to the \gls{sd} of the response variable.

\TODO{Based on preliminary runs, we begin with an initial estimate of 10 replicates for each combination of factor values given in the design; we confirm that this is sufficient through power calculations for specific tests where required in the later sections.}

\section{Factors and levels}\label{factors-and-levels}

Although several parameters in the model are continuous, at this stage the main ones can be reduced to a set of A/B alternatives, or two-level factors.

Of the others, $p_{reproduction}$ and $p_{selection}$ take four levels each to cover a reasonable range given the usual sensitivity of evolutionary models to these type of parameter; the value $0$ for the corresponding model parameter in each case means ``use parent's value'', and so is a special case. And as discussed in \ref{upper-size-bound}, \emph{Population Restriction} is held to Fitness-independent sampling.

The factors and levels used are given in \ref{tbl:factor-levels-for-investigation-into-inheritance-under-low-start-conditions}.

\begin{table}
	\begin{center}
		\caption{Factor levels for investigation into inheritance under low-start conditions}\label{tbl:factor-levels-for-investigation-into-inheritance-under-low-start-conditions}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Factor&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       	4&	0 or 0.33 or 0.66 or 1.0\\
			$p_{selection}$&          	4&	0 or 0.33 or 0.66 or 1.0\\
			$n_{children}$&           	2&	2 or 5\\
			Population Restriction&  	1&	Fitness-independent sampling\\
			Distribution&               2&	Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$ \footnote{But see discussion in \ref{screening-distribution} as to the implementation}\\
			Correlate Fidelity&       	2&	false or true\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\section{Probability of selection and probability of reproduction}

We take these together as they interact. Where one or both parameters equal zero (use the parent's fitness as the probability of selection or probability of reproduction), the model is similar to a standard EA; any other value means selection or reproduction occurs with a fixed probability irrespective of fitness.

Why would we be interested in runs under these conditions? Imagine a run where selection has the value $1.0$ while reproduction takes value $0$ - parents always survive to the next generation (subject to any population limits naturally), while producing children at each generation with a probability proportional to their fitness. Or reverse these values ($p_{selection} = 0$ while $p_{reproduction} = 1.0$), giving a run where parents always produce a fixed number of $n_{offspring}$ children at each generation, but their own survival rate is proportional to their fitness.

The combination where both $p_{selection}$ and $p_{reproduction}$ are non-zero leads to uninteresting behaviour as the major source of variation has been removed from the model.

\section[Upper size bound]{Effect of upper bound on population size}\label{upper-size-bound}

In the absence of any restrictions on population size, there is nothing to prevent a population growing beyond the capacity of the simulation system. This is a practical problem rather than a property of the theoretical model, and so to have faith in the simulation and model it's important we eliminate the possibility of introducing bias to the results from the mechanism used to control population size.

The size of the population is driven by how population elements are introduced and removed. In standard Evolutionary Computation (\eg \cite[50]{DeJong2006}) the choice of strategy is important to the performance and outcomes of the algorithm. New elements can be straight replacements, like-for-like, of their parent, or be placed in competition against elements in the parent population, or completely replace the parent population. Elements may be removed as a result of selection, or through fitness-independent sampling to maintain a particular population size, or through some end-of-life calculation. The population size limit may act as both upper and lower bound on population size to maintain a specific size, or solely as upper bound.

Similar considerations apply to our model. Because we observe that the population size increases exponentially in many experimental runs (e.g., righthand side of \cref{fig:unboundedplot}), some upper bound on population size is needed. In the ``canonical'' Evolutionary Computation algorithm, a population limit results from selection where a set number of elements is extracted from the original population, with elements chosen by one of a wide range of selection algorithms (among many sources, see overviews in \cite[sect. 4.3.1]{DeJong2006} and \cite[sect. 4.2]{Vose:1999di}.) Here though we break the selection function from the population size limit in order to qualify the effect of the specific limiting mechanism used.

To summarize then, the goal of this section is to:

\begin{enumerate}
	\item Confirm that an upper bound on population size is required,
	\item Decide if the choice of method for maintaining the bound might significantly affect any conclusions from the hypothesis tests, and if it might,
	\item Determine which method to use for the remaining experiments.
\end{enumerate}

If there is an affect on the hypothesis tests attributable to the bound mechanism, then the conclusions from the test become contingent on the method. This reduces the scope somewhat, but without a change of investigative approach seems unavoidable.

\subsection{Effect of population limits}

% EXPERIMENT 1 - UNBOUNDED POPULATIONS
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results.simple('results/results-d9f18e2.data'), environment_change_frequency==0)
@

The environment is considered ``fixed'' (that is, each element's fitness value does not change); the initial population consists of \Sexpr{df[1,]['pop']} elements constructed according to the low-start case (\cref{low-start-case}), and the model run with a maximum of \Sexpr{max(df['gen'])} generations with

\begin{table} % d9f18e2
	\begin{center}
		\caption{Factor levels for investigation into the need for an upper-bound mechanism}
		\label{tbl:factors-levels-for-upper-bound-investigation}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       	2&	0 or 1.0\\
			$p_{selection}$&          	2&	0 or 1.0\\
			$n_{children}$&           	2&	2 or 5\\
			Population Restriction&  	2&	No upper-bound, or Fitness-independent sampling\\
			Distribution&               2&	Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$\\
			Correlate Fitness&        	2&	false or true\\
			Correlate Fidelity&       	2&	false or true\\
			Starting population&		1&	Low-start settings\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<unboundedplot, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='On the left, a histogram of number of generations achieved by an unbounded algorithm before either i) the population size increases beyond 10x the original population size, or ii) the simulation succeeds in reaching the expected number of generations (which never occurred in an unbounded population). On the right, the growth of population size grouped by replicate (that is, by common factor levels) showing rapid population growth before reaching the 10x practicality limit.'>>=
ap <- ggplot(aggregate(df$gen,Filter(is.factor,df),max), aes(x))+geom_histogram(binwidth=1,boundary = .5, fill='gray70') + labs(x="Final generation", y="Count of Runs") + scale_x_continuous(breaks=1:12) + scale_y_continuous(limits=c(0,2),breaks=c(0,1,2))
bp <- ggplot(df,aes(x=gen,y=pop,group=interaction(p_reproduce,p_selection,fitness_correlation,correlation_correlation,truncate,distribution))) + geom_line() + geom_point() + scale_x_continuous(breaks=1:100) + labs(x="Generation", y="Population size")
grid.arrange(ap,bp,nrow=1,ncol=2)
@

Under these initial conditions, no experiment without an upper population bound continued for more than a handful of generations before the population size reached more than ten times the initial size. It is clear from \cref{fig:unboundedplot} that some form of upper bound is necessary.

\subsection[Choice of limit mechanism]{Is the choice of limit mechanism significant?}

Given then that an upper bound is needed for practicality, this section describes two possible approaches to the implementation of the \emph{Restriction} function in \ref{upper-bound-model-algorithm} and attempts to understand the form of the bias each introduces into the experimental results.

\begin{algorithm}
	\For{each generation $\in [1\dots$number of generations]}{
		$population\leftarrow Selection(population)$\;
		$population\leftarrow population \cup Variation(population)$\;
		$population\leftarrow Restriction(population)$\;
		\BlankLine
		\lIf{$population$ size is too small}{break}
	}
	\caption{Algorithm for the Model with an upper bound on population size. The \emph{Restriction} function is the only difference with the base model.}\label{upper-bound-model-algorithm}
\end{algorithm}

The first approach is to take a random sample of $n$ elements from the population, without regard for fitness (that is, fitness-independent sampling), while the other is to adopt the \emph{truncation} mechanism (described in \cite[124]{DeJong2006} alongside others) as representative (although strongly elitist) of a fitness-based mechanism (henceforth called fitness-based selection).

It might be argued that \emph{truncation} is too extreme to make a fair comparison. However, without an accepted scheme to order selection algorithms along a dimension of interest, such as selection pressure, it's hard to justify oversetting it by an alternative. The key criteria is whether the conclusions drawn from any particular algorithm can be extended to a more general conclusion, independent of the specifics of the algorithm or algorithms used. The specificity of selection algorithms means this a difficult argument to make, and most likely only possible by examining a number of them, which is beyond the scope of this work.

Returning then to our examination of fitness-independent and fitness-based upper bounds against the unlimited control case, the null, H$_0$, and alternative hypothesis, H$_1$, are as follows (examined for both population mean fitness and fidelity) :

\begin{itemize}[label={}]
	\item H$_0$: the two samples are drawn from the same continuous distribution
	\item H$_1$: the two samples are not drawn from the same continuous distribution
\end{itemize}

\subsubsection{Fitness-independent sampling}\label{fitness-independent-sampling}

The bound is implemented by a fitness-independent sampling of $n$ elements from the population (as given in the $Restriction$ function in \autoref{upper-bound-model-algorithm}), if and only if the pre-sampling population size is greater than $n$.

\begin{function}
	\SetKwProg{Def}{def}{:}{}
	\Def{Restriction(population,n)}{
		\Return $\text{random sample of }n\text{ elements from }population$\;
	}
	\caption{Restriction (Fitness-independent sampling)()}
\end{function}

%If we compare fitness-independent sampling to the control case without limits, we conclude that fitness independent sampling does indeed make a difference to the results, confirming the earlier visual assessment from the box and \gls{qq} plots (\cref{}.) This is somewhat unexpected as the average fitness before and after the application of the mechanism is, within sampling error, unchanged. It seems that the difference between limited and unlimited populations might come instead from the differences in population size in the selection and reproduction steps of the model rather than from any fitness-modifying actions of the restriction mechanism.

\subsubsection{Fitness-based selection}

The fitness-based population limit is based on \emph{truncation} from \cite[124]{DeJong2006}, chosen as it is reasonably representative of methods used in Evolutionary Algorithms, and as a highly-elitist algorithm should provide useful contrast to the effectively uniform mechanism of \cref{fitness-independent-sampling}. If the two mechanisms produce similar results then it might be argued that other mechanisms are likely to be similar also.

It also recognizes that in both Evolutionary Computation and natural populations, the population limit is a function of the carrying capacity of the environment and fitness determines the selection.

\begin{function}
	\SetKwProg{Def}{def}{:}{}
	\Def{Restriction(population,n)}{
		$sortedPopulation\leftarrow$ sorted population by element fitness, in decreasing order \;
		\Return $\text{first }n\text{ elements from }sortedPopulation$\;
	}
	\caption{Restriction (Fitness-based selection)()}
\end{function}

\subsection{Experimental design}

% EXPERIMENT 2 - CHOICE OF LIMIT MECHANISM
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results.simple('results/results-53f6b74.data'), environment_change_frequency == 0)
@

\begin{table} % 53f6b74
	\begin{center}
		\caption{Factor levels for the choice of an upper-bound mechanism}
		\label{tbl:factor-levels-for-the-choice-of-an-upper-bound-mechanism}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       	4&	0 or 0.33 or 0.66 or 1.0\\
			$p_{selection}$&          	4&	0 or 0.33 or 0.66 or 1.0\\
			$n_{children}$&           	2&	2 or 5\\
			Population Restriction&  	2&	Fitness-independent sampling, or Fitness-based selection\\
			Distribution&               2&	Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$\\
			Correlate Fitness&        	2&	false or true\\
			Correlate Fidelity&       	2&	false or true\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<echo=FALSE>>=
runs <- nrow(subset(df,gen==0))
factors <- nrow(unique(Filter(is.factor,df)))
replicates <- runs / factors
df_completed <- subset(df, gen==500)
@

Data is from \Sexpr{nrow(subset(df,gen==0))} runs -- \Sexpr{factors} unique sets of factors, each with \Sexpr{replicates} replicates -- with settings given in \cref{tbl:factor-levels-for-the-choice-of-an-upper-bound-mechanism} under fixed conditions, of which we are exclusively concerned with the subset of \Sexpr{nrow(df_completed)} runs that reached completion at \Sexpr{max(df['gen'])} generations.

\subsection{Results and discussion}

The runs that reached completion are evenly distributed between the two methods, fitness-independent (\Sexpr{nrow(df_completed[df_completed['truncate']==0,])} runs) and fitness-based (\Sexpr{nrow(df_completed[df_completed['truncate']==1,])} runs), indicating that the two methods at least appear comparable with respect to their affect on population size.

<<limitcomparisonplots, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Summary plots for fitness-independent sampling ("Independent") and fitness-based or dependent selection ("Dependent"), where the top line contains density plots and the bottom line boxplots showing ranges for both final mean fidelity (on the left-hand side) and final mean fitness (on the right).'>>=
 df_completed$truncate <- factor(df_completed$truncate, labels=c("Independent","Dependent"))
 ap <- qplot(ave_cor, facets=truncate ~ ., geom="density", data=df_completed, xlab="Fidelity", ylab="Density")
 bp <- qplot(ave_fit, facets=truncate ~ ., geom="density", data=df_completed, xlab="Fitness", ylab="Density")
 cp <- qplot(truncate, ave_cor, geom="boxplot", data=df_completed, xlab="",ylab="Fidelity")
 dp <- qplot(truncate, ave_fit, geom="boxplot", data=df_completed, xlab="",ylab="Fitness")
 grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
@

<<limitqqplots, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='\\Gls{qq} plots comparing fitness-independent and fitness-based or dependent results for fidelity (left) and fitness (right).'>>=
 df_independent <- subset(df_completed,truncate=="Independent")
 df_dependent <- subset(df_completed,truncate=="Dependent")
 ap <- myqqplot(sort(df_independent$ave_cor), sort(df_dependent$ave_cor), main="Final mean fidelity", xlab="Independent",ylab="Dependent")
 bp <- myqqplot(sort(df_independent$ave_fit), sort(df_dependent$ave_fit), main="Final mean fitness", xlab="Independent",ylab="Dependent")
 grid.arrange(ap,bp,nrow=1,ncol=2,heights=unit(0.5, "npc"))
@

%Standard non-parametric tests include Wilcoxon and Mann-Whitney for comparing two
%independent continuous random samples where the underlying distributions
%are known to have essentially the same shape, or a Friedman test where the samples might be related (\eg in block %experiment designs) and the objective is to distinguish differences between treatments.

<<kstest, warning=FALSE, pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
x_cor <- ks.test(df_independent$ave_cor, df_dependent$ave_cor)
x_fit <- ks.test(df_independent$ave_fit, df_dependent$ave_fit)
@

Applying a two-sample Kolmogorov-Smirnov test to determine if the results for each method are taken from the same distribution provides strong evidence (for population mean fidelity, approx. p-value=\Sexpr{round(x_cor$p.value,2)} and for pop. mean fitness, approx. p-value=\Sexpr{round(x_fit$p.value,2)}) to reject the null hypothesis, confirmed by visual inspection of \cref{fig:limitcomparisonplots} and \cref{fig:limitqqplots}. The distributions produced by the two methods are different for both average fidelity and average fitness.

From these tests it is clear that the two methods do not have statistically similar effects for both fidelity and fitness, and hence we cannot find support for the contention that our conclusions in the hypothesis tests can be made without extensive consideration of limit method. Method is important, and our conclusions are dependent on the method chosen.

This conclusion of course is based on a comparison of two particular methods; a third, fitness-based, method may produce distributions that are statistically similar to those of, say, sampling. In which case we might conclude that any conclusions drawn on the basis of sampling would also extend to this third method. This extension however is left for future work.

Returning to our goal, as the choice of method affects our results and for practical reasons we must make a choice, it seems reasonable to choose a method that has as small an influence as possible on the conclusions we draw from our hypothesis tests. As our tests involve fitness, a method that is by design orthogonal to fitness is the better choice. A sampling method is a better fit for this than a selection method, which biases on fitness, and therefore we adopt fitness-independent sampling rather than fitness-based restriction.

\section[Lower size bound]{Lower limit on population size}\label{lower-size-limit}

% EXPERIMENT - Lower size bound

<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results.simple('results/results-53f6b74.data'), environment_change_frequency == 0 & truncate==0)
@

\begin{table} % 53f6b74
	\begin{center}
		\caption{Factor levels for the choice of a lower-bound mechanism}
		\label{tbl:factor-levels-for-the-choice-of-a-lower-bound-mechanism}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       	4&	0 or 0.33 or 0.66 or 1.0\\
			$p_{selection}$&          	4&	0 or 0.33 or 0.66 or 1.0\\
			$n_{children}$&           	2&	2 or 5\\
			Population Restriction&  	2&	Fitness-independent sampling, or Fitness-based selection\\
			Distribution&               2&	Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$\\
			Correlate Fitness&        	2&	false or true\\
			Correlate Fidelity&       	2&	false or true\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

We now turn to the other bound, and the impact of population extinctions. Runs in which the population dies out are incompatible with ongoing evolution, and so should be separated from other, sustainable, runs in the analysis. This situation does arise in our simulation: not all runs in the experiment described in \ref{upper-size-bound} completed the target of \Sexpr{max(df['gen'])} generations (\cref{fig:popoverall}), and of those that did, not all completed with a sustainable population size.

<<popoverall, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Not all runs in fixed conditions can maintain a substantial population size over time. The horizontal line at the top of the figure is from runs that were capped by the population upper-bound, while the great majority of unsustainable runs fall within the asymptotically decreasing band from top-left to bottom-right. Note the extreme case represented by the almost vertical set of points to the far left, discussed in the text.'>>=
ggplot(df) + geom_point(aes(x=gen,y=pop),size=0.25) + labs(x="Generation", y="Population size")
@

Of the \Sexpr{nrow(subset(df,gen==0))} experiment runs, \Sexpr{nrow(subset(df,gen==0))-nrow(subset(df, gen==500))} fail to reach completion, and \Sexpr{english(nrow(subset(df, gen==500 & pop < 4000)))} complete with a population size substantially smaller ($\leq$ \Sexpr{max(subset(df, gen==500 & pop < 4000)['pop'])}) than the population size in the other completed runs. In fact, all other runs reach completion with a population size at the upper bound limit of \Sexpr{min(subset(df, gen==500 & pop >=4000)['pop'])}.

From the clustering evident in \ref{fig:popoverall} we assert that the experiment runs can be separated into two disjoint sets - those that reach completion with a population size at or very near the upper-size limit, and those whose population is tending towards zero. Our interest lies only in sustainable populations and therefore from here on we exclude all runs where the population does not reach completion at or near the upper-size limit, without loss of generality.

%Inspecting the results by factor:
%\begin{enumerate}
%	\item All runs with $p_{reproduction}=0.33$ and $n_{children}=2$ fail to complete.
%	\item All runs that complete, but with an unsustainably small population, also include $p_{selection}=0.66$.
%\end{enumerate}
%
%To conclude this brief examination of lower-bounds, runs with factors associated with population extinction ($p_{reproduction}=0.33$, $n_{children}=2$ and $p_{selection}=0.66$) will be omitted from further analysis.

\TODO{explanation for extreme left case}

Note that these results also provide support for an experiment duration of \Sexpr{max(df['gen'])} generations - either a run maintains a stable population (and so the duration is unrelated to population size considerations), or if a population is to go extinct it does so by \Sexpr{max(df['gen'])} generations or soon after and so can easily be identified for special treatment.

\section{Number of offspring}
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results.simple('results/results-53f6b74.data'), environment_change_frequency == 0)
@

\begin{table} % 53f6b74
	\begin{center}
		\caption{Factor levels for the investigation into number of offspring}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       	4&	0 or 0.33 or 0.66 or 1.0\\
			$p_{selection}$&          	4&	0 or 0.33 or 0.66 or 1.0\\
			$n_{children}$&           	2&	2 or 5\\
			Population Restriction&  	2&	Fitness-independent sampling, or Fitness-based selection\\
			Distribution&               2&	Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$\\
			Correlate Fitness&        	2&	false or true\\
			Correlate Fidelity&       	2&	false or true\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<popoffspring, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Population size over time for two values of parameter $n_{children}$ - 2 (on the left) and 5 (on the right) .'>>=
temp <- subset(df,n_offspring==1 & pop < 5000)
df$n_offspring = factor(df$n_offspring, labels=c("2","5"))
ggplot(df) + geom_point(aes(x=gen,y=pop), size=0.25) + labs(x="Generation", y="Population size") + facet_wrap(~n_offspring)
@

The vertical line on the extreme left of the right-hand side facet of \cref{fig:popoffspring} (for $n_{offspring} = 5$) consists of \Sexpr{nrow(subset(temp,gen==1))} runs from a total of nrow(subset(df,gen==0)) where the population dropped in the first generation to a mean size of \Sexpr{mean(subset(temp,gen==2)$pop)}, before recovering to the population upper limit by generation \Sexpr{max(temp['gen'])+1}. This is a characteristic of the initial conditions: runs where $n_{offspring} = 2$ and the probability of reproduction or selection is low, either because the parameter has a low, fixed, value or because they are derived from the parent's value, suffer high selection and low reproduction initially. Taking the case where both are given by the parent's value,  the initial values are defined by the low-start case and $p_{reproduction} = p_{selection} = $\Sexpr{mean(subset(df,gen==0)$ave_fit)}. The difference between the two facets can be explained by the higher reproduction rate in the right-hand facet.

A similar effect can be seen in the left-hand side facet for $n_{offspring} = 2$ where some of the affected runs recovered while others went to extinction.

\section{Fidelity correlation}

\emph{Fidelity correlation} is parameterized in order to test the hypothesis that correlation is required for inheritance; the alternative hypothesis that inheritance can arise without correlation corresponds to \emph{Fidelity correlation} being false.

\section{Derive parameter}\label{screening-distribution}
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results.simple('results/results-53f6b74.data'), environment_change_frequency == 0)
@

\begin{table} % 53f6b74
	\begin{center}
		\caption{Factor levels for the investigation of Derive}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       	4&	0 or 0.33 or 0.66 or 1.0\\
			$p_{selection}$&          	4&	0 or 0.33 or 0.66 or 1.0\\
			$n_{children}$&           	2&	2 or 5\\
			Population Restriction&  	2&	Fitness-independent sampling, or Fitness-based selection\\
			Distribution&               2&	Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$\\
			Correlate Fitness&        	2&	false or true\\
			Correlate Fidelity&       	2&	false or true\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<gaussian, pdfcrop=TRUE, echo=FALSE, cache=TRUE, out.width='0.45\\linewidth',fig.show='hold'>>=
df$distribution = factor(df$distribution, labels=c('Gaussian','Uniform'))
ggplot(df) + geom_boxplot(aes(x=distribution,y=ave_cor)) + labs(x="Distribution", y="Final mean fidelity")
ggplot(df) + geom_boxplot(aes(x=distribution,y=ave_fit)) + labs(x="Distribution", y="Final mean fitness")
@

The \emph{Derive} parameter describes a function to return a value in the range $[0,1]$ given two parameters, also in the range $[0,1]$. Obvious candidates for \emph{Derive} include probability distributions, where the function samples a value from a distribution. If the distribution is gaussian, the two parameters to \emph{Derive} would describe its mean and variance.  

From a visual inspection, we conclude that the results appear very similar for Uniform and Gaussian distributions.

\section{Conclusions}

Independent variable is Fidelity correlation, the main factor of interest from hypothesis

Dependent variables, to qualify the sensitivity of the model:
\begin{enumerate}
	\item Distribution
	\item Number of offspring
\end{enumerate}

Set remaining parameters based on these screening experiments:
\begin{enumerate}
	\item Upper-size bound - sampling % truncate == 0
	\item Lower-size bound - sustainable populations % gen for run == max gen, and pop at max gen > 1000
	\item Probabilities - at least one of $p_{reproduction}$ and $p_{selection}$ is related to the parent's fitness (\ie the factor level has the special case value of 0.) % 
	
% All runs that complete - that is, where gen == max(df$gen) & pop > 1000 or subset(df,gen==500 & pop>1000)
% runs <- subset(df, gen==max(df$gen) & pop>1000)[,'run']
% subset(df, truncate == 0 & run %in% runs & (p_reproduce==0 | p_selection == 0))
% Use subset(df,correlation_correlation==1) by hypothesis, correlation_correlation==0 is control
	
\end{enumerate}

\chapter[Test under fixed conditions]{Experimental test of hypothesis under fixed conditions}\label{experimental-test-of-h2-under-fixed-conditions}

Returning to the overall goals for these experiments (to test the predictions of hypothesis \autoref{hypothesis-2}, and to examine the impact of the factors on the results), hypothesis \autoref{hypothesis-2} makes two predictions for fixed environments:

\begin{enumerate}
	\item Average inheritance will tend towards perfect inheritance, and
	\item Population variance for inheritance will decrease more than chance.
\end{enumerate}

The low-start case is the most significant as it describes the core of the thesis - that good quality inheritance can develop from imperfect beginnings, such as could occur in the emergence of artificial evolution, or indeed as might be found during the transition from non-living to life. This makes it a key step along the way towards the evolution of artifical evolution, the main subject of this thesis.

The first test therefore is to examine if inheritance emerges from low-fidelity and low-fitness initial conditions \cref{inheritance-low-start} and confirm that it is maintained under high-start conditions \cref{inheritance-high-start}, and then finally test whether the population variance for inheritance decreases as predicted \cref{sd-low-start}.

\section{Emergence of inheritance from low-start initial conditions}\label{inheritance-low-start}

As discussed earlier in \cref{synthesis-from-a-general-evolutionary-model}, inheritance is the outcome of the relationship between parent and child traits, as represented by the measure of fidelity.

% EXPERIMENT 3 - Fidelity approaches 1.0

We start with the following null and alternative hypotheses:

\begin{itemize}[label={}]
	\item H$_0$: fidelity does not approach 1.0 during a run, irrespective of factor values, or \newline
	$\vert \overline{fidelity}_{end}-\overline{fidelity}_{start} \vert = 0$
	\item H$_1$: fidelity increases to near 1.0 during a run, for some factor values, or \newline
	$\overline{fidelity}_{end}-\overline{fidelity}_{start} > 0$ and $1.0-\overline{fidelity}_{end} < \delta$ for some $\delta$ and for some factor values.
\end{itemize}

\subsection{Response variables}\label{response-variables}

From the predictions of the hypothesis (\cref{predictions}), the main property of interest is \emph{fidelity}, or the correlation between parent and child property values. \emph{Fidelity} therefore is our response variable.
\\

Specifically we use $\overline{fidelity}_{end}$, or the mean value for \emph{fidelity} (across all replicates) at the end of a run, as under hypothesis \autoref{hypothesis-2} we expect $\overline{fidelity}_{end}$ to approach 1.0 in an unchanging environment.

\subsection{Design}\label{design}

<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df_full <- load.results('results/results-819350e.data')
df <- subset(df_full, ecf==0 & truncate == 0 & (p_reproduce==0 || p_selection == 0))
@

\begin{table} % 5e33a27 and 819350e
	\begin{center}
		\caption{Factor levels for testing the hypothesis prediction of perfect inheritance in unchanging conditions}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       		2&	0 or 0.66\\
			$p_{selection}$&          		2&	0 or 0.66\\
			$n_{children}$&           		2&	2 or 5\\
			Population Restriction&  		2&	Fitness-independent sampling or fitness-based truncation\\
			Distribution&               	2&	Gaussian dist., $\mathbb{N}$ or folded Gaussian\\
			Correlate Fidelity&       		2&	false or true\\
			Shape of environment change		2&	Abrupt or continuous\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<lowstart, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Summary results for low-start case, showing on the left-hand side an overview for the final mean fidelity ($\\overline{fidelity}_{end}$) for each run and a density plot showing the distribution of $\\overline{fidelity}_{end}$. The right-hand side shows the corresponding plots for final mean fitness ($\\overline{fitness}_{end}$)'>>=
ap <- qplot(row.names(df),ave_cor, geom="point", data=df, xlab='Experiment run',ylab="Final ave. fidelity") + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
bp <- qplot(row.names(df),ave_fit, geom="point", data=df, xlab='Experiment run',ylab="Final ave. fitness") + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
cp <- qplot(ave_cor, geom="density", data=df, xlab="Final ave. fidelity",ylab="")
dp <- qplot(ave_fit, geom="density", data=df, xlab="Final ave. fitness",ylab="")
grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
@

<<lowstartbyfactor, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Summary for low-start, this time averaged across all replicates of each factor combination (that is, grouped by replicate). Dark-coloured points represent values at end of run; light-coloured points are for initial values. Missing points are from those replicates that did not complete.'>>=
ap <- ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=ave_cor, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='Factor combination', y='Mean fidelity') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
bp <- ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=ave_fit, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='Factor combination', y='Mean fitness') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
cp <- ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=sd_cor, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='', y='SD fidelity') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
dp <- ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=sd_fit, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='', y='SD fitness') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
@

<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results('results/results-5e33a27.data'), ecf==0 & truncate == 0 & (p_reproduce==0 || p_selection == 0))
@

<<highstart, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Summary results for high-start case, showing on the left-hand side an overview for the final mean fidelity ($\\overline{fidelity}_{end}$) for each run and a density plot showing the distribution of $\\overline{fidelity}_{end}$. The right-hand side shows the corresponding plots for final mean fitness ($\\overline{fitness}_{end}$)'>>=
ap <- qplot(row.names(df),ave_cor, geom="point", data=df, xlab='Experiment run',ylab="Final mean fidelity") + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
bp <- qplot(row.names(df),ave_fit, geom="point", data=df, xlab='Experiment run',ylab="Final mean fitness") + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
cp <- qplot(ave_cor, geom="density", data=df, xlab="Final ave. fidelity",ylab="")
dp <- qplot(ave_fit, geom="density", data=df, xlab="Final ave. fitness",ylab="")
grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
@

<<highstartbyfactor, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Summary for high-start, averaged across all replicates for each combination of factors, as seen earlier in the low-start case. Light-coloured points for initial state and dark-coloured ones for final values.Missing points are from replicates that did not complete.'>>=
ap <-  ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=ave_cor, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='Factor combination', y='Mean fidelity') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
bp <-  ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=ave_fit, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='Factor combination', y='Mean fitness') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
cp <-  ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=sd_cor, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='', y='SD fidelity') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
dp <-  ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=sd_fit, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='', y='SD fitness') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
@

\subsection{Results and discussion}

Our interest is in the final values for fidelity under fixed-environment, low-start, conditions. Therefore, data is from the final generation of those fixed-environment runs that reached completion at generation \Sexpr{max(df['gen'])} from the low-start dataset; a total of \Sexpr{nrow(unique(df['run']))} runs out of the full dataset of \Sexpr{nrow(unique(df_full['run']))} runs.

A simple visual inspection of this data in \cref{fig:lowstart} reveals that the fidelity measure is distinctly bimodal, with peaks around final mean fidelity values 0.5--0.6 and 1.0. Fitness is also bimodal, but less so than fidelity. From inspection, it seems clear that fidelity does approach 1.0 for some combination of factor levels.

<<lowstartfactors, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Comparison between correlated and uncorrelated factors in the low-start case for the factor \\textbf{Correlate Fidelity} showing results for final mean fidelity ($\\overline{fidelity}_{end}$, on the left) and final mean fitness ($\\overline{fitness}_{end}$, on right)'>>=
df$correlation_correlation <- factor(df$correlation_correlation, labels=c('false','true'))
ap <- ggplot(df) + geom_boxplot(aes(x=correlation_correlation,y=ave_cor)) + labs(x='Final mean fidelity',y='')
bp <- ggplot(df) + geom_boxplot(aes(x=correlation_correlation,y=ave_fit)) + labs(x='Final mean fitness',y='')
grid.arrange(ap,bp,nrow=1,ncol=2)
@

From \cref{fig:lowstartbyfactor}, all runs result in an increase in fidelity from the initial range of $[0, 0.3]$ but only some approach or reach 1.0. Those that do are uniformly associated with the \textbf{Correlate Fidelity} factor value of \emph{true} (see top-left plot in \cref{fig:lowstartfactors}), and those that did not had a \textbf{Correlate Fidelity} value of -1.

In conclusion, H$_0$ can be rejected, and H$_1$ accepted. Inheritance increases regardless of the model design, but is strongest when \textbf{Correlate Fidelity} is \emph{true}, that is, when the child's fidelity is correlated with that of its parent.

\section{Confirmation of inheritance under high-start conditions}\label{inheritance-high-start}

\TODO{Experiment settings}

% EXPERIMENT 4 - Inheritance under high-start conditions
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results('results/results-5e33a27.data'), ecf==0 & truncate == 0 & (p_reproduce==0 || p_selection == 0))
@

\begin{table} % 5e33a27 and 819350e
	\begin{center}
		\caption{Factor levels for confirmation of hypothesis prediction under high-start initial conditions}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       		2&	0 or 0.66\\
			$p_{selection}$&          		2&	0 or 0.66\\
			$n_{children}$&           		2&	2 or 5\\
			Population Restriction&  		2&	Fitness-independent sampling or fitness-based truncation\\
			Distribution&               	2&	Gaussian dist., $\mathbb{N}$ or folded Gaussian\\
			Correlate Fidelity&       		2&	false or true\\
			Shape of environment change		2&	Abrupt or continuous\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<highstartfactors, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Comparison between correlated and uncorrelated factors in the high-start case for the factor \\textbf{Correlate Fidelity}. As before, results for final mean fidelity ($\\overline{fidelity}_{end}$) are on the left, and for final mean fitness ($\\overline{fitness}_{end}$) on the right.'>>=
df$correlation_correlation <- factor(df$correlation_correlation, labels=c('false','true'))
ap <- ggplot(df) + geom_boxplot(aes(x=correlation_correlation,y=ave_cor)) + labs(x='Final mean fidelity',y='')
bp <- ggplot(df) + geom_boxplot(aes(x=correlation_correlation,y=ave_fit)) + labs(x='Final mean fitness',y='')
grid.arrange(ap,bp,nrow=1,ncol=2)
@

\section{Variance of Fidelity reduces under low-start conditions}\label{sd-low-start}

% EXPERIMENT 5 - SD of Fidelity approaches 0

The second prediction of the hypothesis is that population variance for inheritance ($\sigma_{fidelity}$) should decrease over time towards a limit of $0$ in fixed environments.

\begin{itemize}[label={}]
	\item H$_0$: $\sigma_{fidelity_{end}}-\sigma_{fidelity_{start}} >= 0$, for all factor values.
	\item H$_1$: $\sigma_{fidelity_{end}}-\sigma_{fidelity_{start}} < 0$, for some factor values.
\end{itemize}

We use the same experimental setup and data as for the first hypothesis test in \cref{tbl:factor-levels-for-investigation-into-inheritance-under-low-start-conditions}.

<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df_full <- load.results('results/results-819350e.data')
df <- subset(df_full, ecf==0 & truncate == 0 & (p_reproduce==0 || p_selection == 0))
@

\begin{table} % 5e33a27 and 819350e
	\begin{center}
		\caption{Factor levels for testing the hypothesis prediction that the population variance of inheritance should reduce to nothing in unchanging environments}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       		2&	0 or 0.66\\
			$p_{selection}$&          		2&	0 or 0.66\\
			$n_{children}$&           		2&	2 or 5\\
			Population Restriction&  		2&	Fitness-independent sampling or fitness-based truncation\\
			Distribution&               	2&	Gaussian dist., $\mathbb{N}$ or folded Gaussian\\
			Correlate Fidelity&       		2&	false or true\\
			Shape of environment change		2&	Abrupt or continuous\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\subsection{Results and discussion}

<<lowstartranges1, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Smoothed ranges over time of the standard deviation of fidelity (top) and fitness (bottom) for all levels of all factors, broken out by level of \\emph{Correlate Fidelity}. Lines that drop below zero are an artifact of the line smoothing algorithm used (local polynomial regression fitting).'>>=
ap <- ggplot(df,aes(x=gen,y=sd_cor,group=run, colour=factor(correlation_correlation))) + scale_color_manual(name="Correlate\nFidelity", values=c("grey80", "grey10"), labels=c("false","true")) + stat_smooth() + labs(x="Generation", y="Standard Deviation of Fidelity") + theme(legend.key=element_rect(fill="white"))
bp <- ggplot(df,aes(x=gen,y=sd_fit,group=run,colour=factor(correlation_correlation))) + scale_color_manual(name="Correlate\nFidelity", values=c("grey80", "grey10"), labels=c("false","true")) + stat_smooth() + labs(x="Generation", y="Standard Deviation of Fitness") + theme(legend.key=element_rect(fill="white"))
grid.arrange(ap,bp,nrow=2,ncol=1)
@

<<lowstartranges2, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Smoothed ranges for the mean rather than the standard deviation - mean fidelity on the top row, mean fitness below - again by \\emph{Correlate Fidelity}.'>>=
cp <- ggplot(df,aes(x=gen,y=ave_cor,group=run, colour=factor(correlation_correlation))) + scale_color_manual(name="Correlate\nFidelity", values=c("grey80", "grey10"), labels=c("false","true")) + stat_smooth() + labs(x="Generation", y="Mean Fidelity") + theme(legend.key=element_rect(fill="white"))
dp <- ggplot(df,aes(x=gen,y=ave_fit,group=run, colour=factor(correlation_correlation))) + scale_color_manual(name="Correlate\nFidelity", values=c("grey80", "grey10"), labels=c("false","true")) + stat_smooth() + labs(x="Generation", y="Mean Fitness") + theme(legend.key=element_rect(fill="white"))
grid.arrange(cp,dp,nrow=2,ncol=1)
@

Data is all generations for those fixed-environment runs that reached completion with a 'sustainable' population (see \cref{lower-size-limit}) at generation \Sexpr{max(df['gen'])}; \Sexpr{nrow(unique(df['run']))} runs out of the full dataset of \Sexpr{nrow(unique(df_full['run']))} runs.

%<<lowstartranges2, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Range of 25\\% to 75\\% quartiles for \\gls{sd} for \\emph{true} setting for factor \\textbf{Correlate Fidelity}'>>=
%z<-by(df,df$gen,function(x) {aggregate(x$sd_cor ~ x$correlation_correlation, data=x, quantile)}) # quantiles aggregated by correlation_correlation
%z1<-apply(z,1,function(x){c(x[[1]][[2]][[2,2]],x[[1]][[2]][[2,3]],x[[1]][[2]][[2,4]])}) # correlation_correlation == -1
%z1<-apply(z,1,function(x){c(x[[1]][[2]][[1,2]],x[[1]][[2]][[1,3]],x[[1]][[2]][[1,4]])}) # correlation_correlation == 1
%z2<-z1[1:3,1:501]
%z3<-as.data.frame(cbind(1:501,z2[1,],z2[2,],z2[3,]))
%ggplot(z3,aes(V1,V2,V3)) + geom_ribbon(data=z3,aes(ymin=V2,ymax=V4), alpha = 0.2) + geom_line(aes(V1,V3))
%@

\section{Confirmation that variance decreases under high-start conditions}

% EXPERIMENT 6 - SD of Fidelity approaches 0

<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results('results/results-5e33a27.data'), ecf==0 & truncate == 0 & (p_reproduce==0 || p_selection == 0))
@

\begin{table} % 5e33a27 and 819350e
	\begin{center}
		\caption{Factor levels for confirmation that variance also decreases under high-start conditions}
		\label{Factor-levels-for-investigation-into-variance-under-high-start-conditions}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       		2&	0 or 0.66\\
			$p_{selection}$&          		2&	0 or 0.66\\
			$n_{children}$&           		2&	2 or 5\\
			Population Restriction&  		2&	Fitness-independent sampling or fitness-based truncation\\
			Distribution&               	2&	Gaussian dist., $\mathbb{N}$ or folded Gaussian\\
			Correlate Fidelity&       		2&	false or true\\
			Shape of environment change		2&	Abrupt or continuous\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\subsection{Response variables}

The main response variable is the standard deviation of fidelity.

\subsection{Initial conditions}

Initial conditions are given in \cref{Factor-levels-for-investigation-into-variance-under-high-start-conditions}.

\subsection{Results and discussion}

<<ExperimentVarianceUnderHighStart, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='High start case: smoothed ranges for standard deviation of fidelity, as before grouped by \\emph{Correlate Fidelity}.'>>=
ggplot(df,aes(x=gen,y=sd_cor,group=run, colour=factor(correlation_correlation))) + scale_color_manual(name="Correlate\nFidelity", values=c("grey80", "grey10"), labels=c("false","true")) + stat_smooth() + labs(x="Generation", y="Standard Deviation of Fidelity") + theme(legend.key=element_rect(fill="white"))
@

%\section{Factor interactions}
%% EXPERIMENT 7 - Factor significance
%% Dataset: 2860d6fe
%
%<<ExperimentFactorInteractions, pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
%temp <- load.results('results/results-819350e.data')
%temp <- subset(temp, environment_change_frequency == 0 & truncate==0 & correlation_correlation==1)
%completing_factors <- unique(interaction(Filter(is.factor,temp[temp$gen==500,]))) # the factor levels that resulted in 500 generations
%df <- temp[interaction(Filter(is.factor,temp)) %in% completing_factors,] # raw data in long format - all low-start, fixed
%@
%
%<<lowstartfactorinfluence, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Box-plots for the effect of selected factors on final mean fidelity.'>>=
%#m0 <- lm(ave_cor~(p_reproduce+p_selection+n_offspring+distribution+fitness_correlation)^2, data=df)
%#x <- anova(m0)
%df$p_reproduce <- factor(df$p_reproduce,labels=c(0,0.33,0.66,1.0))
%df$p_selection <- factor(df$p_selection,labels=c(0,0.33,0.66,1.0))
%df$n_offspring <- factor(df$n_offspring,labels=c(2,5))
%df$distribution <- factor(df$distribution, labels=c("Gaussian","Uniform"))
%ap <- ggplot(df) + geom_boxplot(aes(x=p_reproduce,y=ave_cor)) + labs(x="Reproduction",y="Final mean fidelity")
%bp <- ggplot(df) + geom_boxplot(aes(x=p_selection,y=ave_cor)) + labs(x="Selection",y="Final mean fidelity")
%cp <- ggplot(df) + geom_boxplot(aes(x=n_offspring,y=ave_cor)) + labs(x="Offspring",y="Final mean fidelity")
%dp <- ggplot(df) + geom_boxplot(aes(x=distribution,y=ave_cor)) + labs(x="Distribution",y="Final mean fidelity")
%grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
%@
%% significant with F = \Sexpr{round(x['F value'],2)} and p-value \textless{} \Sexpr{round(x['Pr(>F)'],2)}.
%
%Taking only runs where the factor \textbf{Correlate Fidelity} is \emph{true}, final mean fidelity is significantly influenced by all factors and by the first-level interactions between $p_{reproduction}$ and $n_{children}$, and $p_{reproduction}$ and $Distribution$.
%
%Examining this further, factor-by-factor, where \textbf{Correlate Fidelity} is \emph{true}, from \cref{fig:lowstartfactorinfluence} \TODO{complete}.
%
%\paragraph{Is the choice of distribution function significant?}\label{distribution-function-1}
%
%gaussian implies a stronger relationship, uniform a broader range and less correlation. Considerations--connection to other fields -> gaussian. But uniform would be worst case--good to know conclusions hold even under these conditions.
%
%\begin{itemize}[label={}]
%	\item H$_0$: $\overline{gaussian} = \overline{uniform}$
%	\item H$_1$: $\overline{gaussian} \ne \overline{uniform}$
%\end{itemize}
%
%<<distributionfunction, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Comparison of Gaussian and Uniform distributions for Factor Distribution.'>>=
%df$distribution <- factor(df$distribution, labels=c("Gaussian","Uniform"))
%ap <- ggplot(df) + geom_boxplot(aes(x=distribution, y=ave_cor)) + labs(x="",y="Final mean fidelity")
%bp <- ggplot(df) + geom_boxplot(aes(x=distribution, y=ave_fit)) + labs(x="",y="Final mean fitness")
%grid.arrange(ap,bp,nrow=1,ncol=2,heights=unit(0.5, "npc"))
%@

\chapter[Test under changing conditions]{Experimental test of hypothesis under changing conditions}\label{experimental-test-of-h2-under-changing-conditions}

Where inheritance is using \textbf{Correlate Fidelity} mechanism, expect
that under changing conditions correlation will tend to some lower value
than 1.0, proportional to variance in change (more change, lower
correlation).

Significance

\begin{itemize}
\item
  Support for overall hypothesis that variability derived from
  copying/inheritance mechanism, and that EvoEvo can tune this mechanism
  to suit conditions--no preselected parameters required
\item
  \gls{sd} of correlation should be greater than in a fixed environment
  (benefit to preserving variation)
\item
  An environment change results in a change in fitness for entities.
  Other elements in the entity initially unaffected
\end{itemize}

Same initial conditions (low \cref{low-start-case} and high start \cref{high-start-case} cases) with the addition to the Bounded Model \autoref{upper-bound-model-algorithm} as follows to introduce a changing environment by ``tweaking'' the fitness of every entity at some frequency, $frequency$.

\begin{algorithm}
	\SetKwFunction{f}{f}
	\For{generation $\in 1\dots$number of generations}{
		\tcp{Core algorithm}
		$population\leftarrow Selection(population)$\;
		$population\leftarrow population + Reproduction(population)$\;
		$population\leftarrow Restriction(population)$\;
		\BlankLine
		\tcp{Check if population size is no longer sustainable}
		\uIf{population size is too small}{
			break\;
		}
		\uIf{time to change environment}{
			$population\leftarrow tweakFitness(population)$\;
		}
	}
	\caption{Algorithm for the Changing Environment Model}\label{changingmodelalgorithm}
\end{algorithm}

\begin{function}
\SetKwFunction{f}{f}
\Def{tweakFitness(population, sd, meanOffset)}{
	$population_{new}\leftarrow \{\}$\;
	\For{each $element$ in $population$}{
		$fitness\leftarrow \text{ fitness of }element$\;
		$mean\leftarrow fitness-meanOffset$\;
		$population_{new}\leftarrow population_{new} + \text{ new Element with fitness } \f{mean, sd} \text{ and fidelity = fidelity of }element$\;
	}
	\Return $population_{new}$\;
}
\caption{TweakFitness()}
\end{function}

\section{Experimental test}\label{experimental-test}

Fluctuating environment with no trend or direction--modelled by applying the distribution function from parameters (i.e., either gaussian or uniform) with a possible change to the mean fitness each generation. In this model, an individual's fitness may increase or decrease.

\section{Experimental design}\label{experimental-design-8}

Four different runs, with changes to the environment at four different frequencies, but all with the same factor settings:

% EXPERIMENT 8 - SD of Fidelity approaches 0
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df_full <- load.results('results/results-819350e.data')
df <- subset(df_full, correlation_correlation == 1 & truncate == 0 & (p_reproduce==0 || p_selection == 0))
@

\begin{table} % 5e33a27 and 819350e
	\begin{center}
		\caption{Factor levels for test of the hypothesis prediction that the maximum possible fidelity is inversely proportional to the degree of environmental change}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       		2&	0 or 0.66\\
			$p_{selection}$&          		2&	0 or 0.66\\
			$n_{children}$&           		2&	2 or 5\\
			Population Restriction&  		2&	Fitness-independent sampling or fitness-based truncation\\
			Distribution&               	2&	Gaussian dist., $\mathbb{N}$ or folded Gaussian\\
			Correlate Fidelity&       		2&	false or true\\
			Shape of environment change		2&	Abrupt or continuous\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

Control--unchanging, but the other three runs with the change in environment, and so a call to $TweakFitness$, every 1, 5 or 10 generations.

The \gls{sd} of the environmental change was set to 0.90 (parameter \emph{sd} to \emph{TweakFitness} equal to 0.70), and \emph{meanOffset}, or the mean change to an individual's fitness as a result of the environmental change, to $-0.2$.

As before, each run has 10 replicates of \Sexpr{max(df['gen'])} generations, with an initial population size of \Sexpr{df[1,]['pop']} entities.

Data is as before from those runs that reached completion at generation \Sexpr{max(df['gen'])} from the low-start dataset; \Sexpr{nrow(unique(df['run']))} runs out of the full dataset of \Sexpr{nrow(unique(df_full['run']))} runs.

\section{Results and discussion}\label{results-8}

<<changing-sd-1, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Standard deviation of final mean fidelity over time for four different frequencies (0 or no change (bottom line), every generation (top line), every 5 generations (second from top), and every 10 generations (second from bottom)) of environmental change.'>>=
df$ecf <- factor(df$ecf,labels=c('Unchanging','Every generation','Every 5 generations','Every 10 generations'))
ggplot(subset(df,correlation_correlation==1)) + stat_smooth(aes(x=gen,y=sd_cor, group=ecf, colour=factor(ecf)), size=0.25) + scale_color_manual(name="Change frequency", values=c("grey70", "grey50", "grey30", "grey10")) + labs(x="Generation", y="SD Fidelity") + theme(legend.key=element_rect(fill="white"))
#ggplot(subset(df,correlation_correlation==1)) + stat_smooth(aes(x=gen,y=sd_fit, group=ecf, colour=factor(ecf)), size=0.25) + scale_color_manual(name="Change frequency", values=c("grey70", "grey50", "grey30", "grey10")) + labs(x="Generation", y="SD Fitness") + theme(legend.key=element_rect(fill="white"))
#grid.arrange(ap,bp,nrow=2,ncol=1)
@

<<changing-means-1, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='The same for the mean rather than the standard deviation - mean fidelity on the top row, mean fitness below.'>>=
ap <- ggplot(subset(df,correlation_correlation==1)) + stat_smooth(aes(x=gen,y=ave_cor,group=interaction(ecf), colour=factor(ecf))) + scale_color_manual(name="Change frequency", values=c("grey70", "grey50", "grey30", "grey10")) + labs(x="Generation", y="Mean Fidelity") + theme(legend.key=element_rect(fill="white"))
bp <- ggplot(subset(df,correlation_correlation==1)) + stat_smooth(aes(x=gen,y=ave_fit,group=interaction(ecf), colour=factor(ecf))) + scale_color_manual(name="Change frequency", values=c("grey70", "grey50", "grey30", "grey10")) + labs(x="Generation", y="Mean Fitness") + theme(legend.key=element_rect(fill="white"))
grid.arrange(ap,bp,nrow=2,ncol=1)
@

<<changing-sd-2, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Standard deviation of fidelity over time by run for four different frequencies (0 or no change, every generation, every 5 generations, and every 10 generations) of environmental change.'>>=
ggplot(subset(df,correlation_correlation==1)) + geom_line(aes(x=gen,y=sd_cor, group=run)) + facet_wrap(~ecf) + labs(x="Generation", y="SD Fidelity") + coord_cartesian(ylim=c(0,0.5))
@
%
%<<changing-means-2, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Mean fitness (top) and mean fidelity (bottom) over time for four different frequencies (0 or no change, every generation, every 5 generations, and every 10 generations) of environmental change.'>>=
%ap <- ggplot(subset(df,correlation_correlation==1)) + geom_point(aes(x=gen,y=ave_fit), size=0.25) + facet_wrap(~ecf, ncol=2)  + labs(x="Generation", y="Mean Fitness") + coord_cartesian(ylim=c(0,1.1))
%bp <- ggplot(subset(df,correlation_correlation==1)) + geom_point(aes(x=gen,y=ave_cor), size=0.25) + facet_wrap(~ecf, ncol=2)  + labs(x="Generation", y="Mean Fidelity") + coord_cartesian(ylim=c(0,1.1))
%grid.arrange(ap,bp,nrow=2,ncol=1)
%@

We expect the variance of fidelity to remain at some non-zero level in changing environments, with greater change reflected in greater variance. This is partially seen in the results with a clear result for the most rapid change (every generation) and less clear for less frequent change.

Does this represent a genuine difficulty with the hypothesis, or is there an alternative explanation?

Of the conceivable alternatives, three appear worthy of further investigation:

\begin{enumerate}
	\item The type of environmental change presented by the current $TweakFitness$ algorithm does not meet the precondition of a changing environment; other forms of change though might show the hypothesised behaviour.
	\item The shape of the distribution function (in Derive) is biasing results towards the outer limits of the range.
	\item The fitness-independent sampling mechanism chosen to provide an upper bound on population size is preserving a number of low-fitness entities that materially alter the model.
\end{enumerate}

\subsection{Effect of form of environment change}

Our initial analysis was with the the \gls{sd} of the environmental change set to 0.90 (parameter $sd$ to $TweakFitness$ set to 0.90), and $meanOffset$, or the mean change to an individual's fitness as a result of the environmental change, to $-0.2$.

Are the results specific to this distribution? Or can we extend our claim across a broader range of cases?

We initially tested this by making a single change, setting $sd$ to 0.70.

Previous experiments used an abrupt change in fitness at a specified frequency. This alternative instead uses a constant, smaller, change to simulate a constant selective pressure. \TODO{Is this what the hypothesis means though? Does it mean uncertainty/unpredictable change, or predictable - this model?} In \ref{fig:summary-abrupt-change} we see the mean and standard deviations for fidelity for the original abrupt form of environmental change; in \ref{fig:summary-constant-change} the same for a steady selective pressure.

<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results('results/results-819350e.data'),correlation_correlation == 1 & truncate == 0 & (p_reproduce==0 || p_selection == 0))
df$ecf <- factor(df$ecf,labels=c('Unchanging','Every generation','Every 5 generations','Every 10 generations'))
@

\begin{table} % 5e33a27 and 819350e
	\begin{center}
		\caption{Factor levels for investigation into the effect of the form of environmental change}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       		2&	0 or 0.66\\
			$p_{selection}$&          		2&	0 or 0.66\\
			$n_{children}$&           		2&	2 or 5\\
			Population Restriction&  		2&	Fitness-independent sampling or fitness-based truncation\\
			Distribution&               	2&	Gaussian dist., $\mathbb{N}$ or folded Gaussian\\
			Correlate Fidelity&       		2&	false or true\\
			Shape of environment change		2&	Abrupt or continuous\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<summary-abrupt-change, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Mean average fidelity (upper) and standard deviation of fidelity (lower) for scenario of abrupt environmental change.'>>=
ap<-ggplot(subset(df,shape==0)) + geom_line(aes(gen,ave_cor,group=run)) + facet_wrap(~ecf)
bp<-ggplot(subset(df,shape==0)) + geom_line(aes(gen,sd_cor,group=run)) + facet_wrap(~ecf)
grid.arrange(ap,bp,nrow=2,ncol=1)
@

<<summary-constant-change, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Constant environmental change.'>>=
ap<-ggplot(subset(df,shape==1)) + geom_line(aes(gen,ave_cor,group=run)) + facet_wrap(~ecf)
bp<-ggplot(subset(df,shape==1)) + geom_line(aes(gen,sd_cor,group=run)) + facet_wrap(~ecf)
grid.arrange(ap,bp,nrow=2,ncol=1)
@

\TODO{Appears to be a difference in response between the two different forms of environmental change.}

\subsection{Effect of the specific distribution produced by Derive parameter}\label{alternative-distribution}

\emph{Derive}, as described earlier in \ref{screening-distribution}, produces a value in the range $[0,1]$; where the underlying method though isn't inherently limited to this range, such as is the case for the gaussian distribution, then the mechanism used to map the underlying raw distribution to the restricted range may modify the distribution of the mapped values.

Specifically, in the experiments described above, one version (level) of the distribution factor used the following method to map the raw gaussian distribution to the range $[0,1]$:

max(0,min(1,gaussian(mean,variance)))

<<example_distributions, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Effect of folding to range $[0,1]$ of gaussian distribution with mean = 0.5, standard deviation = 0.5'>>=
x <- rnorm(n=10000, mean=0.5, sd=0.5)
ap <- qplot(x,geom='density') + coord_cartesian(xlim=c(0,1),ylim=c(0,1.6)) + labs(x="Normal distribution")
bp <- qplot(sapply(sapply(x,min,1.0),max,0),geom='density') + coord_cartesian(xlim=c(0,1),ylim=c(0,1.6)) + labs(x="Folded normal")

x1 <- replicate(10000, {
  repeat {
        x <- rnorm(n=1,mean=0.5, sd=0.5)
        if (x >=0 & x<=1)
          break
  }
  x
})
cp <- qplot(x1,geom='density') + coord_cartesian(xlim=c(0,1),ylim=c(0,1.6)) + labs(x="Sampled normal")
grid.arrange(ap,bp,cp,nrow=1,ncol=3)
@

This produces a distribution with fat tails (middle of \cref{fig:example_distributions}), like a 'W', rather than the expected elongated 'U' of the standard gaussian (on the left-hand side of the same figure), and as \emph{Derive} is used in \ref{base-model-algorithm} describes the relationship between the properties of an entity and its offspring, it seems plausible that this distortion towards the extremes of the range may have a significant effect on the experimental result.
	
To test this hypothesis, we compare the results obtained with the folding algorithm described above to those from a non-folding implementation (in ...) which produces the distribution shown on the right-hand side of \cref{fig:example_distributions}.

<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results('results/results-819350e.data'), (p_reproduce==0 || p_selection == 0))
df$ecf <- factor(df$ecf,labels=c('Unchanging','Every generation','Every 5 generations','Every 10 generations'))
df$truncate=factor(df$truncate,labels=c("Fitness-independent","Fitness-based"))
df$correlation_correlation=factor(df$correlation_correlation,labels=c("Uncorrelated","Correlated"))
@

\begin{table} % 5e33a27 and 819350e
	\begin{center}
		\caption{Factor levels for investigation of the effect of the form of Derive}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter&  Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$&       		2&	0 or 0.66\\
			$p_{selection}$&          		2&	0 or 0.66\\
			$n_{children}$&           		2&	2 or 5\\
			Population Restriction&  		2&	Fitness-independent sampling or fitness-based truncation\\
			Distribution&               	2&	Gaussian dist., $\mathbb{N}$ or folded Gaussian\\
			Correlate Fidelity&       		2&	false or true\\
			Shape of environment change		2&	Abrupt or continuous\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<distributiontest1, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Effect of Correlate Fidelity and Truncate parameters upon standard deviation of fidelity.'>>=
# test is if distribution affects sd_cor
ggplot(df) + geom_line(aes(gen,sd_cor,group=run)) + facet_wrap(~correlation_correlation+truncate)
@

% All runs where sd_cor remains > 0.3 and ave_cor ~ 0.75 for ecf==0 and correlation_correlation==1:
% ggplot(subset(df,correlation_correlation==1&ecf==0&distribution==0&shape==1&truncate==1)) + geom_line(aes(x=gen,sd_cor,group=run))
% But why?

%
%<<distributiontest2, pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
%ap <- ggplot(subset(df,distribution==1)) + geom_line(aes(gen,ave_cor,group=run)) + facet_wrap(~ecf)
%bp <- ggplot(subset(df,distribution==1)) + geom_line(aes(gen,sd_cor,group=run)) + facet_wrap(~ecf)
%grid.arrange(ap,bp,nrow=2,ncol=1)
%@

From the figure, runs in which algorithm X is used result in the standard deviation of fidelity tending towards $0$, while runs with algorithm Y show a more nuanced result. Incidentally from ... it's clear that the unchanging line for the standard deviation of fidelity under algorithm X is exclusively associated with an unchanging environment, as expected.

Fitness ranges as expected, with a higher mean fitness under less changeable conditions. The range of fidelities follows a similar pattern, however mean fidelity approaches $1.0$ under all frequencies other than when the environment changes every generation. 

\subsection{Effect of population restriction mechanism}

\TODO{complete this subsection}

% To see difference between the two truncation mechanisms:
%t <- subset(df,ecf=='Every 5 generations' | ecf=='Every 10 generations')
%ggplot(subset(t,correlation_correlation=='Correlated')) + geom_line(aes(x=gen,y=sd_cor,group=run)) + facet_wrap(~ecf+truncate)
% But the hypothesis is that low-fitness entities are being preserved (and how does this relate to fidelity)?
% First, how do low-fitness entitites affect sd_cor?  Prediction is that will have some effect on sd_cor..
% Second, are there more low-fitness entities under fitness-independent sampling?
% Test is that if we see effect on sd_cor, then should see more low-fitness entities...do we? If we don't then some other mechanism. If we do, then possible explanation.


% Hypothesis is that truncation mechanism affects the degree of sd_cor by altering the ratio of high to low fitness individuals
% By the main hypothesis, high fidelity associated with high fitness. High fidelity associated with low sd_cor? If true, then low sd_cor associated with high fitness. Then lower fitness would be associated with higher sd_cor. Therefore a truncation mechanism which increased the proportion of low fitness entities would have a higher sd_cor...

\chapter{TODO-Elimination of alternative explanations}\label{elimination-of-alternative-explanations}

\section{Variation alone is sufficient for Inheritance}\label{variation-alone-sufficient-for-inheritance -- v-i}

As an experimental test, we can discount this alternative hypothesis by showing an example of not(V-\textgreater{}I) or, V and not I.

Models where p\_selection (factors{[}1{]}) == 1.0 effectively have no
selection (Pr(selected)=1.0), and under changing environment where
fitness changes each generation {[}Element(factors{[}4{]}(x.fitness-0.2,
0.95), x.correlation) for x in population{]}, fitness can and does drop
to zero without entities being removed from population

How to reproduce:

\begin{verbatim}
factor_values = [0, 1, 0, 1, 1, 0, 1] # factor_defns[for selection] = 1.0, meaning no selection! So zero fitness entities persist...
factors = [defn[value] for defn, value in zip(factor_defns, factor_values)]
(initial, final) = model.run(factors, population=init_population(5000, low_start=True), generations=250, population_limit=10, changing_environment=True)
self.assertEqual(5000, final.pop)
\end{verbatim}

Appears to get stuck in local maxima--when correlation goes to 1.0 then no scope for change of fitness \ldots{}

\section{Selection alone is sufficient for Inheritance}\label{selection-alone-sufficient-for-inheritance-s-i}

	Test: show example of S and not I

\section{Variation and Selection, without property correlation, is sufficient for Inheritance}

Test: already shown in earlier analysis, specifically in the Hypothesis analysis sections where the factor \textbf{Correlate Fidelity} is set to the low value ("false").

\chapter{TODO-Conclusions}

Given:\newline
Hypothesis 1 (that Variation, Selection and Inheritance are sufficient for Evolution) and,\newline
hypothesis \autoref{hypothesis-2} (that Variation and Selection are sufficient for Inheritance),\newline
we suggest that:

\textit{Hypothesis 3}: Variation and Selection are sufficient for Evolution

Possible extensions

\begin{itemize}
\item
  Trend to environmental change
\item
  Experimental test: demonstrate Evolution given Variation and Selection
\item
  Consistency and sufficiency--classification of existing systems
\end{itemize}

\part{ToyWorld, a semi-realistic Artificial Chemistry}

\chapter{Introduction}

Artificial Chemistries of discrete atoms provide an interesting testbed
for investigating various evolutionary phenomena. Fundamentally, they
provide a tuneable evolutionary system, capable of highly complex
behaviour, built around familiar metaphors (real-world Chemistry, and
potentially Biology). A set of interaction rules describing how atoms
interact gives rise to emergent forms -- molecules. At a higher level,
these molecules, under the same interaction rules, also interact in
patterns -- reactions.

\TODO{ link emergent levels to copy mechanism}

Still higher emergent levels emerge under favourable conditions.
Reactions may form cycles, where a sequence eventually returns to an
earlier product. Our interest is in identifying the factors that
influence the emergence of these higher levels. Cycles in particular are
interesting as many biological processes are cyclical. Replication,
resulting in an exact copy of an entity, is a macro-example of a cycle;
metabolism is another. Building on the apparent correspondence between
higher emergent levels in Artificial Chemistry evolution and Biology, we
believe like others (e.g., \autocite{Steel2013}) that cycles, of some
form, are a necessary building-block for more complicated structures
again in Artificial Chemistries.

\TODO{ argue that ToyWorld capable of a tunable copying mechanism, similar to that in biology,
but that such a mechanism will be difficult to evolve de novo. Previous work on
evolution of a DNA/RNA copy mechanism in biology: Eigen's paradox, fidelity requires error-correction,
proteins etc}

\section{Contributions}

\begin{enumerate}
\item
	Open-sourced Artificial Chemistry model
\item
	Progress towards useful heredity in an artificial system
\item
	Progress towards OEE in artificial system--evolution compatible with
  OEE without necessarily showing OEE (which is hard to measure and
  prove)
\item
	Demonstration of formation of ACS in an artificial chemistry (previous
  work with ODEs e.g. \autocite{Hurndall2014} not re-usable in that form)
\end{enumerate}

\subsection{Hypothesis}\label{hypothesis}

Because an Artificial Chemistry
\begin{itemize}
\item
  Meets known constraints for OEE--and we have OOL as an example of OEE
  from natural chemistry
\item
  Choosing an artificial chemistry similar to natural chemistry enables
  an argument by analogy
\item
  Catalysis/autocatalysis possible through emergence in some AChems
  (e.g., \autocite{Virgo2013})
\item
	And TODO..
\end{itemize}

Hx: Compliant copy-mechanism possible in an AChem through analogy with DNA/RNA copying

\chapter{Artificial Chemistries}\label{artificial-chemistries}

\section{Introduction}\label{introduction-2}

Full taxonomy in \autocite{Dittrich:2001zr}, also see \autocite{Faulconbridge2011}

One advantage of \glspl{achem} as pointed out by
\autocite[5]{Funes2001} is that it is easier to evaluate solutions in
a domain close to the real-world as opposed to a purely symbolic or
abstract domain such as a lambda-calculus, or a programmatic environment
like Tierra. When contemplating difficult problems such as complexity
our intuition can be helpful, but only in situations close enough to our
normal experience for it to be relevant.

All Artificial Chemistry based on this model:

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}
\node (silico) at (0,0) {\textit{In Silico}};
\node (vitro) at (4,0) {\textit{In Vitro}};
\node (vivo) at (8,0) {\textit{In Vivo}};
\draw [<->] (silico) -- (vitro) node [midway,above] {validation};
\draw [<->] (vitro) -- (vivo) node [midway,above] {validation};
\end{tikzpicture}
\end{center}
\end{figure}

Or in other words, digital models (``in silicon''), wet-chemistry models
(``in glass''), and life itself.

In general, AChems for these problems are without grounding (so start
from high-level). Grounding implies that there is a causal justification
for the higher level chemistry, where that justification is based on
fundamental supportable processes, rather than unique to particular
problem.

All four areas rely on a connection to real chemistries--Molecular
synthesis and Molecular Programming require results to be transferrable
to In Vitro; other two expect results to shed light on In Vivo
processes.

Therefore expect that a grounded AChem for these should have a basis in
real Chemistry.

\subsection{BOX--Physical or Real-world Chemistry}\label{box -- physical-or-real-world-chemistry}

The richness of the real world is built on a substantial foundation of
chemical and physical complexity.

A \textit{reaction} transforms \emph{reactants} into \emph{products},
and is often represented by describing the quantities (or stoichiometry)
of reactant and resulting product molecules. The reaction can also be
characterized by a description of the dynamics, or kinetics, of the
reaction to explain how the reaction proceeds in response to temperature
changes or to varying concentrations of reactants. Reactions often
require an input of energy, such as from molecular collisions, to
proceed; the energy required is called the reaction's activation energy
(\(A_e\)), and is specific to the particular reaction. In general there
is no accurate mechanism to predict reaction dynamics without
experiment.

The \textit{reaction rate} is the change in concentration of a substance
over time: \(\frac{-d[A]}{dt} = k[A][B]\) (where the notation \([X]\)
means that concentration of \(X\) and \(k\) is the reaction rate
constant) leading to Arrenhius' description of the relationship between
the activation energy (\(E_a\)), the temperature (\(T\)) and the rate
constant (\(k\)): \(k = Ae^{E_a/RT}\). The \textit{reaction order}
describes how the reaction rate changes with the concentration of the
reactants, usually captured as a first-, second-, or third-degree
polynomial expression determined empirically. For example, the reaction
rate equation for \(2NO + Cl_2 \rightarrow 2NOCl\) is
\(rate = k[NO]^2[Cl_2]\) (experimentally determined), and is
second-order with respect to \(NO\), first-order with \(Cl_2\) and
overall (the sum of terms), third-order. (Example taken from
\autocite{Kotz2006})

Reactions in theory can be decomposed to a chain of
\emph{elementary steps}, with each step resulting in a single change,
such as bond formation or cleavage, to the reacting molecules.
Elementary steps are somewhat predictable, practical reactions somewhat
less so. Generally in experimental chemistry we know the reactants and
products and can sometimes deduce the sequence of elementary steps.

In modelling, we can either represent reactions exactly as an atomic
transformation from reactants to products with characteristics that can
only be determined experimentally, or we can attempt to construct the
reaction from a sequence of elementary steps with the properties of the
reaction derived from the properties of the steps involved. This later
approach is the only practical one when the reaction is novel, or when
we lack experimental data. Several alternate sequences of steps, or
\textit{reaction pathways}, may be possible between reactants and
products. Each pathway will have a different activation energy, and
hence reaction rate.

The kinetics of elementary steps are defined by the stoichiometry of the
step and are hence predictable. In theory you might expect to be able to
predict the overall reactions kinetics from the composition of these
elementary steps. In practice, the results are close, but not exact,
when compared to experiment. However they form a useful abstraction for
analysis. The reaction rate of an elementary step is defined by the
stoichiometry, where the rate equation is the product of the reactant
concentrations and the rate constant. Therefore a step with one reactant
has order 1, a bimolecular step (\(A + B\)) has order 2 and so on. The
step \(2A + B\) has rate equation \(k[A][A][B] = k[A]^2[B]\) and is of
order 3.

Elementary steps are an abstraction, an aid to analysis, of the
underlying molecular dynamics. At the molecular level, reactions can be
modelled as a series of collisions between molecules. The reaction rate
is then determined by the percentage of collisions (or in other words,
the concentration) that are energetic enough to overcome the inherent
stability of the interacting molecules and cause a change in molecular
structure or shape (in other words, the activation energy).

Recall that the rate of a reaction is a function of concentration (at
the gross level) or collision rate (at the molecular level) and the
activation energy for the reaction, which at the molecular level is
directly related to the energy required to overcome the stability of the
reactants. There are therefore two clear mechanisms to alter the rate
for a reaction: either reduce activation energy, or increase
concentrations. A catalyst does precisely the first, and autocatalysis
is one method for the second.

A catalyst is anything that isn't consumed by the reaction and that
affects the rate (or the kinetic equation for the reaction) without
affecting the reaction's equilibrium constant. Catalysts allow the
reaction to proceed by an alternate, lower activation energy, pathway.
The reaction equation remains the same, but the dynamics are changed -
in the case of biological enzymes the rate can be increased by several
orders of magnitude over the uncatalyzed reaction, enabling reactions
fundamental to life that would be effectively impossible in an
uncatalysed form. At the molecular level catalysts often function by
providing a substrate that preferentially attacks the bonds critical to
the reaction. The platinum within an automotive catalytic converter is a
well-known example of a catalytic substrate.

Autocatalysis, introduced by \autocite{Ostwald1890}, is a different
approach to catalysis. Rather than reducing the required activation
energy, autocatalysis increases the reactant concentrations:
autocatalysis reactions form feedback loops where a compound is both a
reactant and a product. In the standard definition, an autocatalytic
reaction is one that is catalysed by its own products, resulting in a
characteristic rate acceleration over time given by the differential
equation -
\[\frac{dx_i}{dt} = k(\mathbf{X}) \cdot x^n_i + f(\mathbf{X})\] where
\(n\) is the order of the reaction \autocite{Plasson2010}. Example
(indirect network autocatalysis) of glycolysis where pattern is ATP
\(\rightarrow\) n.ATP--ATP initially consumed, but overall created.

Autocatalysis may be realised by 1) either a single reaction (\eg A + X
+ Y \(\rightarrow\) 2A + Z) or a linear chain of reactions through
intermediate products, called template autocatalysis, or 2) by a
reaction network. Networks may be indirect through a series of
intermediary products, or collective where there is no connection
between the component cycles other than through catalysis--as seen for
example in the replication of viroids where each RNA strand can catalyse
the production of the other. However, in all cases, the reactions reduce
to the defining X \(\rightarrow\) nX pattern described by Ostwald's
differential equation.\autocite{Plasson2010}.

A related phenomenon is autoinduction where products increase the
reactivity of the catalyst (rather than directly affecting the reaction
mechanism). Same signature (Ostwald).

Artificial chemistries are regularly employed in three application
areas: real-world chemistry emulators; tools for the exploration of
artificial life, and models to test various hypotheses of the origin of
life. Chemistry emulators and origins-of-life tools aim for fidelity
with real-world chemistry, unlike most artificial life models.
Real-world fidelity requires either the use of a library of predefined
reactions, which conflicts with the goal of unlimited extension, or a
chemically plausible method of constructing reactions from first
principles. Because of the complexities of real-world chemistry this
later method appears to be quite difficult, and the goal of a realistic,
computationally practical, artificial chemistry remains open. However,
the more limited objective of a less realistic, but still fully
constructive chemistry, has been achieved (see Table \cref{classification-of-artificial-chemistries} for
examples.). A constructive chemistry (\autocite{Fontana1994}) is one
where new components may be generated through the action of other
components, and where those new components may themselves take part in
new types of reactions, and so on. This appears fundamental to an
open-ended representation.

A mathematical treatment of \gls{achem} can be found in
\autocite{Benko2009}. \autocite{Dittrich:2001zr} and
\autocite{Suzuki2008a} provide excellent reviews of the field.

\section{Classification of Artificial Chemistries}\label{classification-of-artificial-chemistries}

In \autocite{Dittrich:2001zr}, Achems divided into major types:

\begin{itemize}
\item
  Rewriting or Production Systems (symbols and rewriting rules) -
  Chemical Abstract Machine (CHAM), Chemical Rewriting System on
  Multisets (ARMS), Chemical Casting Model (CCM), Lambda-Calculus
  (AlChemy)
\item
  Arithmetic Operations (molecules are natural numbers, operations are
  arithmetic operators)
\item
  Autocatalytic Polymer Chemistries (polymers made from chains of
  monomer symbols, reactions are concatenation and cleavage)
\item
  Abstract Automata and Artificial Molecular Machines (seem similar)
  e.g., Turing
\item
  Assembler Automata--Avida, Tierra
\item
  Lattice Molecular Systems--\autocite{Ono2000},Madina2003,lattice
  molecular automaton (LMA), Autopoietic System
\end{itemize}

A \gls{achem} can be represented by
\textless{}\emph{S},\emph{R},\emph{A}\textgreater{}
\autocite{Dittrich:2001zr}:

\begin{itemize}
\item
  Set of Molecules (\textless{}\emph{S}\textgreater{})
\end{itemize}

Molecule representation e.g., labelled graphs

Properties of molecules e.g., energy calculation by Extended Huckel
Theory (EHT); salvation energies; reaction rates.

\begin{itemize}
\item
  Reaction rules (\textless{}\emph{R}\textgreater{}) (Chemistry)
\end{itemize}

Transformation from lhs reactants to rhs products. Many (most) reactions
are bidirectional; rates determined by concentrations. Conservation of
energy ? atoms are neither created or destroyed so reactions can be
represented solely by bond changes.

Chemical Abstract Service (CAS) database references approx 34 million
reactions in published work (1) Which ones to adopt? Which reactions
direct evolution to more interesting places?

Reactions may be pre-defined (if S represents real-world molecules) or
dynamically-determined using molecular properties--the problem of how
to select a reaction is addressed by the Reactor Algorithm (below.)

\autocite{Tominaga2007} showed for a particular artificial chemistry,
that it is computationally universal with only unimolecular and
bimolecular reactions.

\begin{itemize}
\item
  Reactor Algorithm (\textless{}\emph{A}\textgreater{}) (Physics)
\end{itemize}

Mechanism to select reactions, e.g., Through the introduction of a
reaction generator (A) the artificial chemistry triplet (S, R, A) is
completely defined \autocite{Lenaerts2009}. Or in
\autocite[sect. 4.1.3]{Faulconbridge2011}, The mixing component of an
Artificial Chemistry is an algorithm which describes the order of and
intervals between reactions, starting from an initial collection of
molecules; this can be termed a ``mixing space''.

\autocite{Faulconbridge2011} identifies three types of
\emph{mixing method} or reactor algorithm:

\begin{itemize}
\item
  Well-mixed/aspatial: with either discrete time (uniform probability
  distribution for selecting reactions) or continuous time
  (Gillespie1976) assuming the reactions are known in advance (not
  possible for a strongly constructive AChem)
\item
  nDimensional: grid with reactions with adjacent cells, or continuous
  where molecules have position and velocity. Major advantage is ability
  to simulate spatial affects; disadvantage is performance
\item
  Mixed scale: hierarchical spaces, such as aspatial cells within bigger
  grid, mostly for simulating biology (e.g., \autocite{Jeschke2008}). One possible
  advantage is potential for parallelization
\end{itemize}

Scaling can be an issue: as new products are generated, the space of
reactions and reactants can expand exponentially, and therefore
performance dives. The general problem is model reduction--see
\autocite{Radulescu2012} for a recent review; \autocite{Faulon2001} has
a well-received method.

Alternative classification in \autocite{Faulconbridge2011}:

\begin{itemize}
\item
  Symbolic--each molecular species labelled by a unique symbol; only
  explicit meanings
\item
  Structured Symbolic \eg \autocite{Hutton2002}--standard, hard to achieve
  emergence or support novel molecules
\item
  Sub-symbolic \eg RBN-World, NAC, ToyChem \autocite{Benko2005}
\end{itemize}

\subsection{Constructive chemistries}\label{constructive-chemistries}

Novel molecules can arise from artificial chemistry:

\quote{
A distinguishing feature of chemistry is that the changes of molecules
upon interaction are not limited to quantitative physical properties
such as free energy, density, or concentrations, since molecular
interactions do not only produce more of what is already there --
rather, novel molecules can be generated.}
{\autocite{Benko2009}}

A chemistry is \textit{constructive} \autocite{Fontana1994} if new components
may be generated through the action of other components. Both implicit
laws and implicit molecule definitions are required for constructive
chemistries:

\begin{itemize}
\item
  Explicit reaction laws = independent of molecular structure; implicit
  = laws must refer to structure. Often used for constructive
  chemistries
\item
  Explicit molecule definitions = from an fixed set of symbols; implicit
  = description for construction.
\end{itemize}

Constructive (weak and strong) initially defined in \autocite{Fontana1994}:

\quote{
We refer to models in which new agents are constructed in an unspecific
(essentially stochastic) fashion as \emph{weakly constructive}. This is
to be contrasted with a situation in which the encounter of two agents
\emph{implies} a \emph{specific} third one\ldots{}Models of this kind
will be termed \emph{strongly constructive}. The prime example of a
strongly constructive system is chemistry.}
{\autocite{Fontana1994}}

The commentary that immediately follows is also significant:

\quote{
A strongly constructive system that contains agent \emph{A} must cope
with the network of its implications. But, then, it also must cope with
the implications of the implications. And so on.}
{\autocite[217]{Fontana1994}}

A strongly constructive system is one which maintains closure, and in
which there is self-consistency and some form of logical structure
(implications.)

A mechanism for exploration in the EA sense--new products can be
generated, and new products can participate in reactions. One approach
is to pre-specify all possible reactions; another is to generate
reactions `on-the-fly' from the structures of the interacting molecules.
The first approach is well suited to simulating real chemistry as it
allows properties of reactions observed in chemical experiments to be
attached to their simulated equivalents. However, it does not allow for
arbitrary reactions, and it requires reaction properties to be
pre-specified--difficult for novel or artificial reactions. It could be
argued that the first approach is not in fact strongly constructive (in
the sense of \autocite{Fontana1994,Dittrich:2001zr}) as it is not completely
open-ended. However, \autocite{Hartenfeller2011} suggests that it is in fact
adequate.

\subsection{Applications in real-world chemistry}\label{applications-in-real-world-chemistry}

Modeling chemical reactions (\eg \autocite{Gibson:2000kx}). Emulators
are often used for backward chaining from a set of desired products to
identify a set of currently-available initial molecules, for example in
drug discovery (e.g., \autocite{Hartenfeller2011}). A second use is in
reaction network discovery, where the goal is to describe a closed set
of reactions and reactants from some initial reactants and reactions
(e.g., \autocite{Faulon2001}). Both applications effectively produce
static descriptions of dynamic processes, and are less useful for
exploring the changes in a network over time.

\autocite{Hartenfeller2012} suggests that
R=\{set of 58 reactions\} might meet requirements for de-novo drug
discovery, but based on S=\{10k-50k building-block molecules\}. Given
much smaller S, will this R still suffice?. Goal of identifying
collection of synthesis reactions that can be used by tools to construct
pathway from provided building blocks to desired compounds. Set of 58
reactions, 29 of which are ring-forming, implemented in reaction-SMARTS
and RDKit.Previous work deficient in its potential to generated
innovative chemo-types, because of the small number of ring-forming
reactions (only 3 ring-formations versus 29 in this work.) Why is a
small set of reactions (50-60) adequate? Most reliable, so transferrable
to bench-top; with reasonable collection of building-blocks (10k-50k)
can fill combinatorial space beyond ability to enumerate. Even with this
set need search mechanism to limit combinatorics.

The primary requirement is fidelity with real-world chemistry, which
requires either a library of empirically derived reaction definitions
and rates, or a model capable of accurately simulating
quantum-mechanical processes. The latter approach has been taken by a
family of Artificial Chemistries, beginning with
\autocite{Benko2003}, built on Extended H\"{u}ckel Theory with parameters
taken directly from chemical experiments and later extended (for example
in \autocite{Benko2005}) to a general purpose model with parameters
derived from theoretical chemistry. The model was used in
\autocite{Hogerl2010} for the study of the behaviour and topology of
chemical reaction networks, specifically Diels-Alder and Formose
reaction networks, and in a series of papers (e.g, \autocite{Flamm2010}
and \autocite{Ullrich2010}) for the examination of the evolution of
metabolic networks in early organisms using a simple model of RNA coding
for catalysts.

An artificial chemistry with the ability to create reactions
``on-the-fly'' given a set of possible reactants may discover more than
one possible reaction pathway between the same reactants and products.
The method used to choose one reaction pathway from the alternatives is
an important component of the artificial chemistry, and the mechanism
may be tuned or tailored to privilege or preferentially chose particular
types of pathways independent to other factors such as temperature or
concentration. In Chemistry the choice of reaction pathway is
fundamentally linked to those other properties and cannot be treated
independently.

Rather than from symbol manipulation, Chemical properties emerge from
fundamental laws. This gives a richness and depth that cannot easily be
equalled in a simulation. For example, the three-dimensional structure
of a molecule--the arrangement of the elements in space--drives even
the simplest chemical reactions. Any (base- or acid) reaction results
from the shift of charge from one region of a molecule to another,
revealing one region while shielding another from activity. Some of the
most complex reactions are shape-driven: the function of many enzymes
(biological catalysts) derives from their shape, and furthermore this
shape is often under regulatory control. A cell can in affect regulate
the activity of an enzyme by either blocking or unblocking the enzymes
active site with another protein. The prediction of the shape and hence
the function of an enzyme from its RNA transcript is perhaps the most
important problem in current molecular biology. In \gls{achem}, an
interesting, but highly simplified, attempt at this can be found in the
work of \autocite{Flamm2010} and \autocite{Ullrich2010}.

Investigating natural selection for chemical evolution
(\eg \autocite{Fernando:2007pf})

Reaction network discovery and Drug discovery

\begin{itemize}
\item
  Chemical Abstract Service (CAS) database references approx 34 million
  reactions in published work
\item
  Either from products back to reactants, or enumeration of all products
  from reactants
\item
  describe a closed set of reactions and reactants from some initial
  reactants and reactions (e.g., \autocite{Faulon2001}
\item
  Realism fundamental
\item
  There is an inbuilt tension between realism and speed in artificial
  chemistries. Realistic reaction selection must involve calculations of
  likelihood, where the current state of the art is to use quantum
  chemistry simulators to determine the properties of a particular
  reaction. Each reaction that fires alters molecular quantities, and in
  models based on a Gillespie-like method, requires the recalculation of
  the firing probabilities for all affected reactions.
\item
  The field has generally progressed from simpler, abstract models (such
  as \autocite{Fontana1992}) to more chemically-realistic ones
  \autocite{Suzuki2008a}.
\item
  Examples
\item
  \autocite{Hogerl2010} etc--Diels-Alder and Formose reaction networks
\end{itemize}

\subsection{Origins of Life}\label{origins-of-life}

Of the historic and ahistoric aspects of the Origin-of-life problem,
historic aspect may never be known \autocite{Pross2013}. Constrained by
what we do know, but many different pathways, and unless some record
somewhere (either geological or phylogenetic), actual path essentially
lost to history. So without evidence for historic aspect, not possible
to test by falsification, and hence can only be speculative.

Consensus forming that early life formed by chemoautotrophs with energy
from inorganic redox couples and biomass from CO\textsubscript{2}, and
that innovations in carbon-fixation created main branches in
tree-of-life \autocite{Braakman2012}. Initiation of selection marked by
\gls{ida}, probably from RNA world, followed substantially later by Last
Universal Common Ancestor (LUCA) \autocite{Yarus2011}, which, it is
important to note for clarity, was almost certainly not a single cell or
even species, but rather a construct of evolutionary genetics because of
the likely predominance of Lateral Gene Transfer (LGT) in archaic
biology (http://sandwalk.blogspot.ca/2007/03/web-of-life.html).
Self-replicating RNA enzymes shown in \autocite{Lincoln2009}, forming
basis of selective system (link to natural selection) (also see
\autocite{Cheng2010}, \autocite{Powner2009} for formation of RNA in
prebiotic conditions). Some elements of \gls{ida} thought still with us
in lineages of informational (for protein synthesis and RNA
transcription) and operational genes (for some standard cellular
processes) \autocite{Ragan2009}, for example the ribosome and
ribonuclease P (RNase P) \autocite{Wilson2009}. Next major transition to
Protein world (although predominance of RNA transcripts leads to
suggestions that should be called RNA-Protein world
\autocite{Altman2013})

Two alternative models for the step from abiotic to \gls{ida}: genetic
or replicators or RNA-first, and metabolism or protein-first. Both
metabolism and replication almost certainly required for \gls{ida}. A
self-sustaining autocatalytic network (in terms of a RAF set
specifically a ``set of molecules and reactions which is collectively
autocatalytic in the sense that all molecules help in producing each
other (through mutual catalysis, and supported by a food set).'')
generally considered essential \autocite{Pross2013}, but not sufficient
\autocite{Hordijk2011}. Both competing models--replication first and
metabolism first--build on that. Autocatalysis expressed by
self-replication of oligomeric compounds in replication first; by cycles
and network in metabolism first. In the broadest sense, life can be seen
as an autocatalytic process where an entity catalyses the production of
one or more descendant entities.

Metabolism-first privileges function, while replicator-first privileges
descent.

If life is metabolism plus information, then for metabolism-first where
lack template-based replication, replication is compositional (composome
- Vasas)

Common functions required in both protocells and minimal cells, but
approaches quite different. Protocells must build up from abiotic
conditions to point where known processes can take over, and to point
where we have historical evidence--bridge gap between abiotic
conditions and first hypothetical cells in record

Main issues with replicator-first model: big step from abiotic compounds
to template-based replication (although ribonucleotides conceivably
could form in pre-life conditions see \autocite{Powner2009}). Templates
encode information in biology, so require a encode/decode mechanism as
well as an information code to represent the product. This is a step
more complex than simpler duplication.

Main issue with metabolism-first model: shift from composome inheritance
to template-based; ability of composomes to fulfill heredity requirement
for natural selection.

Autocatalytic sets are proposed for bootstrapping (from less
differentiated systems to more complex ones), as a mechanism to increase
order and so counteract entropy, for stability, and as a unit of
competition. The catalytic properties of autocatalytic sets result in
bootstrapping, where the reaction rates in the set are ratcheted-up as
the product quantities increase. Autocatalytic sets form a complex or
dynamical system, where the feedback loops and interactions can result
in forms of temporal (waves and cycles) and spatial order (stable
structures.) Similarly stability follows from the formation of
attractors in the complex system that are resistant to pertubation.
Finally, as the set forms a coherent unit expressing certain properties,
such as the ability to maintain itself or to create copies of itself, it
can be seen as a unit of competition where success is measured by
life-time or by control of resources.

Some abiogenesis results fundamentally assume real-world chemistry and
conditions, a constraint that doesn't apply to Alife or artificial OEE,
and so is more restrictive than required. Other abiogenesis work such as
on properties of autocatalytic sets, is broader in applicability.
Genetic and catalytic properties of RNA make well suited to creation of
Alife \autocite{Cheng2010}

Autocatalysis is a source of dynamical behaviour--leads to
bifurcations, multistability, oscillators, attractors
\autocite{Plasson2010}

Some properties of RAF sets established by Hordijk and Steel have
application beyond origin-of-life: linear growth rate in level of
catalysis compared to size of molecules (n) is sufficient for RAF set to
form. For a RAF set to appear with high probability (P\textgreater{}0.5)
in even a model with n=20 (about 1 million molecular types) each
molecule needs to catalyze 1-2 reactions on average. For the given
model, need around 65,000 different molecule types (n=15-16) for RAF set
if use more realistic probability of catalysis of 1:1 million of a
molecule catalysing any particular reaction. \autocite{Hordijk2011}

Lateral or Horizontal Gene Transfer thought so common in early life that
no single common ancestor, but genes from multiple lineages combined
into all lineages today.\autocite{Ragan2009}

Real-world chemical processes are also important to modelling scenarios
for the origin of life or of other related areas such as the formation
of metabolic networks in the earliest protocells

In many cases though the specific focus is less on the bottoms-up model
from the most elementary elements, and more on task-based models of
processes where the particular starting point is predetermined by the
researcher--in the former, Kauffman's autocatalytic protein sets, and
Kaneko's protocell toy model; in the later, Ganti's chemoton.

Examples

\begin{itemize}
\item
  Lattice Artificial Chemistry \autocite{Madina2003,Ono2000}
\item
  the study of membrane formation and cell division assumes five
  different types of particles (some hydrophilic and some hydrophobic)
  that together form an autocatalytic cycle similar to those observed in
  biological cells.
\item
  SCL
\item
  Three types of particle are employed by the Substrate-Catalyst-Link
  (or SCL) chemistry of \autocite{Varela:1974qd,Suzuki2008}: the
  eponymous Substrate, Link and Catalyst. Cells are formed from links
  around a catalyst, with a single predefined reaction rule
  S+S+C\(\Rightarrow\) L+C and some straightforward constraints on
  movement of the particles in the matrix (for example, bonded Link
  particles cannot cross each other.)
\item
  \autocite{Flamm2010, Ullrich2010}--the evolution of metabolic networks in early
  organisms using a simple model of RNA coding for catalysts
\item
  NAC
\item
  \autocite{Dorin:2006fk} the focus is on an ecosystem, based on a set
  of atoms interacting in pre-specified ways that represent biological
  photosynthesis, respiration and biosynthesis (or growth). The goal is
  to explore the interactions in an ecosystem made up of a set of
  organisms pre-built to perform various defined roles.
\item
  \autocite{Gardiner2007}
\item
  string based chemistry to investigate protein metabolism evolution
  under genetic control. Three types of molecule--protein, gene and
  service molecule--react in particular ways associated with the types
  of interacting molecules. Type and pattern of molecules defines type
  of interaction.
\item
  \autocite{Fernando:2008xy},Fernando:2007pf
\item
  flow-reactor for evolution of metabolism in lipid aggregates based on
  predefined types and reactions.
\item
  Natural selection for chemical evolution
\item
  \autocite{Ganti:2003hl}
\item
  \autocite{Dyson1999}
\end{itemize}

\subsection{Alife}\label{alife}

Finally, in Artificial Life, Artificial Chemistries have been used in
the exploration of open-ended or creative evolution. Squirm3
\autocite{Hutton2009,Lucht2012,Hutton2002} adopts fixed molecule types,
and pre-defined reactions for replication and gene-sequence
transcription, and so although capable of interesting behaviour is not
capable of unlimited extension. Stringmol \autocite{Hickinbotham2011} -
a bacterial inspired microprogram chemistry--though does demonstrate a
rich heredity for open-ended evolution using string-matching to model
binding between sequences, and RBN-World \autocite{Faulconbridge2011}
shows that a form of Random Boolean Network, with the addition of a
bonding mechanisms to allow for composition and decomposition of RBNs,
can be used to build a chemistry capable of almost limitless extension
out of non-traditional components.

everything in our artificial world must be built from a common set of
raw materials; a loop connects the targets of selection with the
environment.

Other work, although superficially similar in that it models objects of
similar scale, uses different models for the objects and the world (see
\autocite{Sanchez-Dehesa:2008uq}) and so lack this loop.

\quote{
Erasing the distinction in simulation between organism and environment
allows a model to explore the exchange and transformation of matter and
energy.}
{\autocite{Dorin:2006fk}}

\paragraph{Desirable properties}\label{desirable-properties}

\autocite{Suzuki2003}:

\begin{itemize}
\item
  The symbols or symbol ingredients be conserved (or quasi-conserved) in
  each elementary reaction, at least with the aid of a higher-level man-
  ager.
\item
  An unlimited amount of information be coded in a symbol or a sequence
  of symbols.
\item
  Particular symbols that specify and activate reactions be present.
\item
  The translation relation from genotypes to phenotypes be specified as
  a phenotypic function.
\item
  The information space be able to be partitioned by semi-permeable
  membranes, creating cellular compartments in the space.
\item
  The number of symbols in a cell can be freely changed by symbol trans-
  portation, or at least can be changed by a modification in the
  breeding operation.
\item
  Cellular compartments mingle with each other by some random pro- cess.
\item
  In-cell or between-cell signals be transmitted in the manner of symbol
  transportation.
\item
  Symbols be selectively transferred to specific target positions by
  particular activator symbols (strongly selective), or at least
  selectively transferred by symbol interaction rules (weakly
  selective).
\item
  There be a possibility of symbols being changed or rearranged by some
  random process.
\end{itemize}

\autocite{Tominaga2007}--computationally universal with only uni- and
bi-molecular reactions

Desirable properties from \autocite{Faulconbridge2011}:

\begin{itemize}
\item
  Functional groups.
\item
  Conservation of energy.
\item
  \autocite[sec.4.4]{Faulconbridge2011} contains an interesting
  discussion mapping these desirable properties onto the emergent
  properties that are then required of an \gls{achem}.
\end{itemize}

Additional properties from \autocite{Hickinbotham2010}:

\begin{itemize}
\item
  Novelty and innovation, specifically the ability for new molecules
  introduced to the chemistry to take part in reactions without needing
  changes to the \gls{achem}.
\item
  Range of scales.
\item
  Dynamic environment.
\item
  Redundancy and degeneracy.
\item
  Emergent complex properties.
\item
  Unified molecular representation.
\item
  Stochasticity.
\item
  Emergent mutation rates.
\end{itemize}

\paragraph{Examples}\label{examples}

\begin{itemize}
\item
  Assembler Automata--Avida, Tierra
\item
  Geb \autocite{Channon:2001ly}
\item
  artificial organisms controlled by neural networks created by a
  developmental process from a bit-string genotype.
\item
  Individuals interact with the world through five predefined types of
  interaction generated by the neural network--reproduce (crossover and
  mutation of production rules), fight, turn anti-clockwise or
  clockwise, move forward.
\item
  RBN-World
\item
  Random Boolean Network, with the addition of a bonding mechanisms to
  allow for composition and decomposition of RBNs, can be used to build
  a chemistry capable of almost limitless extension out of
  non-traditional components
\item
  entities are described as a form of Random Boolean Network, with the
  addition of a bonding mechanisms to allow for composition and
  decomposition of RBNs.
\item
  A number of parameters affect the behaviour of the chemistry, and so a
  series of experiments sampled from the parameter-space, and then used
  a GA, to search for interesting variants as measured by non-catalysed
  loops (ideal measures of auto-catalytic sets and Hypercycles too rare
  for use as a measure) \autocite[section 8]{Faulconbridge2011}.
\item
  Ducharme et al \autocite{Ducharme2012}.
\end{itemize}

The approach taken is to model the energy changes associated with
reactions. The chemistry is spatial; atoms are arranged on a
2-dimensional grid and have velocity. When two atoms pass within a
particular distance, they interact.

The possible types of interactions are prespecified, with the type
chosen being driven by the atomic composition and energies of the
interacting atoms. Reactions are therefore between atoms rather than
molecules; a molecule in this chemistry is a combination of atoms
arranged in a particular structure, re-examined after each reaction to
form a stable configuration based on expectations from real-world
chemistry. Although computational costs are not reported, it seems
plausible that the calculation of intersections on a 2-dimensional grid
will be expensive for large molecular populations. Another cost comes
from the re-arrangement of molecules into energy-efficient
configurations. This spatial structuring enables the model to restrict
atomic interactions to those atoms that are accessible on a molecule,
but at the cost of additional modelling complexity.

\begin{itemize}
\item
  Stringmol
\item
  a bacterial inspired microprogram chemistry--though does demonstrate
  a rich heredity for open-ended evolution using string-matching to
  model binding between sequences
\item
  Squirm3
\item
  adopts fixed molecule types, and pre-defined reactions for replication
  and gene-sequence transcription, and so although capable of
  interesting behaviour is not capable of unlimited extension
\item
  Hutton
\item
  Fernando and Rowe
\item
  \autocite{Tominaga2009,Tominaga2007,Tominaga2004}
\item
  string-based pattern matching applied to examination of the
  feasibility of biochemical pathways
\item
  \autocite{Lenaerts2009}
\item
  modelling the topological evolution of chemical networks
\end{itemize}

\chapter{ToyWorld}\label{toyworld}

\section{Introduction}\label{introduction-3}

ToyWorld, our Artificial Chemistry for the exploration of emergent
behaviours, was first introduced in \autocite{Young2013}. The elements
of the model--Atoms, Molecules, Reactions, a Reaction Vessel--are
recognisable from real-world chemistry, but in highly simplified forms.

Although there is no need for ToyWorld to be faithful to standard
Chemistry, a degree of familiarity helps, but only in so far as the
analogy is consistent. Therefore we endeavour to maintain a basic
correspondence wherever possible. However, there is no requirement to
provide chemically-realistic results--our model cannot be used to
investigate real-world chemical behaviours.

There is also another reason. The model has many degrees-of-freedom, and
these must be constrained by parameter choice before the model can be
used in simulation. Some values are important to our thesis, and so are
considered true independent variables in our investigation. These are
examined fully. The remainder however are those to which the simulation
is insensitive, but still must be specified. For these we prefer
real-world values rather than arbitrary artificial values. In short,
where we consider something important, we investigate. Where we think it
less important, we use a consistent set of pre-existing values --
real-world chemistry.

The name has been chosen as both a homage or acknowledgement of ToyChem
\autocite{Benko2003}, and as a hint at the simulation's purpose:
creating an artificial world for exploration. That world, the ToyWorld
model, consists of:

\begin{itemize}
\item
  Analogues of physical elements such as atoms and molecules;
\item
  An overall energy model that describes the transformations that can
  occur between potential, kinetic and internal energy;
\item
  A physics model to describe how molecules interact within the reaction
  vessel; and
\item
  A chemical model that details the bond changes that can occur when two
  molecules collide.
\end{itemize}

All atoms, and therefore molecules and reactions, are contained within a
reaction vessel. ToyWorld provides a basic energy model, where molecules
have kinetic energy and bond breaking requires energy input and bond
formation releases energy. The reaction vessel, which provides the
strategies by which reaction reactants (or input molecules) and products
(output molecules) are determined, is described in detail in the
following section.

In our model, all reactions emerge solely from the properties of the
reacting molecules. For each reaction between two molecules we generate
a list of reaction alternatives by enumerating all possible bond
additions, bond subtractions, and changes in bond type between the
reactants. For example, the reactants H\textsubscript{2} and
O\textsubscript{2} generate three reaction alternatives: breaking of the
H-H bond, breaking of the O=O double bond, and a transformation of the
O=O double bond to a single bond. The reactants H\textsuperscript{+} and
OH\textsuperscript{-} give two alternative reactions: breaking of the
O-H bond (giving H+H\textsuperscript{+}+O\textsuperscript{-}) and
formation of a single bond between H\textsuperscript{+} and O to give
H\textsubscript{2}O.

\section{Atoms and molecules}\label{atoms-and-molecules}

ToyWorld is based on RDKit \autocite{rdkit}, open-source software for
cheminformatics. RDKit provides a number of useful capabilities
including format conversions to and from SMILES \autocite{smiles} and
graphical forms of molecules; standard sanity checks for molecular
structure, and molecular manipulations, but most importantly, is
well-tested and optimised for performance.

Molecules are modelled as an extension of standard RDKit \emph{Mol}
objects, constructed from RDKit \emph{Atoms} connected with
\emph{Bonds}. Standard Lewis dot structures built on the inherited
atomic properties are used to identify possible bonds, and a formal
charge model is used to record the charge changes associated with
modifications to the molecular structure caused by reactions.

The lowest level component in the ToyWorld model is the atom, and atoms
can be joined by bonds to form molecules. Reactions between molecules
are the only mechanism tom modify molecules provided by the model; a
reaction is simply the addition or subtraction of a single bond between
any two atoms in two molecules. ToyWorld provides a strongly
constructive chemistry \autocite{Fontana1994} where completely new forms
of molecules may be generated by reactions, and where the new molecules
may in turn take part in further reactions: the chemistry emerges from
the lower level atomic properties.

Some examples of molecules created by ToyWorld are shown in \cref{fig:toyworld-example-molecules}.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.45\linewidth]{figures/strategies-2-03-molecule.png}
	\includegraphics[width=0.45\linewidth]{figures/strategies-6-02-molecule.png}
	\includegraphics[width=0.45\linewidth]{figures/strategies-13-01-molecule.png}
	\includegraphics[width=0.45\linewidth]{figures/strategies-15-01-molecule.png}
	\caption[Example molecules generated by ToyWorld]{Example molecules generated by ToyWorld, rendered as 2-D objects from the original SMILES notation - [H][C]N(O[N])OOC([H])(O[H])ON([O])[N][N]N(O[O])OOO[H] (top left), [H]OON(O[N+]([O-])[N+]([O])[O-])N([H])O[O] (top right), [H]OOON([H])ON([H])O[H] (bottom left) and [O]C(=O)OOOON=[C]N([O])[O]  (bottom right)}\label{fig:toyworld-example-molecules}
\end{figure}

\section{Energy Model}\label{energy-model}

Energy within ToyWorld is found in only three standard forms -- kinetic,
internal and potential.

\begin{itemize}
\item
  Kinetic energy is the energy of motion, and equals $\frac{1}{2}mv^2$
  where $m$ is the mass of a molecule and $v$ its velocity.
\item
  Internal energy is an abstraction of vibrational energy or heat energy
  within a molecule. Internal energy, while incidentally consistent with
  real-world chemical models, is actually included in ToyWorld as a
  major mechanism to introduce stochasticity in reactions. Without
  internal energy, all excess energy following a reaction must be
  allocated to kinetic energy (following energy conservation), and so
  there is an iso-map between reaction and product kinetic energies.
  With internal energy, we can divert an arbitrary proportion into
  internal energy and therefore we have a stochastic multi-map.
\item
  Potential energy is the energy associated with the bonds between
  Atoms. Creating a bond reduces the potential energy of a molecule;
  breaking a bond increases it. The potential energy of a molecule is
  the sum of the potential energies of every bond in the molecule. The
  base state of no bonds is equal to $0$, and so molecules with bonds
  have negative potential energy. The specific value of each bond is
  absolutely determined by the atomic number of the two Atoms at either
  end of the bond, and the type of bond itself -- single, double or
  triple. ToyWorld provides a table of standard values in
  \cref{default_bond_energies}, based upon real-world chemistry.
  Unspecified bonds are given the average energy of specified bonds of
  the same bond type. Examples of molecules with associated potential
  energies are given in \cref{example_potential_energies}.
\end{itemize}

Our energy model enforces conservation of mass so reactions can be
represented solely by bond changes in RDKit. This follows the approach
taken in graph-based chemistries such as GGL/ToyChem
\autocite{Benko2003,Benko2005} where reactions are modelled as a series
of changes to graph edges, or bonds, only.

\begin{table}[t]
\begin{center}
\caption[Average Bond Dissociation Enthalpies]{Average Bond Dissociation Enthalpies in kcal per mole -- source: \url{http://www.cem.msu.edu/~reusch/OrgPage/bndenrgy.htm}}\label{default_bond_energies}
\begin{tabular}{@{}lp{2cm}p{2cm}p{2cm}@{}}
\toprule
Bond Type & Atom 1 & Atom 2 & Energy of both break and formation(simulation units)\\
\midrule
Single & H & H & 104.2\\
Single & C & C & 83\\
Single & N & N & 38.4\\
Single & O & O & 35\\
Single & H & C & 99\\
Single & H & N & 93\\
Single & H & O & 111\\
Single & C & N & 73\\
Single & C & O & 85.5\\
Single & N & O & 55\\
Double & C & O & 185\\
Double & C & C & 146\\
Double & N & N & 149\\
Double & O & O & 119\\
Double & C & N & 147\\
Double & N & O & 143\\
Triple & C & O & 258\\
Triple & C & C & 200\\
Triple & N & N & 226\\
Triple & C & N & 213\\
Quadruple & C & C & 200\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\begin{center}
\caption{Example potential energies calculated by the DefaultChemistry module using simplified bond energies}\label{example_potential_energies}
\begin{tabular}{@{}p{3cm}p{5cm}p{2.5cm}@{}}
\toprule
Molecule & SMILES & Potential Energy\\
\midrule
H$_2$O 			& [H]O[H] 								& -222.0 \\
H$_2$				& [H][H] 								& -104.2 \\
O$_2$ 				& O=O 									& -119.0 \\
N$_2$O$_4$ 	& [O-][N+](=O)[N+]([O-])=O 	& -434.4 \\
NO$_2$			& N(=O)[O] 							& -198.0 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}%

\subsection{Energy transformations}\label{energy-transformations}

The only energy transformations that occur in ToyWorld are those that
occur during reactions. The energy at the beginning of the reaction is
fully bound in the potential, internal and kinetic energies of the
reactants. At the moment of collision, that portion of the kinetic
energy due to the collision (kinetic energies of reactants less kinetic
energy of centre of mass) is transformed into internal energy
in the combined reactants. If a bond forms in this phase
the freed potential energy is added into this pool of internal energy;
any bond that breaks transforms internal energy into increased potential
energy. Post-collision, a portion of the the pool of remaining internal
energy is transformed back into kinetic energy. The division may be all
to kinetic energy or all to internal or anywhere in between. All as
kinetic would represent a more elastic collision; if all remains as
internal energy, the collision is fully inelastic.

Reactions are modelled as head-on elastic collisions between two
reactants with changes to kinetic energy equalling the increase or
decrease in molecular potential energy associated with the creation,
destruction or change of order of bonds. Creation of a bond results in a
reduction of molecular potential energy and an increase to kinetic
energy; destruction results in the reverse. A change in bond type is
modelled as the sum of a bond creation and of a bond destruction. Total
energy in the system is always constant, and equal to the sum of the
initial kinetic energy of all molecules plus the sum of their potential
energies.

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}
[enode/.style={circle,draw=gray!50,inner sep=5pt,minimum size=30pt}]
\node[enode] (pe) at (6,4) {Potential Energy (PE)};
\node[enode] (ie) at (1,1) {Internal Energy (IE)};
\node[enode] (ke) at (11,1) {Kinetic Energy (KE)};
\end{tikzpicture}
\caption{Energy transformations in ToyWorld}
\label{fig:energy_transformations}
\end{center}
\end{figure}

Reactions conserve energy, and hence the total energy in the reaction
vessel should be constant. However, energy can be explicitly added to
and removed from the system from outside. Either case is modelled as a
uniform change in all molecular internal energies (heat and vibration).
This changes the size of the pool from which the kinetic energies of the
product molecules are determined after a collision or reaction. Adding
energy to the system increases each molecule's internal energy, which
increases the size of the pool of merged internal and kinetic energies
on collision or reaction, which increases the likely kinetic energies of
the product molecules. Removing energy from the system has of course the
opposite affect, down to the point where effectively all motion and
hence all reaction activity ceases.

\section{Reactant and product selection strategies}\label{reactant-and-product-selection-strategies}

A reaction may be seen as two stages in sequence: first, the choice of reactants from a population of possible reactant molecules (the Reactant selection strategy, denoted here by $S_\mathrm{Reactant}$), and second, the determination of products given that set of reactants (Product selection strategy, denoted $S_\mathrm{Product}$). The total energy (that is, potential + kinetic + internal) of the system is maintained, as is the total momentum of the molecules.

\section{Reactant selection strategies: selecting reactants for a reaction}\label{reactant-selection-strategies}

\autocite{Faulconbridge2011} describes two generic strategies for the
selection of reactants -- spatial and aspatial -- where the primary
difference is whether molecular position is a factor in reactant
selection. It is possible to further generalise this scheme by
considering other differentiating factors. Analogous with real-world
chemistry, a cumulative scheme presents itself starting with the pure
aspatial, or uniform probability strategy, and then proceeding through
the spatial strategy, based on molecular kinetics, to kinetics plus
intra-molecular and external forces such as electromagnetism. These
strategies can be viewed as being based on increasing derivatives of
position or location in the reaction vessel; from no position (uniform
selection), through fixed position (uninteresting as we cannot have a
sequence of reactions without motion) to the first derivative (velocity
or kinetic selection) and finally to the second derivative
(acceleration, or force selection.) Accordingly we adopt this more
detailed classification for the descriptions below.

\subsection{Uniform selection}\label{uniform-selection}

In a uniform selection strategy ($S_\mathrm{Reactant} = \mathrm{Uniform}$), reactants are chosen at random with equal (uniform) probability from the population: no property of a molecule has an effect on the selection. Conceptually we have a well-stirred reaction container with no intra-molecular forces.

\subsection{Kinetic selection.}\label{kinetic-selection.}

By contrast, in a kinetic selection strategy ($S_\mathrm{Reactant} = \mathrm{Kinetic}$) molecules have spatial position (and implicitly, velocity) within some assumed reaction vessel, and selection is determined by molecular position -- molecules which are spatially co-located (that is, in collision) form a reactant set. Molecules move at constant velocity until they collide with something else (either another molecule or possibly a boundary of an explicit reaction vessel) and then either react, or bounce.  Currently in our work we assume that all molecules have a fixed and common size and shape (circular in two-dimensions), irrespective of molecular formula.

\subsection{Intra-molecular selection and external force selection}\label{intra-molecular-selection-and-external-force-selection}

More complicated forms, where molecular velocities are not constant, can
be generated by the introduction of some combination of intra-molecular
forces (such as electromagnetism) or external forces (such as gravity or
heat.)

\section{Product selection strategies: determining the products of a
reaction}\label{product-selection-strategies}

In the ToyWorld chemical model, all reactions arise solely from the
properties of the reacting molecules: it therefore defines a
\emph{strongly constructive} chemistry in the sense defined earlier. As
ToyWorld enforces conservation of mass, reactions can be represented
solely by bond changes. This is closely related conceptually to
graph-based chemistries such as GGL/ToyChem
\autocite{Benko2005,Benko2003}, RBN-World \autocite{Faulconbridge2011}
or NAC \autocite{Suzuki2006} where reactions are modelled as a series of
changes to graph edges. However, in ToyWorld, and RDKit, the graph is
implicit rather than explicit as it is in a graph-based chemistry.

For each interaction between two molecules we generate a list of reaction alternatives by enumerating all possible single bond additions, bond subtractions, and changes in bond type between the reactants. Each alternative is the result of a single one of these changes. For example, the reactants H\textsubscript{2} and O\textsubscript{2} generate three reaction alternatives: breaking of the H-H bond, breaking of the O=O double bond, and a transformation of the O=O double bond to a single bond. The reactants H\textsuperscript{+} and OH\textsuperscript{-} give two alternative reactions: breaking of the O-H bond (giving H+H\textsuperscript{+}+O\textsuperscript{-}) and formation of a single bond between H\textsuperscript{+} and O to give H\textsubscript{2}O. We restrict the options to those that can be generated by a single change to the bond structure of the reactants.

Each reaction alternative therefore can be completely described by the
pair of the products of the reaction (that result from the single bond
addition, subtraction or change) and the associated change in overall
potential energy, which of course will be the same as the potential
energy change of the single bond alteration.

Creation of a bond results in a reduction of molecular potential energy,
while bond destruction results in an increase. A change in bond type is
equivalent to a creation and then a destruction. The magnitude of the
change in potential energy, measured in arbitrary energy units, is taken
from a table of bond energies for each combination of atoms and bond
type (\cref{default_bond_energies}. The standard table is based upon
a simplification of real-world chemical bond energies. For example, the
creation of a H-H bond releases 104.2 units; the breaking of a C=O
double bond takes 185 energy units. \cref{example_potential_energies}
shows the potential energy for a sample of molecules.

How should we choose between alternative sets of possible products for
the same reactants? Various product strategies appear plausible: the
random choice of an alternative; the most complex alternative; least
complex; rarest; most common, and so on, but each strategy requires
effort to develop and evaluate.

We also consider a strategy with minimal bias: a Uniform selection
strategy ($S_\mathrm{Product} = \mathrm{Uniform}$), where every
alternative product set has equal probability of selection.

\subsection{Least Energy Strategy}\label{least-energy-strategy}

When following a Least Energy strategy
($S_\mathrm{Product} = \mathrm{LeastEnergy}$) we select a reaction by
choosing with uniform probability from a distribution of reaction
alternatives weighted by the total of the energy changes associated with
the bond changes. This biases selection towards the Least Energy
alternative; the strength of the bias is determined by the degree of the
weighting. \cref{fig1} shows an example of the shift in products that occurs as a result of
this weighting as the overall quantity of energy in the system is
changed.

Specifically, let $E_i$ denote the energy required for the bond change in the reaction option. If $E_i > 0$ the reaction is exothermic, or releases energy; otherwise it is endothermic, requiring energy to proceed.

We calculate a weighted value, $e_i$, based on the combination of $E_i$ and the available energy for the reaction, $E_{avail} > 0$, as follows:

\begin{displaymath}
e_i=
   \begin{cases}
     \lvert E_i\rvert,  &(1) \text{  if $E_i < 0$;}\\
     0,                 &(2) \text{  if $E_{avail} < E_i$;}\\
     E_{avail}--E_i,       &(3) \text{  otherwise.}
   \end{cases}
\end{displaymath}\label{weighting-calculation}

Then, for reaction option $i$ of $n$ options, $p_i = e_i / \sum\limits_{i=1}^n e_i$, where $p_i$ is the probability of option $i$ being selected. A number is chosen from the uniform distribution $[0,1]$ and the selected reaction is found from the inverse of the CDF given by $p_i\text{ for all }i$ by searching for the reaction at that point in the CDF. This method has the property that the probability of a reaction being selected is proportional to its weight.

As a result, for exothermic reactions, highly exothermic reactions are preferred to slightly exothermic ones. For endothermic reactions, the available energy must exceed the energy required by the reaction, and the reaction is preferred according to the degree of the surplus. Note that an option where the energy required exceeds that available ($E_{avail} < E_i$) will have $p_i = 0$, and hence can not be selected. This behaviour is shown in \cref{fig:reaction_selection_weights}.

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}
[enode/.style={circle,draw=gray!50,fill=white,radius=0.5cm}]
\pgftext[at=\pgfpoint{2cm}{0cm},left,base]{\pgfimage[width=0.5\linewidth]{figures/reaction_selection_weights}};
\node[enode] at (5.5,6) {1};
\node[enode] at (7,4) {2};
\node[enode] at (8,3.2) {3};
\end{tikzpicture}
\caption{Weighting calculation for reaction selection under a Least Energy Strategy. Numerical labels refer to cases in \cref{weighting-calculation}.}\label{fig:reaction_selection_weights}
\end{center}
\end{figure}

\subsection{Energy transformations during a reaction}\label{energy-transformations-during-a-reaction}

Molecules have kinetic energy; when they collide the form of the
interaction follows from the energy transformations between kinetic and
internal and potential energy that are preferred under the chemical
model; and finally, the trajectory taken by the resulting products of
the interaction is given by their final post-collision kinetic energy.

First, we determine the kinetic energy of the centre of mass of the
reactants. The available energy to drive the reaction is the total
kinetic energy of the reactants plus the internal energy of the
reactants less the kinetic energy of the centre of mass.

Consider the case where two particles of equal mass but opposite
velocity collide. The KE of the centre of mass will be zero (as it is
motionless) and the energy liberated by the collision will be the sum of
the kinetic and internal energies of the particles.

At the other extreme, consider two equal mass particles, travelling with
the same velocities. Intuitively, it is obvious that the only energy
released by their infinitesimally gentle collision is from their
internal energies, and this is confirmed by the calculation where the
kinetic energy of the centre of mass will equal the combined kinetic
energies of the reactants, leaving only the combined internal energies.

The available energy for bond modifications is calculated as the sum of
the kinetic energies of the reactants less the energy of their centre of
mass, plus any energies internal to the reactants. The final kinetic
energies of the products equals the sum of the initial kinetic energies
of reactants less the change in molecular potential energy from bond
changes and the change in internal energies. Total energy in the system
is always constant, and equal to the sum of the initial kinetic energy
of all molecules plus the sum of their potential energies and internal
energies.

The Reactor Algorithm selects one of the reaction alternatives by
choosing from a distribution of reaction alternatives weighted by
associated energy changes (as discussed in
\cref{product-selection-strategies}.)

\subsection{Setting product velocities and internal energies}\label{setting-product-velocities-and-internal-energies}

The final step in the reaction mechanism is to determine the velocities
and internal energies of the reaction products following the reaction.
Although the method is standard physics, there are two complications:
the number of products may or may not be the same as the number of
reactants, and the pre-reaction energy and post-reaction energy vary as
the reaction itself either consumes or liberates energy.

The only constraints are that velocities of the product molecules must
conserve momentum and total energy within the reacting system. We
recognize that within the frame of reference of the centre of momentum
of the reacting system, the vector sum of the momentum of the products
must equal zero. Therefore one possible solution to the product
velocities is to arrange their vector momentums according to simple
geometry: for two products, we arrange their momentums in a line (line
\cref{alg:2products} in \cref{alg:post_collision_adjustments}); for three
products, an equilateral triangle (line \cref{alg:3products}), and by
extension, for four products, a square and so on.

\begin{algorithm}
\KwData{Reactant molecules fully specified with energies and velocities, Product molecules without energies or velocities}
\KwResult{Velocities for each Product molecule and total Internal Energy for all Product molecules $\ni$ total energy and momentum conserved}
%$\text{sum of reactant kinetic energies}\leftarrow \sum\limits_{i=1}^n \text{kinetic energy}_i$\;
%$\text{sum of reactant internal energies}\leftarrow \sum\limits_{i=1}^n \text{internal energy}_i$\;
$\text{kinetic energy of CoM}\leftarrow \frac{1}{2}(\text{sum of reactant masses})(\text{velocity of CoM})^2$\;
\BlankLine
$\text{collision energy}\leftarrow \text{sum of reactant kinetic energies}+\text{sum of reactant internal energies}-\text{kinetic energy of CoM}$\;
\BlankLine
\For(\tcp*[f]{Transform into CoM frame}){$i\leftarrow 1$ \KwTo $\text{number of Products}$}{\label{alg:lab_to_CoM_frame}
    $v^\prime_i\leftarrow v_i-\text{CoM velocity}$\;
}
\BlankLine
\uIf(\tcp*[f]{Conservation of momentum implies all excess energy must go into internal energy}){Number of products = 1}{\label{alg:allocate_energies}
    $\text{Internal energy of Products}\leftarrow \text{collision energy}$\;
}
\uElse{
    $\text{ke}\leftarrow \text{random}([0,1])$\;
    $\text{Internal energy of Products}\leftarrow \text{collision energy}-\text{ke}$\;
}
\BlankLine
\Switch(\tcp*[f]{Find a set of momentum vectors that sum to zero...}){Number of products}{
\uCase(\tcp*[f]{One product}){1}{
    $mv^\prime\leftarrow (0,0,0)$\;
}
\uCase(\tcp*[f]{Two products}){2}{\label{alg:2products}
    $mv\leftarrow 2\text{ke}\prod\limits_{i=1}^n \text{mass}_i/\sum\limits_{i=1}^n \text{mass}_i$\;
    $mv^\prime_1\leftarrow (v^\prime_{i\theta}+\frac{\pi}{2},v^\prime_{i\rho}+\frac{\pi}{2},mv)$\;
    $mv^\prime_2\leftarrow (v^\prime_{i\theta}+\frac{3\pi}{2},v^\prime_{i\rho}+\frac{3\pi}{2},mv)$\;
}
\uCase(\tcp*[f]{Three products}){3}{\label{alg:3products}
    $mv\leftarrow 2\text{ke}\prod\limits_{i=1}^n \text{mass}_i/\sum\limits_{i=1}^n \text{mass}_i$\;
    $mv^\prime_1\leftarrow (v^\prime_{i\theta}+\frac{\pi}{3},0,mv)$\;
    $mv^\prime_2\leftarrow (v^\prime_{i\theta}-\frac{\pi}{3},0,mv)$\;
    $mv^\prime_3\leftarrow (v^\prime_{i\theta}-\pi,0,mv)$\;
    }
}
\BlankLine
\For(\tcp*[f]{Convert momentums to velocities...}){$i\leftarrow 1$ \KwTo $\text{number of products}$}{
    $v^\prime_i\leftarrow mv^\prime_i / \text{mass}_i$\;
}
\BlankLine
\For(\tcp*[f]{Transform back to standard frame}){$i\leftarrow 1$ \KwTo $\text{number of products}$}{\label{alg:CoM_to_lab_frame}
    $v_i\leftarrow v^\prime_i+\text{CoM velocity}$\;
}
\caption{Algorithm to set post-collision velocities and internal energies}\label{alg:post_collision_adjustments}
\end{algorithm}


Following a standard method, we first transfer the molecules from the
frame of reference of the reaction vessel into the centre of mass (CoM)
reference frame by subtracting the velocity of the CoM from each
particle (line \cref{alg:lab_to_CoM_frame}). Correspondingly, we also
adjust the energy of the collision by subtracting the KE of the CoM. We
recognize that in the CoM frame the vector sum of the momentums will be
zero; working in this frame reduces the number of vectors we must sum by
one (the momentum of the CoM itself.)

We then arbitrarily choose the proportion of total available energy to allocate to the product kinetic energies and assign the remainder to internal energy (line \cref{alg:allocate_energies}.) From the kinetic energy allocation we can determine the total scalar momentum of the products using $KE = 0.5 * velocity * momentum$, and arrange the vector momentums according to the geometry described earlier.

Finally, we convert from the CoM reference frame to the initial frame by
adding back the velocity of the CoM (line \cref{alg:CoM_to_lab_frame}).

This method satisfies our requirements of conservation of momentum and
energy for arbitrary numbers of reactants and products while being
computationally straightforward. The limitation is that product vectors
are arranged in regular and consistent, although reasonably realistic,
configurations. An improvement would be to perturb the geometry of the
vectors in the CoM frame to remove the regularity.

The outcomes for each reaction alternative from an example collision are
shown in \cref{fig:collision_diagrams}.

\tdplotsetmaincoords{60}{110}

\begin{figure}
\centering
\subcaptionbox{O=C=O + C$\rightarrow$ [O] + C + [C]=O}[0.5\textwidth]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.545078574376}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.545078574376}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.741455902351}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-1.00269224405}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{0.791487199406}{-24.6589855625}{-70.1992043245};
\tdplotsetcoord{O1}{0.790684881135}{-24.6841653158}{-219.830383842};
\tdplotsetcoord{O2}{0.342756745318}{-57.7497471497}{-145.082834065};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[O]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[H]C([H])([H])[H]};
\draw[color=blue,-stealth] (O) -- (O2) node[font=\tiny,color=black]{[C]=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\draw[dotted, color=black] (O) -- (O2xy); \draw[dotted, color=black] (O2) -- (O2xy);
\end{tikzpicture}}%
\subcaptionbox{O=C=O + C$\rightarrow$ C + [O][C]=O}[0.5\textwidth]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.545078574376}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.545078574376}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.741455902351}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-1.00269224405}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{0.964870972861}{170.402519717}{-86.502709589};
\tdplotsetcoord{O1}{0.690770768471}{-121.920061352}{-167.114195829};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[H]C([H])([H])[H]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[O][C]=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\end{tikzpicture}}%
\end{figure}\begin{figure}
\centering
\subcaptionbox{O=C=O + C$\rightarrow$ [O] + C + [C]=O}[0.5\textwidth][l]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.545078574376}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.545078574376}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.741455902351}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-1.00269224405}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{0.95444795957}{-20.4286793523}{-48.3296764651};
\tdplotsetcoord{O1}{0.953114678315}{-20.4573799628}{-241.693389093};
\tdplotsetcoord{O2}{0.239763118095}{-84.1834478971}{-145.082834065};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[O]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[H]C([H])([H])[H]};
\draw[color=blue,-stealth] (O) -- (O2) node[font=\tiny,color=black]{[C]=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\draw[dotted, color=black] (O) -- (O2xy); \draw[dotted, color=black] (O2) -- (O2xy);
\end{tikzpicture}}%
\subcaptionbox{O=C=O + C$\rightarrow$ C + [O][C]=O}[0.5\textwidth][r]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.545078574376}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.545078574376}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.741455902351}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-1.00269224405}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{0.683938755498}{116.950434333}{-109.96313961};
\tdplotsetcoord{O1}{0.613112203007}{-91.4698581679}{-158.027937122};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[H]C([H])([H])[H]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[O][C]=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\end{tikzpicture}}%
\end{figure}\begin{figure}
\centering
\subcaptionbox{O=C=O + C$\rightarrow$ [H] + [CH3] + O=C=O}[0.5\textwidth]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.13448625105}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.13448625105}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.182938074093}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-0.247392444315}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{1.0085637103}{-4.76020744144}{22.9882733079};
\tdplotsetcoord{O1}{0.174049248064}{-27.6898552083}{-202.938127977};
\tdplotsetcoord{O2}{0.113557100128}{-42.6716880361}{-145.082834065};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[H]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[H][C]([H])[H]};
\draw[color=blue,-stealth] (O) -- (O2) node[font=\tiny,color=black]{O=C=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\draw[dotted, color=black] (O) -- (O2xy); \draw[dotted, color=black] (O2) -- (O2xy);
\end{tikzpicture}}%
\subcaptionbox{O=C=O + C$\rightarrow$ [H] + [CH3] + O=C=O}[0.5\textwidth]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.212817921117}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.212817921117}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.289490563666}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-0.391486455219}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{1.00268330145}{-7.57832855688}{10.7762512874};
\tdplotsetcoord{O1}{0.247189031344}{-30.8825003788}{-183.173163231};
\tdplotsetcoord{O2}{0.193079615497}{-39.6636579632}{-145.082834065};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[H]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[H][C]([H])[H]};
\draw[color=blue,-stealth] (O) -- (O2) node[font=\tiny,color=black]{O=C=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\draw[dotted, color=black] (O) -- (O2xy); \draw[dotted, color=black] (O2) -- (O2xy);
\end{tikzpicture}}%
\end{figure}\label{fig:collision_diagrams}

\chapter{Model Validation}\label{model-validation}

\section{Introduction}\label{introduction-4}

The design outlined in Section
\cref{toyworld} leads to the following expectations, to be tested
experimentally.

\begin{enumerate}
\item
  Given two reactants, changing the reaction energy should result in
  different sets of reaction products.
\item
  Molecular quantities reach equilibrium -- that is, the set of
  interacting molecules is constant, with fluctuations expected in
  quantities. We expect molecular concentrations to stabilize at
  non-extreme values (equilibrium rather than driven to an extreme)
  after some transition period from the initial conditions.
\item
  The equilibrium point depends on the energy of the system. Our energy
  model preferentially forms bonds at low energies, and breaks bonds at
  high. We expect the average length of molecules in the artificial
  chemistry to be greater at low energies than at high energies.
\end{enumerate}

These predictions were tested by two experiments: first, we examined the
reaction products produced at a range of reaction energies for four sets
of reactants. Second, for a given set of reactants, we ran the
simulation for 10,000 iterations at four successive initial average
kinetic energy levels -- 0, 67, 133, and 200 units per molecule -- with
each molecule initially at quantity 100. The experiment was run first
with a reactant set containing N\textsubscript{2}O\textsubscript{4} and
2NO\textsubscript{2} (results in Fig.\cref{fig1}), and then with a
reactant set of H\textsubscript{2}, O\textsubscript{2} and H\textsubscript{2}O.

\begin{figure}
\begin{center}
    \caption{Molecular quantities over time for initial population of N\textsubscript{2}O\textsubscript{4} and 2NO\textsubscript{2} with initial average KE ranging from 0 to 200 units (only molecules with significant quantities are labelled; remainder appear as light-grey lines). Molecules represented in SMILES notation.}
    \includegraphics[width=\linewidth]{figures/results-test-N2O4a}
    \label{fig1}
\end{center}
\end{figure}

\section{Results and Discussion}\label{results-and-discussion}

Beyond the initial transition period, both reactant sets showed results
essentially consistent with equilibrium. Population variability was high
in both cases, but more so for the N\textsubscript{2}O\textsubscript{4}
and 2NO\textsubscript{2} reactant set. In that case, some molecules
never reached a relatively constant population level (gradient of a
best-fit population line remained significantly non-zero.) The
fluctuations in the quantities of the other molecules are expected
according to our criteria, and result from the inherent variability in
reaction selection which causes the quantities to oscillate around a
norm.

With both reactant sets the model produced a significant number of
molecules which would be considered unstable in real-world chemistry
(such as O\textsuperscript{-} and O.) This is likely an artifact of the method
we use to generate reaction options, where a bond-break plus
bond-formation reaction -- moving through an intermediate unstable
ion -- occurs in our model as two separate reactions. As all molecules
currently react with equal likelihood, significant time can elapse
before the intermediate product reacts to form a stable product.

Both reactant sets showed clear differences in population composition
between the four initial kinetic energy levels. In the
H\textsubscript{2}, O\textsubscript{2} and H\textsubscript{2}O reactant set, no reactions
occurred at the zero energy level. This is expected from our energy
model as only bond-formations are possible without free kinetic energy.
With reactants of H\textsubscript{2}, O\textsubscript{2} and H\textsubscript{2}O no bond formations are possible,
confirmed by examining the bond options returned by the model for the
six possible combinations of initial reactants. By contrast, the
reactant set N\textsubscript{2}O\textsubscript{4} and
2NO\textsubscript{2} at energy zero contains one possible bond formation
reaction (in SMILES, [O]N=O.[O]N=O to O=N[O][O]N=O)
which can proceed without free kinetic energy. This then releases a
product which can also react, and so on, thus explaining the different
results between the reaction sets.

\section{Conclusions}\label{conclusions}

Our base artificial chemistry appears to be at least compatible with the
requirements for the future exploration of open-ended evolution. The
model is simpler than comparable alternatives, and the energy and
reaction models produce results consistent with our predictions for the
system's behaviour (with the exception of achieving equilibrium with the
N\textsubscript{2}O\textsubscript{4} and 2NO\textsubscript{2} reactant
set). An aspatial approach does however come with restrictions. Most
obviously, as there is no concept of proximity in the chemistry, there
can be no boundaries or membranes or even basic distinctions between
\emph{inside} and \emph{outside}. This is critical in biology but it is
unclear if this is equally important in non-biological systems. We
expect that experimental comparison between the aspatial and spatial
approaches in the course of our exploratory experiments will help to
clarify this.

\chapter{Reactant and Product Strategies}\label{reactant-and-product-strategies}

\section{Introduction}\label{introduction-5}

In this section we explore the following research questions:

\vspace{0.3cm}
\begin{minipage}[l]{0.95\textwidth}
\begin{enumerate}[label=RQ\arabic*:]
\item Is there a quantitative difference between different reactant and product selection strategies?
\item Is there a combination of reactant and product selection strategies that leads to increased emergence as measured by cycles?
\item Is emergence significantly affected by the values of other parameters of an Artificial Chemistry, such as initial kinetic energy or bond energies?
\end{enumerate}
\end{minipage}
\vspace{0.3cm}

To the best of our knowledge, this is the first time that reaction and product selection strategies
in Artificial Chemistries have been experimentally compared.
Instead, the general approach of previous work, where there has been a quantitative evaluation,
has been to propose a particular strategy, build, and evaluate against the initial goals,
rather than against alternatives.

Our two primary factors, or independent variables, are $S_\mathrm{Reactant}$ and $S_\mathrm{Product}$.
We also introduce two secondary factors, overall reaction vessel energy ($E_\mathrm{Vessel}$) and
bond energy ($E_\mathrm{Bonds}$), to assess the sensitivity of the simulation to other parameters.
For simplicity of analysis, all of our factors are two-level, meaning they take one of two
possible levels, or values, in each run. The parameter values chosen for each level of
$E_\mathrm{Vessel}$ and $E_\mathrm{Bonds}$ were chosen as representative from a set of alternatives
used in initial exploratory experiments; in each case they allowed the simulation to run for an
extended period without running out of possible reactions (from lack of energy for example.)

\begin{table}
\scriptsize
\caption{Factors, or independent variables}\label{tbl:factors}
\begin{tabular}{p{1.4cm}p{2.2cm}p{4.4cm}p{5.5cm}}
\hline\noalign{\smallskip}
Factor & +1 value & -1 value & Description\\
\hline
\noalign{\smallskip}
$S_\mathrm{Reactant}$&		Kinetic&						Uniform&	See Section \cref{reactant-selection-strategies}\\
$S_\mathrm{Product}$&		LeastEnergy&					Uniform&	See Section \cref{product-selection-strategies}\\
$E_\mathrm{Vessel}$&		300&							100&		Initial kinetic energy of each molecule in the reaction vessel\\
$E_\mathrm{Bonds}$&		Single=50, Double=100, Triple=200& Simplified real-world chemistry. Average values for Single=77.7, Double=148.2, and Triple=224.3&	Energy required to break a bond of the given type\\
\hline
\end{tabular}
\end{table}

We concentrate on three related response, or dependent, variables -- Number of cycles, Length of longest cycle, and Count of most common cycle. All three are derived from a reconstruction of the network of reactions that occur during each experiment run, where every edge represents a specific reaction connecting a particular set of reactants with a particular set of products. Note that the nodes in the constructed network capture specific molecules, rather than molecular types or species that share the same chemical formula (as would be more usual in the construction of a Reaction Network for real-world chemistry.)

We exclude all unique cycles, and all cycles with three or fewer elements (for example, where a molecule loses, then regains, an atom repeatedly). Unique cycles by nature are unlikely to be representative; very short cycles on the other hand are so common as to dominate other more interesting cycles in any analysis.

\section{Experiment Design}
The experiments follow a full factorial design over four factors ($S_\mathrm{Reactant}$, $S_\mathrm{Product}$, $E_\mathrm{Vessel}$ and $E_\mathrm{Bonds}$), each at two levels, run in a randomized order, with three (3) replicates of each combination of factors executed in sequence before beginning the next combination. The first replicate of each combination starts with a predefined random seed incremented by one for each successive replicate of the same combination. The factor levels used are given in Table \cref{tbl:factors}.

Each replicate used the same initial population of 800 molecules, made up of 100 molecules each of [H][H], O=O, [O-][N+](=O)[N+]([O-])=O, and N(=O)[O] and 200 molecules each of O and O=C=O (all represented in SMILES \parencite{smiles}.) This initial population is somewhat arbitrary, although reasonable; given that ToyWorld is a strongly constructive chemistry, we would expect that any differences between initial populations would reduce as the simulation proceeds.

For details of the Artificial Chemistry, see \parencite{Young2013}. The chemistry makes use of some low-level components from RDKit \parencite{rdkit}, open-source software for cheminformatics. RDKit provides a number of useful capabilities, including format conversions to and from SMILES and graphical forms of molecules; standard sanity checks for molecular structure, and molecular manipulations. In ToyWorld, atoms are closely based on real-world chemistry atoms, and in fact are implemented as wrappers around the Atom definitions provided by RDKit; we allow any atom type provided by RDKit. Bonds in ToyWorld are represented by RDKit bonds, but the addition or subtraction mechanism makes use of the parameterised ToyWorld energy model.

\section{Results}
All replicates completed a set of 20,000 reactions; given the initial population size of 800 molecules, and from the summary of results below, we believe that this captures a representative set of reactions. This also simplifies the analysis as we can assume a balanced set of treatments in the statistical sense (that is, the sample sizes for all treatments are equal).

\begin{table}[htbp]
\scriptsize
\caption{Summary of results}\label{tbl:results}
\begin{center}
\begin{tabular}{lrrr}
\hline\noalign{\smallskip}
Statistic&Number of cycles&Length of longest.cycle&Count of most common cycle\\
\hline\noalign{\smallskip}
\multicolumn{4}{c}{Reactions 4750 to 5000}\\
\hline\noalign{\smallskip}
Min.			&0.00	&0.00           	&0.00\\
1st Quartile	&0.00	&0.00        	&0.00\\
Median		&1.50     	&3.50        	&2.50\\
Mean		&219.06   &5.04        	&215.80\\
3rd Quartile	&91.25     	&7.75       		&96.00\\
Max.			&5704.00 &20.00         	&2728.00\\
\hline\noalign{\smallskip}
\multicolumn{4}{c}{Reactions 9750 to 10000}\\
\hline\noalign{\smallskip}
Min.			&0.00	&0.00		&0.00\\
1st Quartile	&0.00	&0.00		&0.00\\
Median 		&6.00	&4.00		&6.00\\
Mean		&62.10	&4.65		&169.21\\
3rd Quartile	&68.75	&8.25		&27.75\\
Max.			&526.00	&13.00		&6684.00\\
\hline\noalign{\smallskip}
\multicolumn{4}{c}{Reactions 14750 to 15000}\\
\hline\noalign{\smallskip}
Min.			&0.00	&0.00	&0.00\\
1st Quartile	&1.00	&3.00	&2.00\\
Median		&5.00	&4.50	&5.00\\
Mean		&27.17	&4.79	&42.27\\
3rd Quartile	&34.50	&7.00	&16.25\\
Max.			&237.00	&12.00	&862.00\\
\hline\noalign{\smallskip}
\multicolumn{4}{c}{Reactions 19750 to 20000}\\
\hline\noalign{\smallskip}
Min.			&0.00	&0.00	&0.00\\
1st Quartile	&0.00	&0.00	&0.00\\
Median		&3.50	&4.00	&4.00\\
Mean		&20.04	&3.90	&14.62\\
3rd Quartile	&20.25	&6.00 	&13.25\\
Max. 		&199.00    &12.00	&216.00\\
\hline
\end{tabular}
\end{center}
\end{table}%

A view of the results is given in Table \cref{tbl:results}: reaction networks built from the full dataset of 20,000 reactions can be too large for easy analysis. Instead, we choose to partition the reaction data into four equally spaced blocks of 250 reactions each and analyse each block independently.

\TODO{convert to ggplot2}

\TODO{convert to ggplot2}
<<cyclesbypartition, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.show='hold', fig.cap='Cycles by reaction partition (starting reaction number for each partition along x-axis)'>>=
	df<-load.toyworld("results/EvaluatorActualCycles.out")
	par(mfrow=c(1,3),cex=0.5)
	plot(df$Number.of.cycles~df$Partition.Start, main="Cycle numbers by Partition", ylab="Number of cycles")
	plot(df$Length.of.longest.cycle~df$Partition.Start, main="Cycle length by Partition", ylab="Length of longest cycle")
	plot(df$Count.of.most.common.cycle~df$Partition.Start, main="Cycle count by Partition", ylab="Count of most common cycle")
@
\label{fig:partitions}

%\begin{figure}[h]
%\centering
%	\includegraphics[width=0.95\linewidth]{figures/partitions}
%	\caption{Cycles by reaction partition (starting reaction number for each partition along x-axis)}
%	\label{fig:partitions}
%\end{figure}

% No R code...
\begin{figure}[h]
\centering
	\includegraphics[width=0.45\linewidth]{figures/PlotMolecularDiversity-strategies-12-0}
	\includegraphics[width=0.45\linewidth]{figures/PlotMolecularDiversity-strategies-16-1}
	\caption{Diversity for Replicates 12-0 and 16-1}\label{fig:diversity}
\end{figure}

\subsection{Analysis and Discussion}
Figure \cref{fig:cyclesbypartition} suggests that the first partition, representing the vessel a quarter of the way into its lifespan, is quantitatively different from the other three partitions, with a significantly greater range for all three response variables. Intuitively this corresponds with an initial period where the diversity in the reaction vessel rapidly increases from the limited starting set of molecules, as seen in some (e.g, Figure \cref{fig:diversity}) but not necessarily all of the replicates. Diversity here is measured by (average molecular quantity)$^{-1}$. All following sections therefore exclude data from the first partition of reaction numbers from 4750 to 5000.

\TODO{convert to ggplot2}
<<reactantstrategy, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.show='hold', fig.cap='Response by Reactant Strategy '>>=
	df <- subset(df,df$Partition.Start != "4750")
	par(mfrow=c(1,3),cex=0.5)
	boxplot(Number.of.cycles~Product.Strategy, data=df, main="Cycle numbers by Product Strategy", names=c("Uniform","Energy"), ylab="Number of cycles")
	boxplot(Length.of.longest.cycle~Product.Strategy, data=df, main="Cycle length by Product Strategy", names=c("Uniform","Energy"), ylab="Length of longest cycle")
	boxplot(Count.of.most.common.cycle~Product.Strategy, data=df, main="Cycle count by Product Strategy",names=c("Uniform","Energy"), ylab="Count of most common cycle")
@

%\begin{figure}[t]
%\centering
%\includegraphics[width=0.95\linewidth]{figures/reactant_strategy}
%\caption{Response by $S_\mathrm{Reactant}$}
%\label{fig:reactant_strategy}
%\end{figure}

\TODO{convert to ggplot2}
<<productstrategy, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.show='hold', fig.cap='Response by Product Strategy '>>=
	par(mfrow=c(1,3),cex=0.5)
	boxplot(Number.of.cycles~Reactant.Strategy, data=df, main="Cycle numbers by Reactant Strategy", names=c("Uniform","Kinetic"), ylab="Number of cycles")
	boxplot(Length.of.longest.cycle~Reactant.Strategy, data=df, main="Cycle length by Reactant Strategy", names=c("Uniform","Kinetic"), ylab="Length of longest cycle")
	boxplot(Count.of.most.common.cycle~Reactant.Strategy, data=df, main="Cycle count by Reactant Strategy",names=c("Uniform","Kinetic"), ylab="Count of most common cycle")
@

%\begin{figure}[t]
%\centering
%\includegraphics[width=0.95\linewidth]{figures/product_strategy}
%\caption{Response by $S_\mathrm{Product}$}
%\label{fig:product_strategy}
%\end{figure}

\subsection{RQ1: Is there a quantitative difference between the different reactant and product selection strategies?}
From visual inspection of Figure \cref{fig:reactantstrategy}, there appears to be a significant difference between the Uniform and Kinetic reactant selection strategies for number and length of cycles. Kinetic reactant selection seems to result in significantly higher levels of emergent behaviour than Uniform reactant selection. Similarly, from Figure \cref{fig:productstrategy}, there is very little apparent difference between the two product strategies, Uniform selection and Least Energy selection.

We use ANOVA (Analysis of Variance) to further examine the relationship of $S_\mathrm{Reactant}$ and $S_\mathrm{Product}$ to the response variables using a two-factor with two-levels (2x2) model (degrees of freedom=1) with interaction effects. There is a highly significant difference (p\textless 0.001) between the Uniform and Kinetic reactant selection strategies when comparing the number of cycles (f-value=40.442) and length of cycles (f-value=361.891) (confirming the impression given by Figure \cref{fig:reactantstrategy}), although again without difference for the count of the most common cycle. The effect of $S_\mathrm{Product}$ on cycle number and length is also significant (f-value=4.050 and 5.705 respectively, p\textless 0.05) and there is a first-order interaction between $S_\mathrm{Reactant}$ and $S_\mathrm{Product}$ for number of cycles (f-value=4.011, p\textless 0.05).

\TODO{convert to ggplot2}
<<reactantproductcombination, pdfcrop=TRUE, echo=FALSE, cache=TRUE, out.width='0.30\\linewidth', fig.height=4, fig.show='hold', fig.cap='Effect of the combination of Reactant and Product Strategies on Response Variables '>>=
	boxplot(Number.of.cycles ~ Reactant.Strategy+Product.Strategy, data=df,  log="y", ylim=c(1,6000),names=c("Uniform:Uniform","Kinetic:Uniform","Uniform:Energy","Kinetic:Energy"))
	boxplot(Length.of.longest.cycle ~ Reactant.Strategy+Product.Strategy, data=df,  log="y", ylim=c(1,6000),names=c("Uniform:Uniform","Kinetic:Uniform","Uniform:Energy","Kinetic:Energy"))
	boxplot(Count.of.most.common.cycle ~ Reactant.Strategy+Product.Strategy, data=df, log="y", ylim=c(1,6000), names=c("Uniform:Uniform","Kinetic:Uniform","Uniform:Energy","Kinetic:Energy"))
@

%\begin{figure}[t]
%\centering
%\subcaptionbox{Cycle Count by Strategy Combination %($S_\mathrm{Reactant}$:$S_\mathrm{Product}$)\label{fig:cycle_count}}[0.45\linewidth][r]{\includegraphics[width=0.45\linewidth]{figures/cycle_count}}
%\subcaptionbox{Cycle Length by Strategy Combination %($S_\mathrm{Reactant}$:$S_\mathrm{Product}$)\label{fig:cycle_length}}[0.45\linewidth][l]{\includegraphics[width=0.45\linewidth]{figures/cycle_length}}
%\subcaptionbox{Count of Most Common Cycle by Strategy Combination %($S_\mathrm{Reactant}$:$S_\mathrm{Product}$)\label{fig:cycle_common}}[0.45\linewidth][r]{\includegraphics[width=0.45\linewidth]{figures/cycle_common}}
%\caption{Effect of the combination of $S_\mathrm{Reactant}$ and $S_\mathrm{Product}$ on Response Variables}
%\end{figure}

\subsection{RQ2: Is there a combination of reactant and product selection strategies that leads to increased emergence as measured by cycles?}

From Figure \cref{fig:reactantproductcombination} it is clear that there is no significant relationship between strategy and the number of occurrence of the most common cycle. However, it seems that such a relationship does exist for the number and length of cycles, with the strongest effect as a result of $S_\mathrm{Reactant}$, and a lesser effect from the choice of $S_\mathrm{Product}$.

We conclude that the greatest levels of emergence are likely to be seen with the combination of $S_\mathrm{Reactant} = \mathrm{Kinetic}$ and $S_\mathrm{Product} = \mathrm{LeastEnergy}$.

\subsection{RQ3: Is emergence significantly affected by the values of other parameters of an Artificial Chemistry, such as initial kinetic energy or bond energies?}

We constructed a two-factor with two-levels (2x2) ANOVA model (degrees of freedom=1) with interaction effects to examine the relationship of the independent variables $E_\mathrm{Vessel}$ and $E_\mathrm{Bonds}$ to the response variables, and applied it to our dataset (summarised in Table \cref{tbl:results}). $E_\mathrm{Bonds}$ is significant (f-value=4.221, p\textless 0.05) to number of cycles. No other significant relationships exist.

\section{Conclusions}
The choice of $S_\mathrm{Reactant}$ is critical to the behaviour of an emergent Artificial Chemistry; $S_\mathrm{Product}$ on the other hand appears to have a lesser effect on the emergence of cycles in our experiments. Furthermore, $S_\mathrm{Reactant} = \mathrm{Kinetic}$ is more effective for cycle emergence than $S_\mathrm{Reactant} = \mathrm{Uniform}$.

The most significant limitation of our analysis overall is that the values chosen for the high and low values of $E_\mathrm{Bonds}$ make it impossible to determine the cause of the difference observed in RQ3. There are two alternative explanations: first, the energy required to make or break bonds is simply different between the two factor levels; second, in the low factor level, based on real-world values, the bond make and break energies for even a single bond vary depending on the atoms involved, while in the high factor level these values are consistent for all bonds of the same degree. To distinguish between the two explanations we would need at least the average levels at each degree to be the same for each factor; this is a suggestion for a future experiment.

\chapter{Discussion}

\TODO{Autocatalysis as a very simple form of inheritance and reproduction without variation. Pathway perhaps from this towards a form with variation, when hypothesis-2 kicks in...}

\part{TODO-The Search for Creativity}\label{the-search-for-creativity}

\chapter{TODO-Introduction}

The parameters themselves become endogenized, in the case of \emph{Fitness} into an implicit relative fitness and for \emph{Fidelity} into the outcome of the copying/variation mechanism.

``Fitness determines context''--in contrast to Part 2, the entity affects its environment

Fitness is relative to a context or niche. Creativity is where the
entity changes niche or context--either by moving to a new niche, or
creating or modifying one

Creativity requires a feedback loop between entity and environment--a
common representation for both so that they can interact (and entities
can interact with other entities, and elements within the enviroment
with other elements\ldots{})

Other open areas--workings of selection

  Hints--e.g., Taylor, Van Valen\ldots{}--regarding resource
  competition, when individuals are embedded (required for single set of
  omnipotent rules)

Hypothesis--Resource competition as the mechanism of selection

\begin{itemize}
\item
  Does this simplify/remove choices? (as we earlier saw with
  reproduction/replication?)
\item
  Addition/Removal follows automatically
\end{itemize}

\section{Requirements for chemistry}\label{requirements-for-chemistry}

\section{Previous work}\label{previous-work-1}

\section{Match to requirements}\label{match-to-requirements}

\section{Chemistry selection}\label{chemistry-selection}

\quote{
Standish (2003), which is that openendedness depends fundamentally on
the continual production of novelty.}
{\autocite{Soros2014}}

\section{\texorpdfstring{What is interesting? Require formal defn for ``proof''\ldots{}}{What is interesting? Require formal defn for proof\ldots{}}}\label{what-is-interesting-require-formal-defn-for-proof}

Levels are interesting in biology (should defn encompass biology or
focus on CS?)

Interesting seems to be different search space, not just bigger space.
What about better search of same space? Is that interesting?

\quote{\ldots{}the processes associated with the major transitions are an
automatic consequence of mutation and selection, due to the generation
of higher levels of selection due to spatial self-organization.}
{\autocite{Hogeweg1998}}

\begin{itemize}
\item
  although somewhat quantified by scope (spatial structures e.g.,
  waves/spirals) and lack of fourth of element of Maynard-Smith:1995lw -
  ``transition from limited inheritance to universal inheritance''
\end{itemize}

Evolutionary mechanism from earlier is made up of variation and
selection elements

Evolutionary mechanism must be itself capable of improvement

\begin{itemize}
\item
  Levels of selection requires changes in mechanism
\item
  And levels are correlated with surprise--major transitions in
  evolution (Maynard-Smith:1995lw) (although specifics here are
  biologically derived)
\item
  e.g., Mechanism of evolution must be itself evolvable; individuals and
  environment interact to give new ways of producing new individuals.
  Dynamics required for novelty-generation (Nellis2014)
\item
  Evolvable mechanism; implies embodied, or endogenous--mechanism must
  be capable of evolvable under same conditions as shown earlier for
  evolutionary potential
\item
  Inheritable variation under selection

  \begin{itemize}
  \item
    If intrinsic selection, then mechanism for
    inheritance/variation/selection should be expressed in same language
    as other evolvable elements--a common rule set or chemistry. If
    different, then cannot have interactions between other elements and
    evolutionary mechanism--limited EvoEvo
  \end{itemize}
\end{itemize}

Show that given 1) OEE evolutionary system from above 2) capable of
self-modification--process evolution, capable of ``interesting''

Sufficient condition

\textit{Hypothesis 4}: Endogenous selection and variation under evolutionary control sufficient for ``novelty-generation'' in evolution

From earlier Hypothesis 3, V+S-\textgreater{}E

Endogenous selection by resource competition

World = Elements + Interactions

More complex interactions emerge (constructive)

\begin{itemize}
\item
  Some compositions actively evolve (strong selection), all others
  potentially affected (weak selection and no selection)
\item
  How do compositions form and how are they maintained? Some rule in
  world must impose a form of bias or asymmetry to allow differences to
  develop. In biology, locality of effect is one example
\item
  Compositions rely on external-to-them elements and lacking those,
  cannot maintain themselves. Growth and maintenance results from
  success in resource competition
\end{itemize}

Variation under evolutionary control

\begin{itemize}
\item
  A' = f(A), if f() fixed then must be capable of generating all future
  variation. Alternative is a f() which is not-fixed, but is itself
  capable of variation. What type of variation?
\item
  Needs to be novel and open-ended itself!
\item
  EvoEvo simplest--same mechanism
\item
  Shared mechanism for I and V--capable of introducing some V during I
\item
  V must be exposed to S
\item
  Feedback loop--V refined by S
\item
  Prediction: V changes over time. In more detailed models, form of V
  may change over time (not possible without an EvoEvo mechanism)
\item
  Prediction: correlation between parent and offspring properties is highest
  in unchanging (stable) environments, and lowest in varying ones. In
  systems without variation under evolutionary control, correlation is
  unchanging. (in previous section, correlation modelled as under
  evolutionary control with degree of correlation as endogenous
  parameter)
\end{itemize}

\subsection{Tests for evolutionary activity}\label{tests-for-evolutionary-activity-1}

Sustainability may be undecidable--Wiedermann:2005ys

Alife community--Channon, etc

Population measures e.g., Vasas2015

Artificial selection

Moran's test

Search mechanism--thermodynamic or ergodic search vs evolutionary
search (implied by Rasmussen2004)

Local entropy (implied by Adami2015)

Measure of OEE -
http://link.springer.com/article/10.1007\%2Fs11084-012-9309-y

\chapter{TODO-Discussion and Conclusions}\label{discussion-and-conclusions}

We have shown that:
\begin{itemize}
\item Selection and Variation are sufficient for Evolution in an Artificial System
\item Such a system is capable of adjusting to changing environments without preselection of parameters
\item Such a system can be implemented in an Artificial Chemistry
\end{itemize}

And therefore that open-ended evolution is possible in an Artificial Chemistry.

\TODO{ link OEE to selection and variation from work in \cref{the-search-for-creativity}}

Requires a series of steps--akin to OOL where single-step
astronomically unlikely (single RNA strand probability about 10E-60,
based on 100 monomers--\autocite{Pascal2013})

\section{Step 1--demonstration of Autocatalytic activity}\label{step-1 -- demonstration-of-autocatalytic-activity}

Previous work

\begin{itemize}
\item
\autocite{Gordon-Smith2013}--Quite complex systems for replication demonstrated
in SimSoup-2, handcrafted molecules
\item
\autocite{Bagley1990}
\item
GARD--\autocite{Segre1998}
\item
\autocite{Ono2002}--Boid-like rules in Lattice Artificial Chemistry leads to
membrane formation. Fixed types of particles with associated
hydrophobic/hydrophilic/neutral class (with orientation). Fixed
reaction paths to form an autocatalytic set of reactions
\item
\autocite{Huning2000}
\end{itemize}

\section{Step 2--demonstration of selection between ACS}\label{step-2 -- demonstration-of-selection-between-acs}

Test of evolution under selection versus drift- artificial selection -
show response to selective pressure

\section{Step 3--Evolution in an artificial chemistry}\label{step-3 -- evolution-in-an-artificial-chemistry}

\section{Alternative explanations}\label{alternative-explanations}

\begin{itemize}
\item
Thermodynamics
\item
Experimental/Model bias (e.g., discovered bias of GARD inheritance
model)
\end{itemize}

\section{Tests for evolutionary activity}\label{tests-for-evolutionary-activity}

\begin{itemize}
\item
Sustainability may be undecidable--\autocite{Wiedermann:2005ys}
\item
Alife community--\autocite{Channon:2006st}, etc
\item
Population measures e.g., \autocite{Vasas2015}
\item
Artificial selection
\item
Moran's test
\item
Search mechanism--thermodynamic or ergodic search vs evolutionary
search (implied by \autocite{Rasmussen2004})
\item
Local entropy (implied by \autocite{Adami2015})
\item
Measure of OEE--\autocite{Markovitch2012}
\end{itemize}

\section{Tests for sufficient conditions}\label{tests-for-sufficient-conditions}

\begin{itemize}
\item
Remove a condition (e.g., \autocite{Soros2014})
\end{itemize}
