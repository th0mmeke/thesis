%% begin.rcode setup, include=FALSE
% library(knitr)
% library(cowplot) # styling of plots, extension of ggplot2
% library(gridExtra) # grid layouts for ggplot2
% library(lattice) # needed for bwplot etc
% opts_chunk$set(fig.path='generated_figures/')
%% end.rcode

\chapter{Introduction}\label{introduction}

\section{Motivation}\label{motivation}

Motivation is to establish the conditions for interesting forms of
directed evolution

Life (see later) provides the canonical example of ``interesting'';
approaching life's creativity in an artificial system would be quite
something

Starting point: evolution of particular solution--certain properties
needed (e.g., cumulative, directed)

Alternatives to ``evolution''? Broad definition\ldots{}:

\begin{itemize}
\item
  Lower bound of random search--not good enough
\item
  No upper bound in practice--but ENS is the gold standard
\item
  Good result would be `interesting' forms
\end{itemize}

Natural selection as mechanism for adaptation (in biological sense).
Must show that adaptation in this sense is useful concept for CS

Isn't this just EA? No--EAs have exogenous fitness, not open-ended
(search through a fixed space, cannot surprise (e.g.,
\autocite{Nellis2014})

OEE is not the same as creative or interesting evolution

\begin{itemize}
\item
  Previous work\ldots{}
\item
  Some systems claim capable of OEE (e.g., Channon, Avida) but not
  necessarily creative
\end{itemize}

Start with minimal requirements for evolution (without need for
interesting)--assume that interesting can be added to this--that it
isn't incompatible with evolution

\begin{itemize}
\item
  Not same as ENS--that explains natural processes; our goal is to
  achieve something that is as interesting in a different domain
\item
  Not same as approach where take ideas from biology and evaluate for
  different purpose (e.g., in EAs, island populations, GRNs (e.g.,
  L-systems), Lamarkian learning, co-evolution, evolutionary transitions
  (cooperation and mediation in \autocite{Defaweux:2005fk}\ldots{}). All
  seem to follow model of currently we use these tools, biology has
  something we don't have, let's try it\ldots{}
\item
  Or properties--robustness and evolvability (redundancy and degeneracy
 --\autocite{Whitacre:2010qy}), novelty (novelty-search -
  \autocite{Lehman:2008cr})
\end{itemize}

Given that, show that under further conditions interesting is possible,
perhaps inevitable

Previous work coming to consensus on conditions--e.g., rich generative
mechanism, unlimited heredity, inexhaustible fitness landscape,
emergence \autocite{Vasas2015}, and good genetic representation,
``sufficiently large world for every individual to be evaluated'', and a
seed or starting point, (plus four specific conditions
\autocite{Soros2014})

\subsection{Why important?}\label{why-important}

\begin{itemize}
\item
  Application to engineering--alternative to EC approaches
\item
  In itself--one of grand challenges of Alife \autocite{Bedau:2000mi}
\end{itemize}

\subsection{Linkage to other fields}\label{linkage-to-other-fields}

\begin{itemize}
\item
  Artificial General Intelligence--one approach to AGI posits that
  intelligence is an evolutionary adaptation, and therefore that most
  promising approach is to follow an evolutionary process
\item
  OOL--similar problem--from chemistry to biology. Analogous to origin
  of life, but despite parallels, must resist temptation to extend
  claims to this--the evolution of life was contingent, and because of
  lack of evidence from early stages, no way anyway to test or confirm
\item
  Evolution in technology occurs by using earlier technologies as
  building blocks in the composition of new technologies, and these new
  technologies then become building blocks for use in later
  technologies, and so on. Arthur calls this ``combinatorial
  evolution.'' But what is the starting point? How is this regression
  grounded? Arthur proposes that the capture and harnessing of natural
  phenomena starts each lineage, and provides new raw components for
  inclusion in later technologies.
\end{itemize}

\subsection{Epistemology}\label{epistemology}

\subsubsection{Biological Evolution}\label{evolution}

\quote{Evolution is a process that results in heritable changes in a population
spread over many generations.}{
``Sandwalk: strolling with a skeptical biochemist'',
\url{http://sandwalk.blogspot.co.nz/2012/10/what-is-evolution.html}}

\quote{
Biological evolution consists of change in the hereditary characteristics of groups of organisms
over the course of generations. Groups of organisms, termed populations and species, are formed
by the division of ancestral populations or species, and the descendant groups then change
independently. Hence, from a long-term perspective, evolution is the descent, with modification,
of different lineages from common ancestors.}{
``Evolution, Science, and Society: Evolutionary Biology and the National Research Agenda'',
Working Draft, 28 September 1998, \url{http://www.zoology.ubc.ca/~otto/evolution/Evolwhite.pdf}}

Separate outcome or result from process or mechanism (e.g., adaptation)

Blurring in use of OEE

\begin{itemize}
\item
  Either in sense of quote{an indefinitely large number of structures are
  each capable of replication.}{\autocite{MaynardSmith1999}}, or
\item
  Additionally meaning novel, interesting, surprising
\end{itemize}

Ongoing generation of novel forms

\begin{itemize}
\item
  Inevitably leads to increasing complexity as without complexity will
  exhaust new possibilities and cover old ground
\end{itemize}

\paragraph{Evolution by Natural Selection (ENS))}\label{ens-evolution-by-natural-selection}

ENS is an example of OEE

http://www.protevi.com/john/Morality/evolution4dimensions.pdf--outline
of Jablonka, summary of evolutionary ``theory'' as of 2005

\begin{itemize}
\item
Biology based around ENS e.g., \quote{god and natural selection are, after
all, the only workable theories we have of why we exist}{\autocite{Dawkins1982}}
\item
But later biology includes HGT (horizontal gene transmission) which
blurs mechanism somewhat, as does importance of neutral-theory,
without invalidating ENS
\item
Summaries and formal models attempting to either capture definition
(constitutive) or sufficiency (causal) of observed systems
\item
ENS (Evolution by Natural Selection) in biology needs variation,
fitness differences, heritability of fitness (Lewontin) or variation,
multiplication, heritability (Maynard-Smith)
\item
See \autocite{Griesemer2001} for discussion of differences
\item
Review of various formulations in Godfrey-Smith2007; all from starting
point of natural evolution -\textgreater{} summary -\textgreater{}
reconcile, address specific difficulties in particular formulations
against common problem cases
\item
Core is \quote{``combination of variation, heredity, and fitness
differences.}{\autocite{Godfrey-Smith2007}}
\item
Argument that heredity may in fact be a product of evolution rather
than a precursor \autocite{Bourrat2015}
\item
Heredity seen as method to maintain low entropy over much longer time
than possible with non-biological systems: \quote{ Living systems can stay away from maximum entropy
for much longer, indeed arbitrarily long (the biotic time scale is, for all we know, only
limited by the existence of the biosphere). It is then this ability: to persist in a state of
reduced entropy for biotic as opposed to abiotic time scales, that defines a set of molecules
as living, and this set of molecules must achieve that feat via the self-replication of
information.}{\autocite{Adami2015}}
\item
HGT (Horizontal gene transfer)
\item
Thought that biological evolution is Evolution by Natural Selection,
but this is not strictly true:
\item
\quote{We take it as given that biology
instantiates ENS {[}Evolution by Natural Selection{]}. That is, ENS
occurs in biological evolution (there is no need to reiterate the
evidence for this). However, we wish to separate the conclusion that
ENS \textit{occurs in} biological evolution from the conclusion that
the algorithm of adaptive biological evolution \textit{is} ENS}{\autocite{Watson2012}}
(emphasis in original)
\item
LGT complicates this
\item
Exaptations more common than thought, meaning that evolutionary driver
shifting towards neutrality from pure selection \autocite{Barve2013}
\item
Junk DNA/Neutral theory--not a highly selective environment
(http://sandwalk.blogspot.co.nz/2008/02/theme-genomes-junk-dna.html)
\item
Random genetic drift
\item
Problems with ENS beyond classical organisms
\item
Previous work in application of ENS to levels of biological hierarchy
(Griesemer2005)
\item
issue in identifying ``individuals'' or ``entities'' in ENS
formulations--part of the ``unit of selection'' problem
\item
issue in meaning of heredity without genotypes--needs a causal
formulation for pre-cellular evolution akin to that in
\autocite{Bourrat2015} or presented in \autocite{Griesemer2005} for
Weismannian causal relationship between genes and organisms
\end{itemize}

\paragraph{Lamarck}\label{lamarck}

Inheritance of acquired characteristics--e.g., epigenetics, but
probably not significant as evolutionary mechanism in biology. But
alternative

\subsubsection{Evolution without life}\label{evolution-without-life}

Not attempting to understand life, but as a guide to achieving a similar
property in another domain

\begin{itemize}
\item
Random search--Random search/walk will explore possibilities, but
without meaning\ldots{}
\item
Compositional--\autocite{Watson2002} discusses compositional evolution and
building blocks
\item
Technology evolution--by components (Arthur2009)

\begin{itemize}
\item
\autocite{Arthur2009} investigates the evolution of
technology, where evolution is used in the sense of \quote{all objects of
some class are related by ties of common descent from the collection
of earlier objects.}{\autocite{Arthur2009}}
\item
Evolution is related to innovation: in fact, Arthur claims that by
understanding the mechanism by which technologies evolve we will
understand how innovations arise. In other words, innovations arise
as the result of an evolutionary process, rather than de novo from
the brain of a designer.
\item
Darwinian evolution, or natural selection, is not appropriate for
technology. Arthur quotes from Samuel Butler's essay ``Darwin Among
the Machines'' : ``{[}t{]}here is nothing which our infatuated race
would desire more than to see a fertile union between two steam
engines\ldots{}'' to illustrate the impossibility of slavish
adoption of biological models.
\item
However, we can clearly see descent of form, in the example given by
Gilfillan in 1935 tracing the development of various elements of the
sailing ship: planking, sails, keels, ribbing and fastenings. In
each case we see a line of gradual improvements leading to the
present day component. But the point is made that this is not
evolution in the full sense, as it lacks both universal scope and an
underlying mechanism.
\item
The first obstacle to a more general scope is the existence of
innovations such as the jet engine, laser, railroad locomotive, or
QuickSort computer algorithm (to name Arthur's examples.)
Innovations seem to appear without obvious parentage; they do not
appear to be the result of gradual changes or adaptations to earlier
technologies.
\item
Arthur's answer is to look inside the innovation and to recognise
that each is made up of recognisable components or modules; the key
lies in the nature of heredity in technology. Technologies are
formed by combining modules of earlier technologies. These groupings
start as loose assemblages to meet some new function, but over time
become fixed into a standard unit (for example, the change in DNA
amplification mechanisms from assemblages of laboratory equipment to
standard off-the-shelf products.)
\item
\autocite{Bourrat2015} comments that distributive evolution (where
distribution of elements changes, as result of selection or drift)
cannot result in novelties
\item
Arthur's response is that novelty comes from incorporating new
phenomena\ldots{}
\end{itemize}
\item
HGT e.g., \autocite{Ochman2000}
\item
\autocite{Pross2011}--Dynamic Kinetic Stability
\end{itemize}

\paragraph{Evolutionary Computation/Evolutionary
Algorithms}\label{evolutionary-computationevolutionary-algorithms}

\begin{itemize}
\item
EAs originally abstracted/inspired by biological ENS
\item
Since specialized, radiated into new areas
\item
Re-unification attempted in works such as \autocite{Paixao2015} but hence not
concerned with extension beyond \quote{models in theoretical population
genetics and in the theory of evolutionary computation}{\autocite{Paixao2015}}
\item
\quote{Some EDAs can be regarded abstractions of evolutionary processes:
instead of generating new solutions through variation and then
selecting from these, EDAs use a more direct approach to refine the
underlying probability distribution. The perspective of updating a
probability distribution is similar to the Wright--Fisher model.}{\autocite{Paixao2015}}
\item
Major points of difference

\begin{itemize}
\item
Mutation combines copying errors with genetic drift (and probably
more)
\item
Search through a fixed space, cannot surprise \eg \autocite{Nellis2014}
\item
Fitness mechanism--implicit vs explicit
\item
top-down for EAs--specific constructs without endogenous evolution
(processes used not subject to evolution)
\end{itemize}
\item
Thought experiments--models
\item
DaisyWorld (Gaia Hypothesis)--\autocite{Saunders1994}--regulation as
model for evolution (selection a hindrance)
\item
MWUA--meta-model?
\item
\autocite{Chastain2014}
\item
\autocite{Barton2014}
\end{itemize}

Seems tightly linked to life--requirements for OEE (hypothesised) look
a lot like requirements for Life (which is only known OEE system, so
perhaps not so surprising)

What elements of life are required for OEE, and which are not?

\begin{itemize}
\item
Exact elements (same thing)--e.g., evolution?
\item
Equivalent elements (same concept/function/purpose, different detail)
e.g., compartments/membranes?
\item
Not required at all e.g., specific metabolic cycles
\end{itemize}



\section{TODO--Previous work}\label{todo---previous-work}

Experimental tests

\begin{itemize}
\item
\autocite{Taylor:1999sc}
\item
\autocite{Sayama2011}
\item
\autocite{Maley1999}
\item
\autocite{Nellis2014}
\item
\autocite{Hickinbotham2011a}
\item
\autocite{Tominaga2005}
\item
\autocite{Lucht2012}
\item
\autocite{Gardiner2007}
\item
\autocite{Fernando:2008xy}
\item
\autocite{Faulconbridge2010}
\end{itemize}

\section{Approach}\label{approach}

Untangle the biological from the general in previous theory

Much previous work aimed at understanding biology--assumes that domain

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{figures/approach}
\end{center}
\end{figure}

EvoEvo project taking a similar approach, but from a higher level
starting point (genotype-phenotype mappings)
http://evoevo.liris.cnrs.fr/about-evoevo-project/

\begin{itemize}
\item
Presupposes microbial evolution, ``at the level of genomes, biological
networks and populations.''
\item
Focus on four specific properties of a genotype-phenotype mapping -
Variability, Robustness, Evolvability, Open-endedness
\item
Later work to remove biological specificity to provide framework for
applying EvoEvo to ICT problems
\end{itemize}

\subsection{The legitimate role of Simulations}\label{the-legitimate-role-of-simulations}

\TODO{ rewrite, properly mark quotations + references!}

The field of artificial life is synonymous with simulation
\autocite[chap.2]{Aicardi2010}. In other forms of science however
practitioners make use of a number of tools, including experiments,
mathematical models, thought experiments, and simulations.
Each method is well suited to some types of
questions, and inappropriate for others. Is the use of simulation
justified for our proposed investigation?

Simulations are becoming central to some disciplines in natural history.
Ecology--\glspl{abm} or \glspl{ibm} (reviewed in
\autocite{DeAngelis2005}; also see
\autocite{Grimm:2006fk,Grimm:2005wd,Grimm:1999kf,Hogeweg:1990jz}).
\glspl{ibm} in Microbiolgy are seen as very close to Alife \autocite{Grimm:2009th}.

The value of simulation over experiments for
\gls{ibm} study lies in a reduction in costs; the difficulties in
cultivation of microbial populations (99\% of known species yet to be
cultivated), and significantly, that they form \quote{complex systems only
poorly explained by reduction.}{\autocite{Ferrer:2008hv}} Emergence and
dynamic behaviours are important, and yet they are hard to capture with
mathematical models.

Types of investigation

\subsubsection{Thought experiments}\label{thought-experiments}

non-empirical, clarification, contradictions/dissonance, fast, cheap.

\subsubsection{Models}\label{models}

\quote{
It is seldom the case in biology that a model is derived deductively
from a more fundamental quantitative theory, with the possible exception
of population genetics which has its foundations in evolutionary
theory.}{\autocite{Krakauer2011}}

Models can be ``useful stop-gaps'' towards a theory, by providing a
testable body of data for experiments and predictions
\autocite{Krakauer2011}, and may be constructed either bottom-up (such
as in ABM) or top-down, by the application of constraints
\autocite{Krakauer2011}. Many types in \eg ecology--eleven according to
\autocite{Jorgensen2008}--of which fall into two main groups

Mathematical models: complexity, need for abstraction/assumptions
(\eg Fisher's famous equation describing the changes in allele
distribution under selection assumes independent genes--extending this
to realistic cases remains an open problem \autocite{Schuster2011}),
difficulty in handling dynamism/emergence. Cheap, fast. Non-empirical.

Simulations: bottom-up approaches, holistic, variability so diversity
closer to real systems, adaptive behaviour/changing
\autocite{Ferrer:2008hv}. Non-empirical data.

\subsubsection{Experiments}\label{experiments}

reductionism, conflation (difficulty in removing other factors e.g.,
Heinemann), expense, time (generations). Source of empirical data.

\quote{
Although this may seem a paradox, all exact science is dominated by the
idea of approximation.}{The Scientific Outlook, Bertrand Russell}

\subsubsection{Benefits}\label{benefits}

Unique ability to explore subject = unique object of enquiry e.g., study
of emergence, complex, self-organizing subjects. Biology stands alone in
the importance of emergence \autocite{Bersini:2006ve}, and the
interconnection of levels of analysis, \eg behaviour can influence gene
expression, and genes can affect behaviour \autocite{Krakauer2011}. As
summarized by \autocite{Krakauer2011}, when asking how much of biology
can be predicted bottom-up from the application of basic physical and
chemical laws--``This question is simple to answer: effectively zero.''

Unique method of enquiry, that is, properties that improve on existing
techniques, e.g., by relaxing assumptions

\subsubsection{The epistemological nature of simulations}\label{the-epistemological-nature-of-simulations}

Simulations seem to fall somewhere in between thought experiments or
abstract models, and experiments. They are also relatively novel; common
use has only come with increased access to digital computers.
Consequently the nature of simulation--what can be claimed as a result
of simulation, and what role may be played legitimately by simulation in
scientific discovery--is a hot topic for philosophers of science. As
might not be unexpected, there are two opposing positions taken, plus a
synthesis that claims the middle ground.

\newthought{Simulations are just calculators}\label{simulations-are-just-calculators}

a computational means to solve analytically intractable equations
\autocite[31]{Winsberg2010}, producing nothing new (just consequences
of what is ``fed in''\autocite{DiPaolo2000}), nothing empirical.

\quote{A simulation is ultimately
only a high-speed generator of the consequences that some theory assigns
various antecedent conditions.}{\autocite{Eldridge}, quoting from Dennett 1979 p192} 

If a model, then might take many forms--analogy, model, pure exploration \autocite{Webb2009}.

Models are \quote{a purposeful representation. A model needs to have a
purpose because otherwise there would be no way to decide what to
include in it. A model's purpose is a filter: the model should not
include anything not believed essential for explaining the phenomenon
of interest}{\autocite{Grimm:2009th}}

\autocite{MaynardSmith1974} distinguishes between
``practical'' descriptions of ecological systems, or ``simulations'', and
theoretical ones: ``models''.

Simulations are aimed at answering specific questions, or analysing
particular scenarios. The more accurate the simulation however, and
hence the more valuable the results, the harder it is to generalize to
other cases. It is hard to understand the behaviour of complicated
simulations, and the causes of particular behaviours of interest may be
unclear if there are many variables in play.

Instead, \autocite{MaynardSmith1974} prefers the use of simple models, designed
to illuminate the ``causes of differences of behaviour between different
species or systems'' rather than ``assertions which are true of all
systems or of all species.''

Hughes 1999 would argue that simulations have genuinely ``mimetic''
character, particularly ones that present results graphically as
real-world systems do, that goes beyond plain number-crunching. They use
a variety of methods beyond calculation (such as graphics) to draw
inferences from data. They also incorporate approximations and creative
choices to make the problem tractable, which introduces need for
justification. Simulations need interpretation and justification--they
are not self-contained, their own justifications
\autocite[31]{Winsberg2010}

\newthought{Simulations are themselves an instance of the
thing}\label{simulations-are-themselves-an-instance-of-the-thing}

That is, the thing is not a shadow but the object. The Animats are
actually alive, and therefore instances of biology.
\footnote{And this way leads us to the claims of Strong Alife--the simulation is actually alive.}
The simulation is a stand-in for the real world, and you can perform
experiments on it as would any other system
\autocite[31]{Winsberg2010}. As \autocite{Adami2002} says, describing
Avida,

\quote{
These organisms, because they are defined by the sequence of
instructions that constitute their genome, are not simulated. They are
physically present in the computer's memory and live there. The world to
which these creatures adapt, on the other hand, is
simulated\ldots}{\autocite{Adami2002}}

They certainly have elements of uncertainty and error, like experiments.

TODO Paul Humphreys 1994 says Monte Carlo simulations are experiments; Von
Neumann ``replace a computation from an unquestioned theory by direct
measurement'' (quoted in TODO [28]{Winkler et al 1987}).

Norton and Suppe 2001 argue that they are experiments, when proper conditions met:
``Empirical
data about real phenomena are produced under conditions of experimental
control'', although ``lacking other data, we can never evaluate the
information that these experiments provide'' Hughes 1999 p.142 so no inherent epistemological force to his
argument. Validity solely depends on validation.

\newthought{A third-way, neither experimental or theoretical}\label{a-third-way-neither-experimental-or-theoretical}

\autocite[31]{Winsberg2010} or an Opaque Thought Experiment"
\autocite{DiPaolo2000}. A common view, among others Dowling 1999, 264:
simulation is like theory as about ``manipulating equations'' and
``developing ideas'' but like experiments as ``fiddling with machines'',
``trying ideas out'', ``watching to see what happens.'' Simulation is a
form of Kuhn's theory articulation or ``model building''--making
principles apply to local, concrete systems in the real world. In this
view, \quote{Simulation is a process of knowledge creation}{\autocite[6]{Winsberg2010}}

\subsubsection{The legitimate role of simulation in scientific
enquiry}\label{the-legitimate-role-of-simulation-in-scientific-enquiry}

Following this third way, simulations might be seen as a source of new
hypotheses \autocite{Eldridge}. The fundamental question remains
however: what is the exact relationship between world and simulation? If
the simulation exhibits an analogous behaviour, under a set of
assumptions, to the real world, then we can contend that there exist
similar mechanisms in the real world to the assumptions in our model.
TODO Noble 1997, quoted in \autocite{Eldridge}). The example given by
\autocite{Eldridge} is Boids \autocite{Reynolds1987} where flocking behaviour in
birds is very similar to that that results from a set of three simple
rules in a simulation.

However, \autocite{Eldridge} identifies three problems with this
approach: first, logically, similarity does not require congruence
\autocite{Weitzenfeld1984}, second, how is the degree of similarity to
be assessed, and third, the impossibility of proving that a simulation
is an accurate model of a theory. That is, where does attribution lie?
Is the result a result of the underlying theory or a quirk of the
implementation? There may be no way of resolving this absolutely as it
may not even be possible to distinguish between the two
\autocite{DiPaolo2000}--we cannot prove correctness through testing.

TODO On the other hand, Taylor (1989) in \autocite{Webb2009}, argues for
``pure exploration'' or ``exploratory tools'' that do not need
justification, and that may be used to generate ``new questions to ask,
new terms to employ, or different models to construct'' (Taylor 1989,
p122). But in this case you might reasonably argue that any insights are
``insights about a mathematical system'' not necessarily insights into
the real-world (123–124 Taylor 1989).

In practice then, any form of model claiming a significance beyond its
own self must show a correspondence with the thing it claims to be
modelling.
\quote{However, existence proofs clearly do require comparisons
between model results and empirical data. One cannot evaluate the claim
that phenomenon X requires condition Y unless one can show that
phenomenon X is actually produced (with or without Y). And the claim or
proof will be stronger or weaker depending on how well the simulated X
matches the real X; for example, demonstrating successful behaviour in
the same physical situation as the animal.}{\autocite[278]{Webb2009}}
The strength of our belief depends on the degree of similarity.

\subsection{Alternative approaches}\label{alternative-approaches}

From Origins-of-Life (OOL)

\begin{itemize}
\item
Theory--an artificial chemistry capable of Evolution
\item
Guided by earlier work, stripped of biological assumptions
\item
hypercycles \autocite{Eigen1971}
\item
autocatalysis
\begin{itemize}
\item
present in all three elements (\autocite{Ganti:2003hl}--or
earlier?) of life
\item
DNA replicates with enzymatic help
\item
some metabolites, such as ATP, exclusively autocatalytic
\item
lipids in a membrane enhance addition of other lipids--Formation of
autocatalytic cycles/sets (\autocite{Hordijk2004})--percolation increases
likelihood
\item
\autocite{Sousa2015} for detection of RAF sets in e coli metabolic
networks
\item
Selection amongst autocatalytic networks (Ganti and Wachtershauser referenced in \autocite{Fernando:2005ly})
\item
\autocite{Fernando:2007pf}--selection/liposomes/chemical avalanches
based on Wachtershauser
\item
autocatalytic cores (\autocite{Vasas2012})
\item
Kauffman's original Reflexively Autocatalytic Polymer Networks
(RAPN) \autocite{Kauffman1986,Farmer1986} are not capable of
non-digital evolution. RAPN stabilise into single network without
variation
\end{itemize}
\item
GARD (fixed catalysts) \autocite{Segre1998} not capable of evolution -
lack heredity of variation (mutations overwhelm heredity) \autocite{Vasas2010}
\item
Bimolecular rearrangements (\autocite{Fernando:2008xy,Fernando:2007pf})?
\item
template replicators--highly unlikely without intermediate steps
\item
Must be driven far-from-equilibrium (many references e.g.,
\autocite{Pascal2015}--continuous supply of energy required (explicit
modelling unlike say \autocite{Fontana1994} where energy only
implicitly modelled)--and maintained there (by metabolism--e.g., how
\quote{living matter evades the decay to equilibrium}{\autocite{Schrodinger1944}})
\item
Eigen threshold for replicators--high mutation rates overwhelm
heredity
\item
Biological evolutionary theory
\item
Extension beyond biology

\begin{itemize}
\item
Previous work on extending evolution--\autocite{Bourrat2015} etc
\item
Evolution of culture, language, technology
\end{itemize}
\item
OOL is focussed on plausible explanations for life-as-we-know-it
\item
And must be constrained by biological givens, i.e.
\item
Starting point consistent with what is known about archaic Earth
\item
Must lead to end-point consistent with earliest known life
\item
In a reasonable time period
\item
single-step astronomically unlikely (single RNA strand probability
about 10E-60, based on 100 monomers--\autocite{Pascal2013})
\item
Requires a series of steps--akin to OOL where single-step
astronomically unlikely (single RNA strand probability about 10E-60,
based on 100 monomers--\autocite{Pascal2013})
\item
Contingent and specific--constraints
\item
starting point compatible with what is known of prebiotic conditions
(either on earth or extraterrestrially)
\item
end point of ENS at something that might be LUCA
\item
Must be simplified and abstracted
\item
Choosing an artificial chemistry similar to natural chemistry enables
an argument by analogy
\item
Meets known constraints for OEE--and we have OOL as an example of OEE
from natural chemistry
\item
Catalysis/autocatalysis possible through emergence in some AChems
(\eg \autocite{Virgo2013})
\item
Some fundamental differences remain however between our domain and the
natural domain
\item
The sheer size of the Biosphere means we can never duplicate the
number of individual evolutionary ``trials'' (selection, mutation and
reproduction) events in our model
\item
The Biosphere is underpinned by a phenomenally rich set of physical
and chemical laws, implicitly constraining each and every action and
interaction
\end{itemize}

\section{TODO--Related work}\label{todo---related-work}

\subsection{Artificial Life}\label{artificial-life}

``Synthesis and simulation of living systems'' or contemporary
artificial life as \quote{an interdisciplinary study of life and life-like
processes, whose two most important qualities are that it focuses on the
essential rather than the contingent features of living systems and that
it attempts to understand living systems by artificially synthesizing
simple forms of them.}{\autocite{Bedau:2007ga}}

``life-as-it-could-be'' rather than ``life-as-we-know-it'' \autocite{Langton1989}

Implicit rather than explicit fitness

\begin{itemize}
\item
The mapping between representation and fitness must be implicit
\item
a property that arises from the representation itself rather than from
an external measure
\item
difficult to imagine how to pre-specify a mapping that remains
relevant in an open-ended system
\end{itemize}

Life a subset of Alife

Life is the only provided example that we have

Life has non-trivial emergence

Alife doesn't have to, but it is a guide

Hard vs Soft vs Wet forms of Alife

Purposes

\begin{itemize}
\item
Insights into life
\item
Swimming--\autocite{Terzopoulos1994}
\item
Foundations for other fields e.g., AI
\item
In own right--life ``de novo''
\item
Philosophy--what is living?
\end{itemize}

Themes \autocite{Aguilar2014}

\begin{itemize}
\item
Properties of living systems--Origins of life, autonomy,
self-organization, adaptation (evolution, development, and learning)
\item
Life at different scales--Ecology, artificial societies, behaviour,
computational biology, artificial chemistries
\item
Understanding, uses and descriptions of the living--information,
living technology, art and philosophy
\end{itemize}

History

\begin{itemize}
\item
Prehistory
\item
Various automata
\item
Philosophy about automata
\item
1818 Mary Shelley ``Frankenstein; or, The Modern Prometheus''
\item
Uptick in mentions of ``Artificial Life'' as quoted in \autocite{Aguilar2014}
\item
Modern field
\item
1951 Von Neumann--first formal model
\item
1984 Christopher Langton
\item
1987 ``Official'' birth of field--first ``Workshop on the Synthesis
and Simulation of Living Systems'' in Sante Fe, NM, by Langton
\item
Conway Game of Life
\item
Cellular Automata
\item
Tierra, Avida
\item
Dawkin's Biomorphs
\item
Bedau Challenges
\item
Overlap with Brooks's robotics, AI, etc
\end{itemize}

\subsection{Biology}\label{biology-1}

\settowidth{\epigraphwidth}{Wonderful life : the Burgess Shale and the nature of history}
\epigraph{%
Without hesitation or ambiguity, and fully mindful of such paleontological wonders as large dinosaurs andAfrican ape-men, I state that the invertebrates of the Burgess Shale, found high in the Canadian Rockies in YohoNational Park, on the eastern border of British Columbia, are the world's most important animal fossils. Modern multicellular animals make their first uncontested appearance in the fossil record some 570 million years ago--and with a bang, not a protracted crescendo.}% 
{\textit{\\Wonderful life : the Burgess Shale and the nature of history}\\\textsc{Stephen Jay Gould}}

Our primary example of life and evolution

\subsubsection{What is life?}\label{what-is-life}

Autopoesis

Self-replication

Rosen (M,R) systems

Ganti Chemoton

Autonomy

Or \quote{
What modifications must be made to this type of
experiment to allow at least one of the following outcomes:
‘open-ended evolution’ (Bedau et al., 2000); the origin of
basic autonomy, i.e. a dissipative system capable of the
recursive generation of functional constraints (Ruiz-Mirazo
et al., 2004); a process ultimately capable of the
production of nucleic acids or other modular replicators
with unlimited heredity potential (Maynard-Smith and
Szathmary, 1995; Szathmary, 2000); identification of ‘‘the
course of evolution by which the determinate order of
biological metabolism developed out of the chaos of intercrossing
reactions’’ (Oparin, 1964); the coupled cycling of
bioelements (Morowitz, 1968, 1971); the maximization of
entropy production by a biosphere (Kleidon, 2004); the
minimal unit of life (Ganti, 2003a, b); or an autopoetic unit
(Maturana and Verela, 1992)?}{\autocite{Fernando:2007pf}}

\subsubsection{Arrow of complexity}\label{arrow-of-complexity}

Explored in \autocite{Miconi:2008cy}:
\begin{itemize}
\item Passive (inevitable result of non-repeating evolution from simple seed)
\item Active--some drive towards increasing complexity
\end{itemize}

\subsubsection{Living organisms are supremely well suited to their
environments, and can adapt to environmental
changes}\label{living-organisms-are-supremely-well-suited-to-their-environments-and-can-adapt-to-environmental-changes}

Adaptation of organisms to their environments occurs in the main on two
different time-scales.

Evolution by \gls{naturalselection} acts over a period of generations on
populations of individual organisms. Changes are therefore relatively
gradual, and many generations can pass before a change such as a
beneficial mutation becomes ubiquitous in a population (\eg 300-500
generations for targeted modifications in lactose processing in
\emph{E. coli} \autocite{Dekel:2005fk}. In contrast, gene regulatory
effects act during the life cycle of a single individual, either during
development to affect morphology, or during the adult lifespan in
reaction to seasonal or other environmental cues. These
regulation-driven changes are not in themselves heritable, but they can
be assimilated back into the population by influencing the organisms
fitness under natural selection (\eg,
\autocite{Baldwin:1896ly,Dennett:2003ve,Paenke:2009xe,Paenke:2007ve}).

Similar effects can be seen by another adaptive mechanism that operates
on individuals during their lifespan: learning, where behavioural
adaptions can also lead to genetic change
(\eg \autocite{Hinton:1987vy}.)

\subsubsection{Natural selection, acting on populations, is the primary
driver for long-term
adaptation}\label{natural-selection-acting-on-populations-is-the-primary-driver-for-long-term-adaptation}

The year 2009 saw the celebration of the 150\textsuperscript{th}
anniversary of the publication of \emph{On the Origin of Species}, the
explanation of evolution by natural selection, with extensive coverage
in scientific and popular media. The terms are therefore fairly well
known to many people, but what exactly does \emph{natural selection}
mean? To quote from \autocite{Futuyama:1979tg}, \gls{naturalselection}
is the ``differential survival and reproduction of genotypes''.

Let's examine each of the components in this idea in turn:

Differential survival. Living organisms are constantly engaged in an
intimate relationship with their environments. Indeed, according to the
theory of \emph{autopoiesis} \autocite{Varela:1974qd}, organisms are
defined by this engagement: to be alive means maintaining oneself
against the surrounding environment. In general the more effectively the
organism is able to do this, the more likely it is to survive. However,
in practice survival for an individual may be affected by random events.
Scholarship winning students can be killed by drunk drivers. Sardines in
shoals flash and turn, yet sharks still manage to grab one or two from
the shoal effectively at random. Averaged over a population however
these chance events balance out; a succession of random trials leads to
a skewed distribution of fitness away from the less able.
\footnote{This introduces two significant differences between \glspl{ea}
and biology: first, \glspl{ea} conduct a series of discrete trials of fitness,
rather than a continuous evaluation. Second, fitness in an \gls{ea} is
measured by an explicit \emph{objective function} whereas in biology
fitness \emph{emerges dynamically} through continuous interaction with
the environment.}

Reproduction. In this sense, reproduction simply means inheritance. Only
those characteristics that can be passed on from one generation to the
next are relevant. One implication of this is that the only traits of an
organism that matter to natural selection are those that are apparent
while the organism can reproduce. Altruism and kin-selection, where an
individual acts to increase the fitness of a related individual, are
interesting for the light they shed on this implication.

Genotypes. An organism's \gls{genotype} is the heritable information
that defines an individual, of which the great majority is encoded in
DNA (some \emph{epigenetic} information is inherited through
DNA-methylation, maternal protein concentrations and other mechanisms.
However, generally DNA remains the primary source.)

How then does natural selection unfold in practice? Although an organism
is defined by its genotype, its survival is based not on the raw
genotype, but on the expression of the genotype--called the
\gls{phenotype}--that participates in the interaction with the
environment. There is not necessarily a direct one-to-one mapping
between genotype and phenotype; for example, environmental triggers
during development can switch the phenotype in different directions (a
phenomenon called \gls{polyphenism}.)

This indirect mapping enables a number of important mechanisms
significant to the operation of natural selection: first, changes in the
genotype (caused by mutations for example) may build up independent of
phenotype changes--the idea of \emph{neutral mutations}
\autocite{Ohta:1996vn,Ohta:2002ys,Ohta:1973kx}. Examples from studies
of RNA secondary structures (the physical folding of RNA molecules) show
that many closely-related RNA sequences can produce the same RNA folding
\autocite{Fontana:1993zn}. Adjacent changes often have little effect on
structure.

Second, by extension, \autocite{Gavrilets:1997qt} and
\autocite{Gravner:2007yd} have shown that in cases involving many gene
loci under well-defined conditions there is a path between viable
phenotypes that requires only neutral mutations.

Third, behaviours such as learning, rather than purely genetic
mechanisms, can influence the form of this \gls{gpmap} in combination
with natural selection, as illustrated by the Baldwin effect
\autocite{Baldwin:1896ly} and other examples of genetic assimilation
{[}\autocite{Hinton:1987vy};Siegal:2002qn;Waddington:1942jb{]}.

Finally, \glspl{grn} provide another mechanism to modify the \gls{gpmap}
and hence to guide natural selection.

\subsubsection{Regulation provides a source of variation in populations and
is an important mechanism for adaptability in individual
organisms}\label{regulation-provides-a-source-of-variation-in-populations-and-is-an-important-mechanism-for-adaptability-in-individual-organisms}

Genes, encoded by the familiar A-T-G-C base pairs of DNA, are
transcribed into RNA and then translated into the amino acids that, when
strung together in long chains, make up proteins. Proteins are the
workers of the cell, catalysing reactions and forming the cellular
scaffolding of the cell itself. As every cell in a multi-cellular
organism (with the exception of sex cells) contains the same DNA and
hence genes, it would seem to follow that every cell should produce the
same proteins and so all cells should be identical.

Unfortunately for this model, cells are not identical. The missing
element from our understanding is a mechanism to turn selected genes on
or off--in the language of biology, to activate or repress gene
expression--so that different groups of proteins can be produced from
the same overall DNA. This is known as gene regulation, and is the
source of all diversity within an organism. Different cell types are in
fact distinguished by specific, partially overlapping, groups of
expressed genes. For example, in the nematode worm,
\textit{Caenorhabditis elegans}, approximately 7-8\% (or 1500 genes)
only of the complete genome are expressed in muscle cells
\autocite{Watson:2008fm}.

Cells call upon a complex of processes to regulate the expression of
genes. The most common regulatory mechanism
\autocite[542]{Watson:2008fm} is the use of proteins
\emph{\glspl{tf}}) to control the initiation of \gls{transcription} (the
production of RNA from a segment of DNA, the first stage of gene
expression), as illustrated by the CAP and repressor proteins in the
\textit{lac} operon. Post
transcription, other processes involving RNA molecules can then edit and
splice the RNA products of transcription in bacteria and
\glspl{eukaryote} prior to \gls{translation} (the production of a
protein from mRNA). As a result of RNA regulation many alternative
proteins from one transcript can be produced, triggered by environmental
or other factors.

\subsubsection{\texorpdfstring{The \textit{lac} Operon--an example of how
regulation can enable a cell to respond appropriately to environmental
triggers}{The  Operon--an example of how regulation can enable a cell to respond appropriately to environmental triggers}}\label{the-operon---an-example-of-how-regulation-can-enable-a-cell-to-respond-appropriately-to-environmental-triggers}

In the bacterium \textit{Escherichia coli}, and other
\glspl{prokaryote}, the choice between two alternative energy sources -
glucose, the preferred source, and lactose, the alternative--is under
regulatory control. The details of the mechanism, known as the
\textit{lac} operon
\footnote{An operon is a set of genes in prokaryotes that share promotor and regulatory regions and hence are transcribed and regulated as a unit},
were established by a series of experiments by Jacques Monod and
Francois Jacob in the late 1950's and early 1960's
(\eg \autocite{Jacob:1961ys,Jacob:1961rv}), although the account given
here is based on the later summary, using current terminology, from
\autocite[chap. 16]{Watson:2008fm}.

The processing pathway for lactose is provided by three proteins -
\(\beta\)-galactosidase, the lactose permease, and thiogalactoside
transacetylase, for completeness--encoded by three contiguous genes
(\textit{lacZ}, \textit{lacY} and \textit{lacA} on the chromosome. Two
further regulatory proteins are involved: an activator called CAP and a
repressor protein, the Lac repressor.

CAP, the activator protein, increases the production of the three
lactose-processing proteins by helping the RNA polymerase molecule, that
performs transcription, bind to the promotor region. Once bound RNA
polymerase can start the process of turning the genes into proteins.

The repressor protein however, as the name implies, stops production of
the lactose-processing pathway. When the repressor is bound to a
particular repressor site on the chromosome near the promotor it
physically interferes with the binding of RNA polymerase, preventing it
from binding, and thus preventing the start of transcription. This
repressor protein is produced \gls{constitutively} or continuously, by
the cell.

How then do these proteins--glucose, lactose, two regulatory proteins,
and the three proteins in the lactose processing pathway--interact to
activate or repress the lactose processing pathway, and so switch
between energy sources?

Glucose lowers the concentration in the cell of a further protein, cAMP,
which is required for CAP to function as an activator. Therefore, when
glucose is present, cAMP is low, CAP cannot function as an activator and
the three \textit{lac} genes are not transcribed: no lactose processing
takes place.

In a similar fashion, when lactose is present in the cell it interacts
(after conversion to allolactose) with the lac repressor protein to
prevent it binding to the repressor site on the chromosome, and so the
presence of lactose removes the repression of the \textit{lac} proteins.
cAMP will be high, CAP will function as an activator, and the
\textit{lac} proteins will be produced.

In summary, in the presence of glucose lactose processing is repressed;
if glucose is absent but lactose present, lactose processing is
activated: the cell preferentially processes glucose over lactose.

\paragraph{Novelties often arise from new regulatory connections
rather than changes to
genes}\label{novelties-often-arise-from-new-regulatory-connections-rather-than-changes-to-genes}

\autocite{Prudhomme:2007ax} believe that evolutionary novelties more
commonly arise from changes or additions of regulatory
\emph{connections} than from the development of \emph{new} genes or
regulatory elements; that is, from changes to the network topology
rather than from additions to the network elements. The underlying
implication is that novelties are therefore new compositions of
pre-existing elements, rather than being constructed `\emph{de novo}',
and that production of novelties may be relatively rapid. Connection
changes may happen quickly; however, it is hard to see how a new
functional gene can be constructed other than over many, many
generations.

\paragraph{Elements under regulatory control may be conserved over
long periods of
time}\label{elements-under-regulatory-control-may-be-conserved-over-long-periods-of-time}

Eye development in animals \autocite{Shubin:2009vw}. Light-sensing
organs across the animals show a number of deeply-conserved common
elements: a small set of \glspl{tf} including \emph{eyeless},
\emph{atonal}, and \emph{eye absent} in \emph{Drosophila} and analogues
in vertebrates are central to the design and development of the animal
eye. The \glspl{tf} \emph{CHX10} and \emph{VSX2} are found in the
neurons for processing visual signals in both vertebrates and
\emph{Drosophila}. All animal eyes are based on opsin proteins in a
cascade. It is thought that these common elements show both a common
ancestry, and reflects a \emph{\gls{deephomology}}.

As another example forms of the homeotic genes \emph{hox} and others are
found in every animal.

\paragraph{Regulation allows environmental signals to trigger
adaptive phenotypic change
(``polyphenism'')}\label{regulation-allows-environmental-signals-to-trigger-adaptive-phenotypic-change-polyphenism}

\Gls{polyphenism} is the production of multiple, alternate, phenotypes
under environmental influence: \autocite{Fusco:2010fk},
\autocite{Minelli:2010vn} and \autocite{Beldade:2011ly} provide useful
reviews. Two forms of polyphenism are generally distinguished:
irreversible changes during development, and reversible changes during
adulthood.

\paragraph{Regulation enables reversible environmental change in
adult
phenotypes}\label{regulation-enables-reversible-environmental-change-in-adult-phenotypes}

The arctic fox (\emph{Vulpes lagopus}, previously known as
\emph{Alopex lagopus}) changes coat colour, in anticipation of the
season, from brown in summer to white in winter
\autocite{Pocock:1912uq}, and although the mechanism enabling this is
not well understood beyond an extrapolation of the circadian rhythm
imposed by the hypothalamus, there is evidence that a regulatory system
(agouti/MCR1) is involved \autocite{Vage:2005kx}.

Nutrition in Hymenoptera influences caste selection during development
\autocite[1350]{Beldade:2011ly}. The social Hymenoptera (ants and many
common bees and wasps) are insects where individuals belong to castes,
distinguished both by form (the presence of wings in queens, their
absence in workers) and function (only queens reproduce.) The selection
of caste for a developing larvae is partially controlled by the presence
of a hormone (Juvenile Hormone, or JH) in the larvae's diet. A diet rich
in JH leads to elevated levels of JH in the larvae. Levels of JH in
excess of a threshold switch development along a pathway that results in
a queen; below the threshold the larvae becomes a worker. JH induces the
expression of a set of JH-responsive genes, which control the
development of queen-specific features. The network that controls wing
development in insects \autocite[Young:1961nx]{Zera:2003cr} is present
in Hymenoptera; in the presence of JH it is fully expressed leading to
queens with wings, whereas in JH's absence, wing development is
suppressed.

\subsubsection{Synthetic biology}\label{synthetic-biology}

ADD IN NOTES

Minimal cells

\begin{itemize}
\item
Minimal cell created either by removing elements from simple cell to
produce functioning minimal cell or alternatively by synthesising cell
from bottom-up from a set of hypothesised necessary elements.
\item
``chemical reaction networks coupled to containers''--protocell
definition, from Protocells:Back to the Future workshop materials
(http://www.unamur.be/en/sci/naxys/pb2f).
\item
Containers useful for protection, for concentration of elements, and
for selection, allowing benefits to accrue to originator.
\item
Ganti's observation that contemporary living things always have a
metabolic subsystem, a heritable control system, and a boundary system
to contain (in \autocite{Szathmary:2006ty}).
\end{itemize}

\subsubsection{Modelling--understanding biological
processes}\label{modelling---understanding-biological-processes}

Regulation Network modelling--perhaps just one or two examples only

\paragraph{Aevol}\label{aevol}

Describes the genotype-phenotype map of an artificial organism modelled
closely on a prokaryotic cell

The genotype is a double-stranded circular bit-string with genes marked
by promotor sequences and terminated by sequences that can form a
``stem-loop'' structure

Expression levels of a gene are based on the degree of similarity
between the gene's promotor sequence and a predefined ``consensus''
sequence (as calculated by the Hamming distance), as is the case in
real-world prokaryotes where the basal transcription level depends on
the quality of the promotor

Codon translated according to an artificial genetic code into an
amino-acid equivalent in a protein

Proteins describe abstract ``processes'' instead of chemical
interactions, where a process is described by a triangular probability
distribution

The proteome is then either the superimposition of all protein
distribution, or as the network of their ``functional interactions''
(the overlap between two protein distributions.)

The phenotype of the organism is then the set of all positive processes
(as calculated by Lukasiewicz fuzzy operators); that is, all activated
but not inhibited processes

Selection modelled by a distribution. The closer the match between the
phenotype and the environment distribution, the fitter the organism

\paragraph{Regulatory or RAevol}\label{regulatory-or-raevol}

fine-grained time model; protein concentrations vary dynamically
(instead of remaining static as in aevol), and the organism's proteome
and phenome change over time to reflect these protein changes

Expression level up or down regulated by concentration of proteins that
may bind to two regulatory sites (an enhancer site and an operator site
for inhibition) each 20-bits long.

The ability of a protein to bind to the binding site is calculated from
a predetermined ``affinity'' matrix that mediates between the DNA
bit-string representation of the binding site, and the amino-acid
representation of the protein.

Interaction with the environment: during simulation, a signalling
protein of set sequence that has no metabolic function but can act as a
\gls{tf} is sent to the organism when the target environment changes.

\paragraph{Biosys}\label{biosys}

Signalling protein concentrations to continually connect the developing
system with its surrounding environment, according to the ideas of
autopoiesis and embodiment theory.

bit-string genotype contains 8-bit genes made up of a number of 4-bit
``activatory'' sites (equally divided between inhibiting and activating
sites) and a 4-bit ``output template.''

Genes encode 3-bit proteins with the first 3-bits of the output template
directly specifying the protein, and the last bit by setting the initial
level of activation for the gene effectively determines if the gene is
constitutively expressed or if it requires activation for expression.

The model shares similarities with
\autocite{Reil:1999rp} \autocite[sec.~2.2]{Quick:2003uq}. Differences however lie
in the environmental connection, and in the modelling of the affect of
\gls{tf} upon expression. In this, it more closely resembles the later
work by \autocite{Rohlf:2009sh}, \autocite{Banzhaf:2003kx} and
\autocite{Kuo:2006fv}.

\paragraph{Knabe2006}\label{knabe2006}

gene can have multiple regulatory modules (``\emph{cis}-regulatory
modules'') and each module can contain multiple protein binding sites

motivated by a desire to approach the complexity of regulatory
interactions found in nature \autocite[16]{Knabe:2006vn}

the activation level of a gene is now calculated by an expression that
adds or subtracts only the \emph{minimum} amount for any protein bound
to a \emph{cis}-module, and the genotype is now an integer-string
(instead of the bit-string found in \autocite{Quick:2003uq}) to
accommodate two new coding signals : `2' to mark the end of a
\emph{cis}-module and `3' the end of a gene. Otherwise the basics are as
found in the \emph{Biosys} model

``smooth binding'' based on the Hamming distance between protein and
site (similar to the mechanism in \emph{aevol} and \emph{RAevol})

Specificity Factors (SFs), proteins produced by the same gene as their
companion regulatory protein, are used to further modify the binding
action of the regulatory protein, where the concentration of the
associated SF adjusts the sensitivity of the binding

\subsubsection{Systems Chemistry}\label{systems-chemistry}

Transition to biology--Origins of life

Natural selection in pre-biology

ADD IN NOTES

\subsection{AI}\label{ai}

Early review of AL approach to AI in
http://www.mitpressjournals.org/doi/pdf/10.1162/artl.1993.1.1\_2.75

Rodney Brooks/Maes--actionist approach to robotics

Intelligence needs environment

Manipulating symbols that are not grounded in the environment doesn't
look like it'll result in emergent intelligence

Intelligence arose from interactions with the world

Understanding of intelligence based on humans--replicating this
difficult. And yet, hard to generalize from this one sample. So rather
than trying to recreate human intelligence, let intelligence emerge from
interactions with world

\begin{itemize}
\item
Similar approach to emergent Alife
\item
The AL approach to AI
\end{itemize}

Similar problems to Alife

\begin{itemize}
\item
``Life'' and ``Intelligence'' are both hard to define and measure
\item
Both appear emergent--that is, hard to measure progress--either yes
or no
\item
Best measures for both may be Turing-like--if it quacks like a
duck\ldots{}comparison to existing life or human intelligence
\end{itemize}

\section{Guide to this work}\label{guide-to-this-work}

\subsection{Start with evolution}\label{start-with-evolution}

In the Spencerian sense of progressive improvement, rather than
necessarily any particular mechanism (e.g., evolution by natural
selection)

Simply a change of population distribution over time--but if just
purely distributive (sensu \autocite{Bourrat2015}), rather than transformative, not
very interesting

Abstract idea, different from mechanisms such as ENS (grounded perhaps
inseparably in biology) e.g., ``Darwin's theory of evolution by natural
selection is restricted in scope. One sense in which it is restricted is
that it refers to organisms.'' \autocite{Griesemer2005}

ENS as ``a theory of descent with modification''

\subsection{Preliminary Investigation}\label{preliminary-investigation}

ToyWorld

Difficulties\ldots{}

\begin{itemize}
\item
Reflective of Pattee's criticism of Alife models in \autocite{Taylor2001} -
``simulations that are dependent on ad hoc and special-purpose rules
and constraints for their mimicry cannot be used to support theories
of life'' \autocite[68]{Pattee1988}
\item
Too biological
\item
Too special-purpose
\end{itemize}

\subsection{Evolutionary Potential}\label{evolutionary-potential}

\subsection{Search for Creativity}\label{search-for-creativity}

\section{Previous publications}\label{previous-publications}

A version of Reactant and Product Strategies \cref{reactant-and-product-selection-strategies} was published as \cite{Young2015},
and material from \cref{model-validation} in \cite{Young2013}.

ToyWorld is available under an GNU GPL v2 open source licence from GitHub \cite{toyworld}.

\section{Contributions}\label{contributions}

\begin{enumerate}
\item
A grounding for Artificial OEE in evolutionary theory
\item
Continue work towards a non-biological perspective on evolution
\item
Progress towards OEE in artificial system--evolution compatible with
OEE without necessarily showing OEE (which is hard to measure and
prove)
\end{enumerate}

\part{Evolutionary Potential}

\chapter{Introduction}

In this section we explore the following research questions:

\vspace{0.3cm}
\begin{minipage}[l]{0.95\textwidth}
\begin{enumerate}[label=RQ\arabic*:]
\item Can inheritance emerge from simple selection and variation?
\item Can evolution act on the inheritance mechanism to tune it for varying environmental conditions?
\end{enumerate}
\end{minipage}
\vspace{0.3cm}

V+S-\textgreater{}I

``Context determines fitness'', where the environment is stable and
affects the development of the entities

Minimal conditions for evolutionary system capable of open-ended (but
not necessarily interesting behaviour)

\begin{itemize}
\item
  Elements that allow for ongoing evolution--necessary, and starting
  point for novelty
\item
  Open-ended evolution can be seen as evolution in an open-ended system
  (\eg Chemistry), where an open-ended system has effectively
  unrestricted representation: the number of possible types must be much
  larger than the number of individuals (ideally without any
  restriction). Without this property all possible types can be
  generated in a finite time, and the system will either reach stasis or
  begin to repeat. Not all open-ended systems necessarily support
  evolution, but in those that do, our intuition suggests that
  open-ended evolution produces increasing complexity, increasing
  diversity, accumulation of novelty and continual adaptation
  \autocite{Lehman2012}.
\end{itemize}

\quote{
by open-ended evolutionary capacity we understand the potential of a
system to reproduce its basic functional-constitutive dynamics, bringing
about an un-limited variety of equivalent systems, of ways of expressing
that dynamics, which are not subject to any predetermined upper bound of
organizational complexity (even if they are, indeed, to the
energetic-material restrictions imposed by a finite environment and by
the universal physico-chemical laws.}
{\autocite{Ruiz-Mirazo2004}}

\begin{itemize}
\item
  Open-ended from \autocite{MaynardSmith1999} definition--\TODO{ size of search space vs population}
\item
  Heritability a challenge--biological organisms employ digital
  heredity; sophisticated mechanism with controlled error rates, but
  exceedingly unlikely to arise spontaneously
\item
  Multiplication/heredity for maintenance of population
\item
  Analog methods possible--\eg:

  \begin{itemize}
  \item
    compositional (where new entity contains some elements of original)
    (as seen in ACS ``core'' inheritance e.g., \autocite{Vasas2015, Watson2012}?)
  \item
    \quote{migrant pools}{\autocite{Watson2015}}
  \item
    Group fissioning \autocite{Watson2015}
  \item
    Attractor based \autocite{Szathmary2000}
  \end{itemize}
\item
  Heredity seen as method to maintain low entropy over much longer time
  than possible with non-''biological'' systems \autocite{Adami2015}
\item
  Argument that heredity may in fact be a product of evolution rather than a precursor \autocite{Bourrat2015}
\item
  \autocite{Kauffman:1993kk} argued that self-organization (RAPN) can replace the genome
\end{itemize}

\section{Previous work}

General difficulties:
\begin{itemize}
\item Somewhat arbitrary choices of elements of description
\item Genotype/Phenotype, Selection,\ldots{} often based on goal of rationalizing existing descriptions, so not a re-examination
\item Lack causality--so hard to use as mechanism
\item Leave options and alternatives for implementer
\item Sheer number of EA algorithms
\end{itemize}

Proofs of effectiveness:
\begin{itemize}
\item Base equations from Malthus and population genetics
\item \autocite{Vose:1999di} for EAs
\item Underlying assumptions should be maintained in our models
\end{itemize}

\autocite{Godfrey-Smith2007}
\begin{itemize}
\item Biological summaries
\item Purpose of summaries as opposed to Formal models
\item Identify the major elements in biology
\end{itemize}

\autocite{Bourrat2015}--Not all of these elements are essential

Other attempts at general models:
\begin{itemize}
\item \autocite{VonNeumann1966} as reviewed in \autocite{Taylor:1999sc} (Lack of environmental emphasis)
\item \autocite{Waddington2008} as reviewed in \autocite{Taylor:1999sc}--Originally published in ``Towards a Theoretical Biology, Vol. 2'' in 1969
\item \autocite{Paixao2015}--reconciliation of EA models with PG models--A synthesis rather than a reformulation--way of describing both A and B using generalizations of each. Not a causal explanation
\end{itemize}

\section{Method}\label{method}

Goal is a minimal description of necessary factors--enough to build a
mechanism

Strategy is to make as many factors as possible endogenous rather than
needing specification by the modeller--more general and hence more
powerful claim

Therefore separate out descriptive and causal descriptions of process

Descriptive--outcome or goal--based on Evolution. Not useful for
construction, but rather for testing

Causal--mechanism

\begin{itemize}
\item
  Want to remove extraneous factors
\item
  More factors introduce complications
\item
  Complicate identification of necessary conditions vs contingent
  factors
\item
  Ideally minimal choices to be made or parameters to be chosen
\item
  Broadest claim
\item
  Worth reflected in proportion of population--core principle of method
\end{itemize}

\section{Mechanism is cumulative adaptive evolution}\label{mechanism-is-cumulative-adaptive-evolution}

From earlier work, attempt a synthesis based on common factors

\section{Assumptions}\label{assumptions}

No learning mechanisms--changes during lifetime

Corollary--addition and removal of elements

\begin{itemize}
\item
  If have no removals then population is either static (contradiction)
  or indefinitely expanding (practical problem)
\item
  If have no additions, then cannot adjust population proportions
\end{itemize}

\section{Contributions}

\begin{itemize}
\item General model for evolutionary processes
\item Experimental support for hypothesis that selection and variation sufficient for evolution
\item Experimental support for ability of a copy mechanism with adjustable fidelity to auto-adjust to changing environmental conditions
\end{itemize}

\chapter{Synthesis based on first principles}\label{synthesis-based-on-first-principles}

\section{Variation (and Inheritance)}\label{variation-and-inheritance}

Assume that variation occurs only on addition

\begin{itemize}
\item
  If during life then, by definition, a learning mechanism
\end{itemize}

Corollary: no Lamarckian inheritance

Variation is in some element from parent to offspring

\begin{itemize}
\item
  Degree of relationship interesting
\item
  No relationship (no covariance) is as before--no learning
\item
  If not related, then effectively random search--process is not
  `learning'--no information inheritance
\item
  Duplication is effectively the same as just extending the lifespan -
  not interesting
\item
  Somewhere in between (between 0 and 1) means heredity/inheritance
\item
  Earlier work in biology has connected evolvability to mutation
  rate/inheritance rate
\end{itemize}

\section{Selection}\label{selection}

\subsection{Exogenous}\label{exogenous}

External calculation used directly to adjust population

Or guided evolution--external agency adjusts population directly

But as external element shared across population not evolvable

\subsection{Endogenous}\label{endogenous}

Feedback loop between element and everything else (in the sense of Pattee’s ``semantic
closure’’ discussed extensively in \autocite[sect. 3.5]{Taylor2001})

Causally results in a proportional change in population proportions

e.g., resource competition, or some other interaction between element and other things

Could occur on fixed timeframe (fixed generations, as common in
biological modelling) or continuous

More generally, on range from \textasciitilde{}0 (continuous selection)
to 1 (generations), although mid-points likely to be of little
additional value

Affected by element and by everything else, so conceivably could be
guided by modifying the environment in a calculated way

\section{Issues--still parameters and choices}\label{issues---still-parameters-and-choices}

When do additions happen?

\begin{itemize}
\item
  Degree of correlation between addition events and selection events
\end{itemize}

\autocite{Bourrat2015} introduces a check-for-overcrowding step--is
this necessary? Under endogenous selection shouldn't overcrowding also
be endogenous?

\section{Can a causal reading further simplify the model?}\label{can-a-causal-reading-further-simplify-the-model}

Reproduction follows from replication--no need to choose degree of
similarity, emergent from lower level rules

\chapter{Synthesis from a general evolutionary model}\label{synthesis-from-a-general-evolutionary-model}

Reformulate as causal reading

\autocite{Bourrat2015} showed that inheritance bias (correlation in trait between parent and offspring) increases over time


TODO
\begin{enumerate}
\item
Fitness independent of inheritance potential--as explored in
\autocite{Bourrat2015}--bias applied only to bias value of offspring, not fitness
\item
fitness dependent on inheritance--more likely. A mechanism that
doesn't copy well unlikely to preserve information leading to high
fitness\ldots{}--the parent's fidelity influences the offspring's fidelity, and to offspring's fitness
\end{enumerate}

\begin{itemize}
\item
  Model 4--proportion of high fitness entities rapidly increases to
  near 1.0
\item
  Model 5--both the proportion of offspring that are procreators
  (rather than persistors) and the heritability of ability to procreate
  increases over time to 1.0
\end{itemize}

\section{Hypothesis 1: Variation and Inheritance and Selection are sufficient for Evolution}\label{h1}

\textit{Hypothesis 1}: Variation and Inheritance and Selection are sufficient for Evolution, or V+S+I-\textgreater{}E

From previous work\ldots{}

For some forms of V, S, I, E\ldots{}what are requirements?

\begin{itemize}
\item
  For I, reasonable to define by degree--\autocite{Bourrat2015} = bias
  or degree of correlation between parent and offspring
\item
  For S, compatible with resource competition--endogenous fitness
\item
  For V, compatible with copy mutation
\end{itemize}

Can we construct a causal diagram from variation and selection that:

\begin{itemize}
\item
  complies with requirement (optimizer, population dominated by best)
\item
  less choice, more inevitable, less arbitrary
\item
  foundation for creativity?
\end{itemize}

\section{Hypothesis 2: Correlated Variation and Selection are sufficient for Inheritance}\label{h2}

Inheritance comes from a copy mechanism under evolutionary control, such that the fidelity of the copy may
be varied by interaction with the environment.

Full or complete inheritance implies that the correlation (or fidelity) between parent and offspring trait approaches 1.0. 

\textit{Hypothesis 2}: Variation, where there is some initial correlation between generations for a property, and Selection are sufficient for Inheritance, or $V'+S\rightarrow E$

\TODO{ I iff V'}
\TODO{ mechanism vs measure for fidelity}

Can this be strengthened to include V+S necessary for I, or I-\textgreater{}V+S?

\section{Predictions}\label{predictions}

In an unchanging environment, where the relative fitness of an entity, with respect to the environment, does not change over time, our expectations are:

\begin{enumerate}
	\item Average inheritance will tend towards perfect inheritance.
		Higher fitness entities will survive longer and reproduce more; higher fidelity reduces variation in fitness and so results in higher fitness being preferred.
	\item Population variance for inheritance will decrease more than chance.		
\end{enumerate}

Under changing environmental conditions, where the fitness of an unchanging entity with respect to the environment changes in response to environmental changes, we expect:

\begin{enumerate}
	\item The \gls{sd} of final fidelity under changing conditions \textgreater{} that under fixed conditions.
	\item The higher the variability in the environment, the higher the \gls{sd} of \emph{Fidelity}.
	\item \emph{Fidelity} at the end of a run under changing conditions \textless{} that under fixed conditions.
	\item The final \emph{Fitness} will be in the the range described by the distribution applied in the $tweakFitness$ function.
\end{enumerate}

\section{Theory}\label{theory}

Degree of variation between generations important (no correlation means
effectively unguided search, complete correlation means no source of
novelties)

  Problem is how can the optimal degree of variation (that is,
  inheritance) be established endogenously rather than as a model
  parameter?

Inheritance related to variation (between generations). Low variation
implies high inheritance

Selection strengthens degree of inheritance

  high fitness lineages more successful. Inheritance increases
  correlation along lineage, so high fitness more likely to be passed
  down. (Also low fitness, but they will suffer). On average then high
  inheritance increases average fitness over time

\section{Alternative explanations}\label{alternative-explanations-1}

The three main alternatives, examined later in \cref{elimination-of-alternative-explanations} are:

\begin{enumerate}
	\item Variation alone is sufficient for Inheritance, or $V\rightarrow I$.
	\item Selection alone is sufficient for Inheritance, $S\rightarrow I$.
	\item Variation and Selection, without trait or property correlation, is sufficient for Inheritance.
\end{enumerate}

\chapter[Test of Hypothesis 2 under fixed conditions]{Experimental test of Hypothesis 2 under fixed conditions}\label{experimental-test-of-h2-under-fixed-conditions}

%% begin.rcode functions, echo=FALSE
% load_dfa <- function(t) {
%   colClasses <- c("numeric","numeric","integer","integer","numeric","numeric","numeric","numeric","integer","integer","numeric","numeric","factor","factor","factor","factor","factor","factor","factor")
%   df <- read.csv(t, colClasses=colClasses)
%   df$response_cor <- df$final_ave_cor-df$initial_ave_cor
%   df$response_fit <- df$final_ave_fit-df$initial_ave_fit
%   df
% }
% load_dfb <- function(t) {
%   colClasses <- c("numeric","numeric","integer","integer","numeric","numeric","factor","factor","factor","factor","factor","factor","factor","factor")
%   df <- read.csv(t, colClasses=colClasses)
%   df
% }
% myqqplot <- function(x,y,...) {
%    sx <- sort(x)
%    sy <- sort(y)
%    lenx <- length(sx)
%    leny <- length(sy)
%    if (leny < lenx) 
%	    sx <- approx(1L:lenx, sx, n = leny)$y
%    if (leny > lenx) 
%	    sy <- approx(1L:leny, sy, n = lenx)$y
%	qplot(x,y,...)
% }
%% end.rcode

To test our hypothesis we turn to an experiment. If a simulation that accurately represents the system in the hypothesis behaves in a way that matches our predictions, the hypothesis has support. On the other hand, if the behaviour doesn't align with the predictions, our hypothesis will be rejected. 

Experimental tests are the strongest method we have to examine the claims made in Hypothesis 2. As explored earlier in \cref{the-legitimate-role-of-simulations}, logical argument or theorem-proving is difficult to apply to model-based systems. Thought experiments lack the strength we hope for, while experimentation is both feasible and, assuming correct design, rigorous.

Note that for those familiar with the design of experiments in the physical world, there are some differences in simulations, with the most significant being the sources and understanding of experimental errors. In simulation, experimental runs are exactly reproduceable, absent any dependency on factors external to the simulation. Variation is explictly introduced usually through a random number generator, which can be seeded to produce the same sequence of numbers again and again. This means that the practice in real-world experiments of ``blocking'' to control external variation is not required in simulation experiments. However, \gls{replicate}s where the same combination of factor values is run several times each with a different random seed value, remains valuable, but in this case less to control for experimental error and more to examine the variation or range exposed in the response.

Our goal for this first set of experiments is to:
 \begin{enumerate}
	\item Experimentally test the predictions of Hypothesis 2 under a fixed environment (where the fitness of an entity does not change), and 
	\item Identify the strength of support for the Hypothesis across a variety of possible influences or factors.
\end{enumerate}

\section{Base model}

All experiments in this section \cref{experimental-test-of-h2-under-fixed-conditions} make use of the same base model where the key elements of the hypothesis, such as inheritance fidelity, are explicit (or endogenous, to use the language adopted by \cite{Bourrat2015}) parameters in the model. This model will be quite familiar to anyone from Evolutionary Computation or Evolutionary Biology.

We define a population of entities, each with two properties -- \emph{Fitness} and \emph{Fidelity} -- and a set of population-transforming functions (\cref{base-model-algorithm}) .

\begin{itemize}
	\item \emph{Fitness} represents the probability that an entity will survive and possibly also reproduce, and has the usual range for a probability of $[0,1]$. 
	\item \emph{Fidelity} is a measure of the correlation between the child's value for a property and the same trait's value in the parent. The range is $[0,1]$ where $0$ means that the value for a child's property is has no correlation with its parent's value for that property, while high \emph{Fidelity} values mean high correlation, and when $fidelity = 1.0$ the child's value is the identical to the parent's.
\end{itemize}

The specific relationship between parent and child property values is given by a single mapping, represented in the algorithm below by the function $f:[0,1] \times [0,1]\leftarrow [0,1]$. 

The only functions in the base model are for the two core elements from the Hypothesis, \emph{Selection} (\cref{function-selection})and \emph{Variation} (\cref{function-variation}), although in later sections we will add others to examine the sensitivity of the model to various influences.

A \gls{run} of the model consists of a fixed number of time steps, or generations, where at each step the functions are applied in a defined order to the current population to form a replacement population, starting from some defined initial population. In familiar terms, the replacement population is formed by a some combination of parents from the current population, plus children derived in some way from the parents. 

\begin{figure}[htb]
\centering
\includegraphics[width=0.95\linewidth]{figures/model}
\end{figure}

\begin{algorithm}
\For{generation $\in 1\dots$number of generations}{
\tcp{Core algorithm}
$population\leftarrow Selection(population)$\;
$population\leftarrow population \cup Variation(population)$\;
\BlankLine
\tcp{Sustainable?}
\uIf{population size is too small}{
break\;
}
}
\caption{Algorithm for the base Model}\label{base-model-algorithm}
\end{algorithm}

\begin{function}
\Def{Selection(population)}{
	$population_{new}\leftarrow \{\}$\;
	\For{each $entity$ in $population$} {
		$p_{selection}\leftarrow $parent's fitness\;
		\Prob($p_{selection}$:){
			Add $entity$ to $population_{new}$\; % with probability $p_{selection}$\;
		}
	}
	\Return $population_{new}$\;
}       
\caption{Selection()}\label{function-selection}
\end{function}

\begin{function}
\SetKwFunction{f}{f}
\Def{Variation(population)}{
	$children\leftarrow \{\}$\;
	\For{each $entity$ in $population$}{
		$p_{reproduce}\leftarrow $parent's fitness\;
		\BlankLine
		\Prob($p_{reproduce}$:){
			$n\leftarrow \text{some random integer between 0 and }n_{children}\text{, inclusive}$\;
			\BlankLine
			\For{each of the $n$ children}{
				Child's fitness $\leftarrow $\f{parent's fitness, parent's fidelity value}\;
				Child's fidelity $\leftarrow$ \f{parent's fidelity value, parent's fidelity value}\;
				Add new child with these values for fitness and fidelity to $children$\;
			}
		}
	}
	\Return $children$\;
}
\caption{Variation()}\label{function-variation}
\end{function}

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\linewidth]{figures/correlation}
\end{figure}

\begin{table}
	\begin{center}
		\tiny
		\caption{Model parameters}\label{tbl:factor_definitions}
		\begin{tabular}{@{}lllp{7cm}@{}}
			\toprule
			& Factor&  Values& Description\\
			\midrule
			0&  $p_{reproduction}$&       $[0,1]$&       Default probability of reproduction. The probability of reproduction for a particular entity is $p_{reproduction}$ unless $p_{reproduction}=0$ in which case it is the parent's $fitness$\\
			1&  $p_{selection}$&          $[0,1]$&           Default probability of selection, where again if $p_{selection}=0$, use the parent's $fitness$ instead\\
			2&  $n_{children}$&           $n_{children}\in \mathbb{Z}_{\ge 0}$& Maximum number of children per parent\\
			3&  Limit population size&    $\{\mathrm{true}, \mathrm{false}\}$&  Use initial population size as upper limit on size\\
			4&  $f$&                      $(\mu,\sigma)\mapsto x$&    Function for generating a sample value $x$ from a particular distribution specified by mean $\mu$ and standard deviation $\sigma$\\
			5&  Correlate Fitness&        $\{\mathrm{true}, \mathrm{false}\}$&  Child's fitness is related to parent's fitness (by $f$)\\
			6&  Correlate Fidelity&       $\{\mathrm{true}, \mathrm{false}\}$&  Child's fidelity is related to parent's fidelity (by $f$)\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\subsection{Claim of generality}

The model as described is general, under the assumptions given below. It is also directly comparable to models from Evolutionary Computation and so we also expect any results and conclusions to be relevant to EC models.

\subsection{Assumptions}\label{assumptions-1}

The model incorporates several assumptions which are worth making explicit:
\begin{enumerate}
\item \emph{Fidelity} and \emph{Fitness} are taken from a distribution $f$ in the range $[0,1]$, around a mean $\mu$ and with some \gls{sd}. \emph{Fitness} and \emph{Fidelity} are both \TODO{ implications on shape of distribution.}
\item \emph{Fitness} and \emph{Fidelity} are independent, implying that \emph{Fidelity} changes have no \emph{direct} effect on \emph{Fitness} (and vice versa.)
\item A parent cannot directly influence the fitness of children--selection is an unseeing hand.
\end{enumerate}

\section{Experimental design}\label{experimental-design}


\subsection{Response variables}\label{response-variables}

From the predictions of Hypothesis 2 (\cref{predictions}) the main property of interest is \emph{Fidelity}, or the correlation between parent and child property values. \emph{Fidelity} therefore is our response variable. 

Specifically we use $\overline{fidelity}_{end}$, or the mean value for \emph{Fidelity} (across all replicates) at the end of a run, as under Hypothesis 2 we expect $\overline{fidelity}_{end}$ to approach 1.0 in an unchanging environment. \TODO{ need this?} In addition, as we predict that \emph{Fidelity} will increase, we expect $\overline{fidelity}_{end}--\overline{fidelity}_{start} > 0$.

\subsection{Factors and levels}\label{factors-and-levels}

We use a fractional factorial design to reduce the number of experimental runs required when compared to a full factorial design \autocite{Montgomery2009}. 

A full factorial design with seven 2-level factors would require testing $2^{7}$ combinations of factor values, or 128 sets of replicated runs, while a $2^{(7-3)}$ Fractional Factorial design \footnote{\eg  \url{http://www.itl.nist.gov/div898/handbook/pri/section3/eqns/2to7m3.txt}} can reduce this to 16 sets of replicated runs without loss of validity on the assumption that 3-factor interactions and higher are not significant. In other words, a $2^{(7-3)}$ design is sufficient to separate the main effect from any 2-factor interactions.  This seems a reasonable tradeoff between discriminatory power and total number of runs required, given our goals.

\begin{table}
\begin{center}
\tiny
\caption{Factor levels used in the Fractional Factorial experiment design}\label{tbl:factor_levels}
\begin{tabular}{@{}lllp{6cm}@{}}
\toprule
& Parameter&  Low value ('-1')&  High value ('+1')\\
\midrule
0&  $p_{reproduction}$&       Parent's fitness& 1.0 (always reproduce)\\
1&  $p_{selection}$&          Parent's fitness& 1.0 (always survive)\\
2&  $n_{children}$&           2&                5\\
3&  Limit population size&  false&            true\\
4&  $f$&                      Gaussian dist., $\mathcal{N}$&   Uniform dist., $\mathcal{U}$\\
5&  Correlate Fitness&        false&            true\\
6&  Correlate Fidelity&       false&            true\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\paragraph{Other settings}\label{other-settings}

The number of replicates required for a particular statistical power is related to the \gls{sd} of the response variable. 

Based on preliminary runs, we begin with an initial estimate of 10 replicates for each of the 16 sets of factor values given in the design, resulting in 160 runs in total; we confirm that this is sufficient through power calculations for specific tests in the later analysis sections.

\subsection{Effect of an upper bound on population size}\label{check-effect-of-truncation}

\TODO{ truncation has a major effect on conclusions, so good to know either that results are not dependent on truncation...}

In the absence of any restrictions on population size, there is nothing to prevent a population growing beyond the capacity of the simulation system. This is a practical problem rather than a property of the theoretical model, and so to have faith in the simulation and model it's important we eliminate the possibility of introducing bias to the results from the mechanism used to control population size.

The size of the population is driven by how population elements are introduced and removed. In standard Evolutionary Computation (\eg \autocite[50]{DeJong2006}) the choice of strategy is important to the performance and outcomes of the algorithm. New elements can be straight replacements, like-for-like, of their parent, or be placed in competition against elements in the parent population, or completely replace the parent population. Elements may be removed as a result of selection, or through fitness-independent sampling to maintain a particular population size, or through some end-of-life calculation. The population size limit may act as both upper and lower bound on population size to maintain a specific size, or solely as upper bound.

Similar considerations apply to our model. Because we observe that the population size increases exponentially in many experimental runs, some upper bound on population size is needed. In the ``canonical'' Evolutionary Computation algorithm, a population limit results from selection where a set number of elements is extracted from the original population, with elements chosen by one of a wide range of selection algorithms (among many sources, see overviews in \autocite[sect. 4.3.1]{DeJong2006} and \autocite[sect. 4.2]{Vose:1999di}.). Here though we break the selection function from the population size limit in order to qualify the effect of the specific limiting mechanism used.

This section describes two possible mechanisms to implement an upper bound, and attempts to understand the form of the bias each introduces into the experimental results. The first is to conduct a random sample of $n$ elements from the population, without regard for fitness, while the other is to adopt the \emph{truncation} mechanism (described in \autocite[124]{DeJong2006} alongside others) as representative (although strongly elitist) of a fitness-based mechanism.

In both cases the null hypothesis, H$_0$, and alternative hypothesis, H$_1$, are as follows (where $\mu$ is examined for both population mean fitness and fidelity):

\begin{itemize}[label={}]
	\item H$_0$: $\mu_{truncated} = \mu_{untruncated}$
	\item H$_1$: $\mu_{truncated} \ne \mu_{untruncated}$
\end{itemize}

One consideration is that the number of generations at the conclusion of each run will not necessarily be comparable. 
This is as expected, as the purpose of a population restriction here (other rationale may apply in general though, such as increasing selection bias) is to avoid a terminal population explosion.

Therefore when testing H$_0$ we modify the model to complete after 15 generations with a parallel population size limit of 50 times original population size ($=50\times 5000$) for fair(er) comparison

\TODO{ but 15 is too little - still in startup}
\TODO{ also rest of parameters}

%% begin.rcode truncationdensity, echo=FALSE, fig.show='hold', fig.keep='last', fig.cap='Generations completed without upper-bound size restriction (left) or fitness-independent sampling restriction (right).'
% df <- load_dfa('results/results_lowstart.data')
% ggplot(df, aes(final_gen)) + geom_histogram(bins=50) + facet_grid(.~truncate)
%% end.rcode

Finally, the environment was fixed (that is, each element's fitness value does not change) and the initial population followed the low-start case \cref{low-start-case} with other parameters unchanged from \cref{other-settings}.

\begin{algorithm}
	\For{generation $\in 1\dots$number of generations}{
		\tcp{Core algorithm}
		$population\leftarrow Selection(population)$\;
		$population\leftarrow population + Reproduction(population)$\;
		$population\leftarrow Restriction(population)$\;
		\BlankLine
		\tcp{Check if population size is no longer sustainable}
		\uIf{population size is too small}{
			break\;
		}
	}
	
	\caption{Algorithm for the Model with an upper bound on population size. The only difference to the earlier base Model is a call to a \emph{Restriction} function}\label{upper-bound-model-algorithm}
\end{algorithm}

\subsubsection{Fitness-independent sampling}\label{fitness-independent-sampling}

The bound is implemented by a fitness-independent sampling of $n$ elements from the population (as given in the $truncation()$ function in \cref{upper-bound-model-algorithm}), iff the pre-sampling population size is greater than $n$.

\begin{function}
	\SetKwProg{Def}{def}{:}{}
	\Def{Restriction(population)}{
		\Return $\text{Random sample of }n\text{ elements from }population$\;
	}
	\caption{Fitness-Independent Restriction()}
\end{function}

%% begin.rcode fitnessindependentplots, echo=FALSE, fig.show='hold', fig.keep='last', fig.cap='Summary plots for fitness-independent sampling, where the top line contains density plots for fidelity and fitness, and the middle line boxplots showing ranges.'
% df <- load_dfa('results/results_lowstart.data')
% df$truncate <- factor(df$truncate, labels=c("Unbounded","Bounded"))
% ap <- qplot( final_ave_cor, facets=truncate ~ ., geom="density", data=df, xlab="",ylab="")
% bp <- qplot( final_ave_fit, facets=truncate ~ ., geom="density", data=df, xlab="",ylab="")
% cp <- qplot(truncate, final_ave_cor, geom="boxplot", data=df, xlab="",ylab="")
% dp <- qplot(truncate, final_ave_fit, geom="boxplot", data=df, xlab="",ylab="")
% grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2, right="Final average fitness", left="Final average fidelity")
%% end.rcode

%% begin.rcode fitnessindependentqqplots, echo=FALSE, fig.show='hold', fig.keep='last', fig.cap='Q-Q plots for fitness-independent sampling, comparing Unbounded and Bounded results for fidelity (left) and fitness (right).'
% df <- load_dfa('results/results_lowstart.data')
% dfa <- df[df$truncate==-1,]
% dfb <- df[df$truncate==1,]
% ap <- myqqplot(sort(dfa$final_ave_cor), sort(dfb$final_ave_cor), main="Final average fidelity", xlab="Unbounded",ylab="Bounded")
% bp <- myqqplot(sort(dfa$final_ave_fit), sort(dfb$final_ave_fit), main="Final average fitness", xlab="Unbounded",ylab="Bounded")
% grid.arrange(ap,bp,nrow=1,ncol=2)
%% end.rcode

From the plots in \cref{fig:fitnessindependentplots} it is clear that the results are strongly non-normal, so a non-parametric test is appropriate to use to check if the truncated and untruncated results are similar (from the same distribution) and hence bias-free.

%Standard non-parametric tests include Wilcoxon and Mann-Whitney for comparing two
%independent continuous random samples where the underlying distributions
%are known to have essentially the same shape, or a Friedman test where the samples might be related (\eg in block %experiment designs) and the objective is to distinguish differences between treatments.

%% begin.rcode kstest, echo=FALSE
% df <- load_dfa('results/results_lowstart.data')
% x_cor <- ks.test(df[df$truncate=='Unbounded',]$final_ave_cor,df[df$truncate=='Bounded',]$final_ave_cor)
% x_fit <- ks.test(df[df$truncate=='Unbounded',]$final_ave_fit,df[df$truncate=='Bounded',]$final_ave_fit)
%% end.rcode

% Applying a \rinline{x_cor['method']} to determine if the truncated and untruncated results for population mean fidelity are taken from the same distribution provides strong evidence (for population mean fidelity, p-value=\rinline{x_cor['p.value']} and for pop. mean fitness, p-value=\rinline{x_fit['p.value']}) to reject the null hypothesis that $\mu_{truncated} = \mu_{untruncated}$.

We conclude, somewhat surprisingly, that Fitness independent sampling does indeed make a difference to the results, confirming the earlier visual assessment from the box and Q-Q plots \cref{fig:fitnessindependentplots}. Surprisingly as the average fitness before and after the application of the mechanism is, within sampling error, unchanged. It seems that the difference between limited and unlimited populations might come instead from the differences in population size in the selection and reproduction steps of the model rather than from any fitness-modifying actions of the restriction mechanism. 

\subsubsection{Fitness-based selection}

The fitness-based population limit is based on \emph{truncation} from \cite[124]{DeJong2006}, chosen as it is reasonably representative of methods used in Evolutionary Algorithms, and as a highly-elitist algorithm should provide useful contrast to the effectively uniform mechanism of \cref{fitness-independent-sampling}. If the two mechanisms produce similar results then it might be argued that other mechanisms are likely to be similar also.

\begin{function}
	\SetKwProg{Def}{def}{:}{}
	\Def{Restriction(population)}{
		$sortedPopulation\leftarrow$ sorted population by element fitness, in decreasing order \;
		\Return $\text{First }n\text{ elements from }sortedPopulation$\;
	}
	\caption{Fitness-based Restriction()}
\end{function}

\subsubsection{Discussion}
This however recognizes that in both Evolutionary Computation and natural populations, the population limit is a function of the carrying capacity of the environment and therefore fitness is fundamental to the selection.

\subsection{Is the choice of distribution function significant?}\label{distribution-function-1}

gaussian implies a stronger relationship, uniform a broader range and less correlation. Considerations--connection to other fields -> gaussian. But uniform would be worst case--good to know conclusions hold even under these conditions.

\TODO{ complete analysis--H0 is that they are the same, H1 different. Test. Discuss.}

\begin{itemize}[label={}]
	\item H$_0$: $\overline{gaussian} = \overline{uniform}$
	\item H$_1$: $\overline{gaussian} \ne \overline{uniform}$
\end{itemize}

%% begin.rcode distributionfunction, echo=FALSE, fig.show='hold', fig.keep='last', fig.cap='Comparison of Gaussian and Uniform distributions for function $f()$.'
% df <- load_dfa("results/results_lowstart.data")
% df <- df[df$truncate==1,]
% df$distribution <- factor(df$distribution, labels=c("Gaussian","Uniform"))
% ap <- qplot(distribution, final_ave_cor, geom="boxplot", data=df, xlab="",ylab="Final average fidelity")
% bp <- qplot(distribution, final_ave_fit, geom="boxplot", data=df, xlab="",ylab="Final average fitness")
% grid.arrange(ap,bp,nrow=1,ncol=2,heights=unit(0.5, "npc"))
%% end.rcode

\subsection{Initial conditions and settings}\label{initial-conditions}

At the beginning of each run we construct a initial population to some design. Although a randomly chosen population is useful for generality, so as not to introduce any bias that may result from a consistent starting point, our interest is also in testing two core attributes for any evolutionary model. First, under evolution we expect a general increase in fitness over time from any low initial starting point, and second, if a population is already in an evolved state with a reasonably high relative fitness we would not expect to see a decrease over time in fitness. We call these two initial starting points the Low Start and High Start cases respectively, and they are described in the sections below.

\paragraph{Low Start case}\label{low-start-case}

Entities begin with a low initial fidelity and fitness (see \cref{tbl:ic}.) This case is of interest for two reasons: first, directly from Hypothesis 2, and driven by the overall goal of this thesis, we expect to see a population of low fidelity entities  eventually replaced by one of high (or at least higher) fidelity ones. Second, this is analogous to a key step on the path taken in biology from the abiotic world, where early copy mechanisms lacked the capabilities for high-fidelity copying (see earlier discussion in \cref{alternative-approaches}.)

\begin{table}[t]
\begin{center}
\tiny
\caption{Initial conditions for Low Start and High Start cases}\label{tbl:ic}
\begin{tabular}{@{}lll@{}}
\toprule
Case&	Property&  Initial range\\
\midrule
\multirow{2}{*}{Low Start}&			Fitness&	From a uniform distribution $\mathcal{U}$ with range $[0, 0.3]$\\
&													Fidelity&	$\mathcal{U}\in [0, 0.3]$\\
\midrule
\multirow{2}{*}{High Start}&		Fitness&	From a uniform distribution $\mathcal{U}$ with range $[0, 0.9]$\\
&													Fidelity&	$\mathcal{U}\in [0.5, 0.9]$\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\paragraph{High Start case}\label{high-start-case}

In this case, entities start with a higher initial fidelity and a greater range of fitnesses (see \cref{tbl:ic}). This case examines the ability of the model to preserve advantageous entities, although, in the broader scope of the thesis, it is highly unlikely that any randomly created population would be of this initial form. In biological systems, random genetic drift can result in a decreasing fitness trend under certain circumstances \TODO{ref} but as our model omits drift \TODO{ true?} we can eliminate it as an acceptable explanation for any downwards trend.

\section{Analysis}\label{analysis}

\subsection{Low start case}\label{low-start-case-analysis}

As mentioned earlier, this analysis has two goals: \begin{inparaenum}[(i)]
	\item experimentally test the predictions of Hypothesis 2, and 
	\item examine the impact of the factors on the results.
\end{inparaenum}

\subsubsection{Hypothesis tests}
Hypothesis 2 makes two predictions under fixed conditions: 
\begin{inparaenum}[(i)]
	\item Average inheritance will tend towards perfect inheritance, and
	\item Population variance for inheritance will decrease more than chance.
\end{inparaenum}

The first test therefore is to examine if inheritance emerges from low-fidelity/low-fitness initial conditions, with the following null and alternative hypotheses:

\begin{itemize}[label={}]
\item H$_0$: fidelity does not approach 1.0 over a run, irrespective of factor values, or \newline
		$\vert \overline{fidelity}_{end}--\overline{fidelity}_{start} \vert = 0$
\item H$_1$: fidelity increases to near 1.0 over a run, for some factor values, or \newline
		$\overline{fidelity}_{end}--\overline{fidelity}_{start} > 0$ and $1.0--\overline{fidelity}_{end} < \delta$ for some $\delta$ and for some factor values.
\end{itemize}

%\item H$_1$: fidelity increases over a run, for some factor values, or \newline
%$\overline{fidelity}_{end}--\overline{fidelity}_{start} > 0$

%% begin.rcode lowstart, echo=FALSE, fig.show='hold', fig.keep='last', fig.cap='Summary results for Low Start case, showing in the top line an overview for the final average fidelity ($\\overline{fidelity}_{end}$) for each run and a density plot showing the distribution of $\\overline{fidelity}_{end}$. The middle line shows equivalent plots for final average fitness ($\\overline{fitness}_{end}$.) The final line shows the difference $\\overline{fidelity}_{end}--\\overline{fidelity}_{start}$, which by Hypothesis 2, should always be greater than zero.'
% df_low <- load_dfa("results/results_lowstart.data")
% df_low <- df_low[df_low$truncate==1,]
% ap <- qplot(row.names(df),final_ave_cor, geom="point", data=df, xlab='Experiment run',ylab="Final ave. fidelity")+theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
% bp <- qplot(row.names(df),final_ave_fit, geom="point", data=df, xlab='Experiment run',ylab="Final ave. fitness")+theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
% cp <- qplot(final_ave_cor, geom="density", data=df, xlab="Final ave. fidelity",ylab="")
% dp <- qplot(final_ave_fit, geom="density", data=df, xlab="Final ave. fitness",ylab="")
% ep <- qplot(row.names(df),response_cor, geom="point", data=df, xlab='Experiment run', ylab='Final--initial ave. fidelity')+theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
% grid.arrange(arrangeGrob(ap,bp, ncol=2), arrangeGrob(cp,dp, ncol=2), ep, ncol=1)
%# plot(df_low$final_ave_cor, xlab='Experiment run', ylab='Final average fidelity')
%# plot(df_low$final_ave_fit, xlab='Experiment run', ylab='Final average fitness')
%# densityplot(~df_low$final_ave_cor, xlab='Final average fidelity')
%# densityplot(~df_low$final_ave_fit, xlab='Final average fitness')
%# plot(df_low$response_cor, xlab='Experiment run', ylab='Final--initial ave. fidelity')
%% end.rcode

A simple visual inspection of the results in \cref{fig:lowstart} reveals that the fidelity measure is distinctly bimodal, with peaks around final average fidelity values 0.5--0.6 and 1.0. Fitness is also non-normal, but less so than fidelity. This restricts the analysis to tests that do not require assumptions of normality.

%% begin.rcode lowstartcorrelation, echo=FALSE, fig.show='hold', fig.keep='last', fig.cap='Comparison between results for correlated and uncorrelated factors. Top line shows results for the factor \\textbf{Correlate Fidelity} for final average fidelity ($\\overline{fidelity}_{end}$, on the left) and final average fitness ($\\overline{fitness}_{end}$, on right), while the bottom line the same for the factor \\textbf{Correlate Fitness}.'
% ap <- qplot(correlation_correlation, final_ave_cor, geom=c("boxplot","jitter"), data=df, xlab="",ylab="")
% bp <- qplot(correlation_correlation, final_ave_fit, geom=c("boxplot","jitter"), data=df, xlab="",ylab="")
% cp <- qplot(fitness_correlation, final_ave_cor, geom=c("boxplot","jitter"), data=df, xlab="",ylab="")
% dp <- qplot(fitness_correlation, final_ave_fit, geom=c("boxplot","jitter"), data=df, xlab="",ylab="")
% grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2, right="Final average fitness", left="Final average fidelity", top="Factor Correlate Fidelity", bottom="Factor Correlate Fitness")
%% end.rcode

By inspection, all runs resulted in increased fidelity \cref{fig:lowstart}, but only some showed an increase towards 1.0. Those that did were uniformly associated with \textbf{Correlate Fidelity} == 1 (see top-left plot in \cref{fig:lowstartcorrelation}). Child and parent fidelities are related by \emph{f()} (\ie the factor \textbf{Correlate Fidelity} is +1) for all runs where  $\overline{fidelity}_{end} > 0.8$, and effectively unrelated (\textbf{Correlate Fidelity} is -1) for all others. 

In conclusion, H$_0$ can be rejected, and H$_1$ accepted. Inheritance increases regardless of the model design, but is strongest when \textbf{Correlate Fidelity} == 1.

The second test is to examine the \gls{sd} of \emph{Fidelity} changes over time -- by hypothesis, $\sigma{fidelity}$ should decrease.

\begin{itemize}[label={}]
	\item H$_0$: $\sigma_{fidelity_{end}}--\sigma_{fidelity_{start}} >= 0$, for all factor values.
	\item H$_1$: $\sigma_{fidelity_{end}}--\sigma_{fidelity_{start}} < 0$, for some factor values.
\end{itemize}

\TODO{ complete analysis}

\subsubsection{Factor significance and interactions}

%% begin.rcode lowstartanova, echo=FALSE
% df_test <- df_low[df_low$correlation_correlation == 1,]
% m0 <- lm(response_cor~(p_reproduce+p_selection+n_offspring+distribution+fitness_correlation)^2, data=df_test)
% #summary(m0)
% x <- anova(m0)
%% end.rcode
% significant with F = \rinline{round(x['F value'],2)} and p-value \textless{} \rinline{round(x['Pr(>F)'],2)}.

Taking only runs where the factor \textbf{Correlate Fidelity} is +1, final average fidelity is significantly influenced by all factors and by the first-level interactions between $p_{reproduction}$ and $n_{children}$, and $p_{reproduction}$ and $f()$.

Examining this further, factor-by-factor, where \textbf{Correlate Fidelity} is +1:

%% begin.rcode lowstartfactorinfluence, echo=FALSE, fig.show='hold', fig.keep='last', fig.cap='.'
% ap <- qplot(p_reproduce, final_ave_cor, geom=c("boxplot","jitter"), data=df_test, xlab="Reproduction",ylab="")
% bp <- qplot(p_selection, final_ave_cor, geom=c("boxplot","jitter"), data=df_test, xlab="Selection",ylab="")
% cp <- qplot(n_offspring, final_ave_cor, geom=c("boxplot","jitter"), data=df_test, xlab="Offspring",ylab="")
% dp <- qplot(distribution, final_ave_cor, geom=c("boxplot","jitter"), data=df_test, xlab="Distribution",ylab="")
% grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
%% end.rcode

\subsection{High start case}\label{high-start-case-analysis}

\TODO{ follow general Low Start analysis template }

\begin{itemize}[label={}]
\item H$_0$: fidelity remains the same over a run, for all factor values, or \newline $\vert \overline{fidelity}_{end}--\overline{fidelity}_{start} \vert = 0$
\item H$_1$: fidelity changes over a run, given some factor values, or \newline $\vert \overline{fidelity}_{end}--\overline{fidelity}_{start} \vert > 0$
\end{itemize}

%% begin.rcode highstart, echo=FALSE, out.width='0.45\\linewidth',fig.show='hold', fig.keep='all', fig.cap='Summary results for High Start case, showing in the top line an overview for the final average fidelity ($\\overline{fidelity}_{end}$) for each run and a density plot showing the distribution of $\\overline{fidelity}_{end}$. The bottom line shows equivalent plots for final average fitness ($\\overline{fitness}_{end}$)'
% df_high <- load_dfa("results/results_highstart.data")
% df_high <- df_high[df_high$truncate==1,]
% plot(df_high$final_ave_cor, xlab='Experiment run', ylab='Fidelity')
% plot(df_high$final_ave_fit, xlab='Experiment run', ylab='Fitness')
% densityplot(~df_high$final_ave_cor, xlab='Final average fidelity') # Strongly non-normal data--bimodal, with cluster
% densityplot(~df_high$final_ave_fit, xlab='Final average fitness') # less bimodal, although non-normal
%% end.rcode

By inspection of \cref{fig:highstart}, the fidelity measure is again strongly bimodal with a cluster around +0.3, and another between -0.1 and -0.5, approximately.

As in the Low Start case, the clusters are driven by the value of the factor \textbf{Correlate Fidelity}. Fidelity decreased in this High Start case when \textbf{Correlate Fidelity} was +1, and the resulting correlation values were similar to those with \textbf{Correlate Fidelity} = +1 in the Low Start case .

%% begin.rcode highstartanova,echo=FALSE
% df_test <- df_high[df_high$correlation_correlation == -1,]
% m0 <- lm(response_cor~(p_reproduce+p_selection+n_offspring+distribution+fitness_correlation)^2, data=df_test)
% #summary(m0)
% x <- anova(m0)
%% end.rcode

%% begin.rcode power, echo=FALSE
% df_high <- load_dfa("results/results_highstart.data")
% df_high <- df_high[df_high$truncate==1,]
% df_low <- load_dfa("results/results_lowstart.data")
% df_low <- df_low[df_low$truncate==1,]
% between <- var(c(mean(df_high$response_cor),mean(df_low$response_cor)))
% within <- mean(c(var(df_high$response_cor),var(df_low$response_cor)))
% x_power <- power.anova.test(groups=2,n=dim(df_high)[1],between.var=between,within.var=within)
%% end.rcode

The statistical power of the design is \rinline{x_power['power']} with \rinline{x_power['groups']} groups and \rinline{x_power['n']} observations per group.

Again, H$_0$ can be rejected (F = \rinline{round(x['F value'],2)} and p-value \textless{} \rinline{round(x['Pr(>F)'],2)}) and H$_1$, that fidelity increases even under high-start conditions, can be accepted.

\chapter[Test of Hypothesis 2 under changing conditions]{Experimental test of Hypothesis 2 under changing conditions}\label{experimental-test-of-h2-under-changing-conditions}

Where inheritance is using \textbf{Correlate Fidelity} mechanism, expect
that under changing conditions correlation will tend to some lower value
than 1.0, proportional to variance in change (more change, lower
correlation).

Significance

\begin{itemize}
\item
  Support for overall hypothesis that variability derived from
  copying/inheritance mechanism, and that EvoEvo can tune this mechanism
  to suit conditions--no preselected parameters required
\item
  \gls{sd} of correlation should be greater than in a fixed environment
  (benefit to preserving variation)
\item
  An environment change results in a change in fitness for entities.
  Other elements in the entity initially unaffected
\end{itemize}

Same initial conditions (low \cref{low-start-case} and high start \cref{high-start-case} cases) with the addition to the Bounded Model \cref{upper-bound-model-algorithm} as follows to introduce a changing environment by ``tweaking'' the fitness of every entity at some frequency, $frequency$.

\begin{algorithm}
	\SetKwFunction{f}{f}
	\For{generation $\in 1\dots$number of generations}{
		\tcp{Core algorithm}
		$population\leftarrow Selection(population)$\;
		$population\leftarrow population + Reproduction(population)$\;
		$population\leftarrow Restriction(population)$\;
		\BlankLine
		\tcp{Check if population size is no longer sustainable}
		\uIf{population size is too small}{
			break\;
		}
		\uIf{time to change environment}{
			$population\leftarrow tweakFitness(population)$\;
		}
	}
	\caption{Algorithm for the Changing Environment Model used in \cref{experimental-test-of-h2-under-changing-conditions}}\label{changingmodelalgorithm}
\end{algorithm}

\begin{function}
\SetKwFunction{f}{f}
\Def{tweakFitness(population, sd, meanOffset)}{
	$population_{new}\leftarrow \{\}$\;
	\For{each $element$ in $population$}{
		$fitness\leftarrow \text{ fitness of }element$\;
		$mean\leftarrow fitness--meanOffset$\;
		$population_{new}\leftarrow population_{new} + \text{ new Element with fitness } \f{mean, sd} \text{ and fidelity = fidelity of }element$\;
	}
	\Return $population_{new}$\;
}
\caption{TweakFitness()}
\end{function}

\section{Experimental test}\label{experimental-test}

Fluctuating environment with no trend or direction--modelled by applying the distribution function from parameters (i.e., either gaussian or
uniform) with a possible change to the mean fitness each generation. In this model, an individual's fitness may increase or
decrease.

\section{Experimental design}\label{experimental-design-1}

% git commit reference 229445f

Four different runs, with changes to the environment at four different frequencies, but all with the same factor settings:

\begin{table}
	\begin{center}
		\tiny
		\caption{Factor levels for all Changing Environment experiments}\label{tbl:factor_levels_changing}
		\begin{tabular}{@{}llllp{5cm}@{}}
			\toprule
			& Factor&  Value&  Description\\
			\midrule
			0&  $p_{reproduction}$&      -1& Parent's fitness& Probability of reproduction for an entity\\
			1&  $p_{selection}$&          	-1&	Parent's fitness&   Probability of selection for an entity\\
			2&  $n_{children}$&           	-1&	2&                Maximum number of children per parent\\
			3&  Limit population size&  	1&	true&                   Use initial population size as upper limit on size\\
			4&  $f$&                      			-1&	Gaussian dist., $\mathcal{N}$&          Function for generating a sample value from a particular distribution\\
			5&  Correlate Fitness&        	1&            true&                   Child's fitness is related to parent's fitness (by $f$)\\
			6&  Correlate Fidelity&       	1&            true&                   Child's fidelity is related to parent's fidelity (by $f$)\\
			\bottomrule
		\end{tabular}
	\end{center}

\end{table}

Control--unchanging, but the other three runs with the change in environment, and so a call to $TweakFitness$, every 1,5 or 10 generations.

The \gls{sd} of the environmental change was set to 0.90 (parameter $sd$ to $TweakFitness$ equal to 0.90), and $meanOffset$, or the mean change to an individual's fitness as a result of the environmental change, to $-0.2$.  

As before, each run has 10 replicates of 500 generations each, with an initial population size of 5000 entities, and a population limit of 250,000 entities.

\section{Results}\label{results-1}

%% begin.rcode changingenvironmentsummary, echo=FALSE, out.width='0.45\\linewidth', fig.show='hold', fig.cap='Fidelity and fitness over time for four different frequencies of environmental change'
% df<-read.csv("results/results-229445f.data")
% ecf <- factor(df$environment_change_frequency,levels=c(0,1,5,10))
% bwplot(df$ave_cor~ecf, xlab="Generation",ylab="Average fidelity")
% bwplot(df$ave_fit~ecf, xlab="Generation",ylab="Average fitness")
%% end.rcode

\section{Analysis}\label{analysis-1}

Rapid change (every generation) leads to population extinction as individuals in the population are unable to maintain a viable level of fitness.

For 5 and 10 generation change, as predicted. \gls{sd} of 5 generation change
higher than for 10; overall fitness for 5 lower than for 10, both less
correlated and less fit than control.

\subsection{Are the results sensitive to the form of the TweakFitness distribution?}

Our initial analysis was with the the \gls{sd} of the environmental change set to 0.90 (parameter $sd$ to $TweakFitness$ set to 0.90), and $meanOffset$, or the mean change to an individual's fitness as a result of the environmental change, to $-0.2$. 

Are the results specific to this distribution? Or can we extend our claim across a broader range of cases?

We initially tested this by making a single change, setting $sd$ to 0.70.

\subsection{Is the choice of distribution function significant under changing conditons?}\label{is-the-choice-of-distribution-function-significant}

From a visual inspection, we conclude that the results appear very similar for uniform and gaussian.

%% begin.rcode gaussian, echo=FALSE, out.width='0.45\\linewidth',fig.show='hold', fig.cap='Results for Gaussian distribution function $f()$'
% df<-load_dfb("results/results-229445f.data")
% df1<-split(df,df$gen)
% df2<-df1$`500` # last gen
% ecf<-factor(df2$environment_change_frequency,c(0,1,5,10))
% bwplot(df2$ave_cor~ecf,xlab="Changeability of environment (see text)",ylab="Average parent-child correlation")
% bwplot(df2$ave_fit~ecf,xlab="Changeability of environment (see text)",ylab="Average fitness")
%% end.rcode

%% begin.rcode uniform, echo=FALSE, out.width='0.45\\linewidth',fig.show='hold', fig.cap='Results for Uniform distribution function $f()$'
% df<-load_dfb("results/results-b67ce0e.data")
% df1<-split(df,df$gen)
% df2<-df1$`500` # last gen
% ecf<-factor(df2$environment_change_frequency,c(0,1,5,10))
% bwplot(df2$ave_cor~ecf,xlab="Changeability of environment (see text)",ylab="Average parent-child correlation")
% bwplot(df2$ave_fit~ecf,xlab="Changeability of environment (see text)",ylab="Average fitness")
%% end.rcode

\chapter{Elimination of alternative explanations}\label{elimination-of-alternative-explanations}

\section{Variation alone is sufficient for Inheritance}\label{variation-alone-sufficient-for-inheritance---v-i}
	
	As an experimental test, we can discount this alternative hypothesis by showing an example of not(V-\textgreater{}I) or, V and not I.
	
	Models where p\_selection (factors{[}1{]}) == 1.0 effectively have no
	selection (Pr(selected)=1.0), and under changing environment where
	fitness changes each generation {[}Element(factors{[}4{]}(x.fitness-0.2,
	0.95), x.correlation) for x in population{]}, fitness can and does drop
	to zero without entities being removed from population
	
	How to reproduce:
	
	\begin{verbatim}
	factor_values = [0, 1, 0, 1, 1, 0, 1] # factor_defns[for selection] = 1.0, meaning no selection! So zero fitness entities persist...
	factors = [defn[value] for defn, value in zip(factor_defns, factor_values)]
	(initial, final) = model.run(factors, population=init_population(5000, low_start=True), generations=250, population_limit=10, changing_environment=True)
	self.assertEqual(5000, final.pop)
	\end{verbatim}
	
	Appears to get stuck in local maxima--when correlation goes to 1.0 then no scope for change of fitness \ldots{}
	
\section{Selection alone is sufficient for Inheritance}\label{selection-alone-sufficient-for-inheritance-s-i}
	
	Test: show example of S and not I
	
\section{Variation and Selection, without property correlation, is sufficient for Inheritance}

Test: already shown in earlier analysis, specifically in the Hypothesis analysis sections where the factor \textbf{Correlate Fidelity} is set to the low value ("-1"), for example in \cref{low-start-case-analysis} and \cref{high-start-case-analysis}.
	
\chapter{Conclusions}

Given:\newline
Hypothesis 1 (that Variation, Selection and Inheritance are sufficient for Evolution) and,\newline
Hypothesis 2 (that Variation and Selection are sufficient for Inheritance),\newline
we suggest that:

\textit{Hypothesis 3}: Variation and Selection are sufficient for Evolution

Possible extensions

\begin{itemize}
\item
  Trend to environmental change
\item
  Experimental test: demonstrate Evolution given Variation and Selection
\item
  Consistency and sufficiency--classification of existing systems
\end{itemize}

\part{ToyWorld, a semi-realistic Artificial Chemistry}

\chapter{Introduction}

Artificial Chemistries of discrete atoms provide an interesting testbed
for investigating various evolutionary phenomena. Fundamentally, they
provide a tuneable evolutionary system, capable of highly complex
behaviour, built around familiar metaphors (real-world Chemistry, and
potentially Biology). A set of interaction rules describing how atoms
interact gives rise to emergent forms---molecules. At a higher level,
these molecules, under the same interaction rules, also interact in
patterns---reactions.

\TODO{ link emergent levels to copy mechanism}

Still higher emergent levels emerge under favourable conditions.
Reactions may form cycles, where a sequence eventually returns to an
earlier product. Our interest is in identifying the factors that
influence the emergence of these higher levels. Cycles in particular are
interesting as many biological processes are cyclical. Replication,
resulting in an exact copy of an entity, is a macro-example of a cycle;
metabolism is another. Building on the apparent correspondence between
higher emergent levels in Artificial Chemistry evolution and Biology, we
believe like others (e.g., \autocite{Steel2013}) that cycles, of some
form, are a necessary building-block for more complicated structures
again in Artificial Chemistries.

\TODO{ argue that ToyWorld capable of a tunable copying mechanism, similar to that in biology,
but that such a mechanism will be difficult to evolve de novo. Previous work on 
evolution of a DNA/RNA copy mechanism in biology: Eigen's paradox, fidelity requires error-correction,
proteins etc}

\section{Contributions}

\begin{enumerate}
\item
	Open-sourced Artificial Chemistry model
\item
	Progress towards useful heredity in an artificial system
\item
	Progress towards OEE in artificial system--evolution compatible with
  OEE without necessarily showing OEE (which is hard to measure and
  prove)
\item
	Demonstration of formation of ACS in an artificial chemistry (previous
  work with ODEs e.g. \autocite{Hurndall2014} not re-usable in that form)
\end{enumerate}

\subsection{Hypothesis}\label{hypothesis}

Because an Artificial Chemistry
\begin{itemize}
\item
  Meets known constraints for OEE--and we have OOL as an example of OEE
  from natural chemistry
\item
  Choosing an artificial chemistry similar to natural chemistry enables
  an argument by analogy
\item
  Catalysis/autocatalysis possible through emergence in some AChems
  (e.g., \autocite{Virgo2013})
\item
	And TODO..
\end{itemize}

Hx: Compliant copy-mechanism possible in an AChem through analogy with DNA/RNA copying

\chapter{Artificial Chemistries}\label{artificial-chemistries}

\section{Introduction}\label{introduction-2}

Full taxonomy in \autocite{Dittrich:2001zr}, also see \autocite{Faulconbridge2011}

One advantage of \glspl{achem} as pointed out by
\autocite[5]{Funes2001} is that it is easier to evaluate solutions in
a domain close to the real-world as opposed to a purely symbolic or
abstract domain such as a lambda-calculus, or a programmatic environment
like Tierra. When contemplating difficult problems such as complexity
our intuition can be helpful, but only in situations close enough to our
normal experience for it to be relevant.

All Artificial Chemistry based on this model:

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}
\node (silico) at (0,0) {\textit{In Silico}};
\node (vitro) at (4,0) {\textit{In Vitro}};
\node (vivo) at (8,0) {\textit{In Vivo}};
\draw [<->] (silico) -- (vitro) node [midway,above] {validation};
\draw [<->] (vitro) -- (vivo) node [midway,above] {validation};
\end{tikzpicture}
\end{center}
\end{figure}

Or in other words, digital models (``in silicon''), wet-chemistry models
(``in glass''), and life itself.

In general, AChems for these problems are without grounding (so start
from high-level). Grounding implies that there is a causal justification
for the higher level chemistry, where that justification is based on
fundamental supportable processes, rather than unique to particular
problem.

All four areas rely on a connection to real chemistries--Molecular
synthesis and Molecular Programming require results to be transferrable
to In Vitro; other two expect results to shed light on In Vivo
processes.

Therefore expect that a grounded AChem for these should have a basis in
real Chemistry.

\subsection{BOX--Physical or Real-world Chemistry}\label{box---physical-or-real-world-chemistry}

The richness of the real world is built on a substantial foundation of
chemical and physical complexity.

A \textit{reaction} transforms \emph{reactants} into \emph{products},
and is often represented by describing the quantities (or stoichiometry)
of reactant and resulting product molecules. The reaction can also be
characterized by a description of the dynamics, or kinetics, of the
reaction to explain how the reaction proceeds in response to temperature
changes or to varying concentrations of reactants. Reactions often
require an input of energy, such as from molecular collisions, to
proceed; the energy required is called the reaction's activation energy
(\(A_e\)), and is specific to the particular reaction. In general there
is no accurate mechanism to predict reaction dynamics without
experiment.

The \textit{reaction rate} is the change in concentration of a substance
over time: \(\frac{-d[A]}{dt} = k[A][B]\) (where the notation \([X]\)
means that concentration of \(X\) and \(k\) is the reaction rate
constant) leading to Arrenhius' description of the relationship between
the activation energy (\(E_a\)), the temperature (\(T\)) and the rate
constant (\(k\)): \(k = Ae^{E_a/RT}\). The \textit{reaction order}
describes how the reaction rate changes with the concentration of the
reactants, usually captured as a first-, second-, or third-degree
polynomial expression determined empirically. For example, the reaction
rate equation for \(2NO + Cl_2 \rightarrow 2NOCl\) is
\(rate = k[NO]^2[Cl_2]\) (experimentally determined), and is
second-order with respect to \(NO\), first-order with \(Cl_2\) and
overall (the sum of terms), third-order. (Example taken from
\autocite{Kotz2006})

Reactions in theory can be decomposed to a chain of
\emph{elementary steps}, with each step resulting in a single change,
such as bond formation or cleavage, to the reacting molecules.
Elementary steps are somewhat predictable, practical reactions somewhat
less so. Generally in experimental chemistry we know the reactants and
products and can sometimes deduce the sequence of elementary steps.

In modelling, we can either represent reactions exactly as an atomic
transformation from reactants to products with characteristics that can
only be determined experimentally, or we can attempt to construct the
reaction from a sequence of elementary steps with the properties of the
reaction derived from the properties of the steps involved. This later
approach is the only practical one when the reaction is novel, or when
we lack experimental data. Several alternate sequences of steps, or
\textit{reaction pathways}, may be possible between reactants and
products. Each pathway will have a different activation energy, and
hence reaction rate.

The kinetics of elementary steps are defined by the stoichiometry of the
step and are hence predictable. In theory you might expect to be able to
predict the overall reactions kinetics from the composition of these
elementary steps. In practice, the results are close, but not exact,
when compared to experiment. However they form a useful abstraction for
analysis. The reaction rate of an elementary step is defined by the
stoichiometry, where the rate equation is the product of the reactant
concentrations and the rate constant. Therefore a step with one reactant
has order 1, a bimolecular step (\(A + B\)) has order 2 and so on. The
step \(2A + B\) has rate equation \(k[A][A][B] = k[A]^2[B]\) and is of
order 3.

Elementary steps are an abstraction, an aid to analysis, of the
underlying molecular dynamics. At the molecular level, reactions can be
modelled as a series of collisions between molecules. The reaction rate
is then determined by the percentage of collisions (or in other words,
the concentration) that are energetic enough to overcome the inherent
stability of the interacting molecules and cause a change in molecular
structure or shape (in other words, the activation energy).

Recall that the rate of a reaction is a function of concentration (at
the gross level) or collision rate (at the molecular level) and the
activation energy for the reaction, which at the molecular level is
directly related to the energy required to overcome the stability of the
reactants. There are therefore two clear mechanisms to alter the rate
for a reaction: either reduce activation energy, or increase
concentrations. A catalyst does precisely the first, and autocatalysis
is one method for the second.

A catalyst is anything that isn't consumed by the reaction and that
affects the rate (or the kinetic equation for the reaction) without
affecting the reaction's equilibrium constant. Catalysts allow the
reaction to proceed by an alternate, lower activation energy, pathway.
The reaction equation remains the same, but the dynamics are changed -
in the case of biological enzymes the rate can be increased by several
orders of magnitude over the uncatalyzed reaction, enabling reactions
fundamental to life that would be effectively impossible in an
uncatalysed form. At the molecular level catalysts often function by
providing a substrate that preferentially attacks the bonds critical to
the reaction. The platinum within an automotive catalytic converter is a
well-known example of a catalytic substrate.

Autocatalysis, introduced by \autocite{Ostwald1890}, is a different
approach to catalysis. Rather than reducing the required activation
energy, autocatalysis increases the reactant concentrations:
autocatalysis reactions form feedback loops where a compound is both a
reactant and a product. In the standard definition, an autocatalytic
reaction is one that is catalysed by its own products, resulting in a
characteristic rate acceleration over time given by the differential
equation -
\[\frac{dx_i}{dt} = k(\mathbf{X}) \cdot x^n_i + f(\mathbf{X})\] where
\(n\) is the order of the reaction \autocite{Plasson2010}. Example
(indirect network autocatalysis) of glycolysis where pattern is ATP
\(\rightarrow\) n.ATP--ATP initially consumed, but overall created.

Autocatalysis may be realised by 1) either a single reaction (\eg A + X
+ Y \(\rightarrow\) 2A + Z) or a linear chain of reactions through
intermediate products, called template autocatalysis, or 2) by a
reaction network. Networks may be indirect through a series of
intermediary products, or collective where there is no connection
between the component cycles other than through catalysis--as seen for
example in the replication of viroids where each RNA strand can catalyse
the production of the other. However, in all cases, the reactions reduce
to the defining X \(\rightarrow\) nX pattern described by Ostwald's
differential equation.\autocite{Plasson2010}.

A related phenomenon is autoinduction where products increase the
reactivity of the catalyst (rather than directly affecting the reaction
mechanism). Same signature (Ostwald).

Artificial chemistries are regularly employed in three application
areas: real-world chemistry emulators; tools for the exploration of
artificial life, and models to test various hypotheses of the origin of
life. Chemistry emulators and origins-of-life tools aim for fidelity
with real-world chemistry, unlike most artificial life models.
Real-world fidelity requires either the use of a library of predefined
reactions, which conflicts with the goal of unlimited extension, or a
chemically plausible method of constructing reactions from first
principles. Because of the complexities of real-world chemistry this
later method appears to be quite difficult, and the goal of a realistic,
computationally practical, artificial chemistry remains open. However,
the more limited objective of a less realistic, but still fully
constructive chemistry, has been achieved (see Table \cref{classification-of-artificial-chemistries} for
examples.). A constructive chemistry (\autocite{Fontana1994}) is one
where new components may be generated through the action of other
components, and where those new components may themselves take part in
new types of reactions, and so on. This appears fundamental to an
open-ended representation.

A mathematical treatment of \gls{achem} can be found in
\autocite{Benko2009}. \autocite{Dittrich:2001zr} and
\autocite{Suzuki2008a} provide excellent reviews of the field.

\section{Classification of Artificial Chemistries}\label{classification-of-artificial-chemistries}

In \autocite{Dittrich:2001zr}, Achems divided into major types:

\begin{itemize}
\item
  Rewriting or Production Systems (symbols and rewriting rules) -
  Chemical Abstract Machine (CHAM), Chemical Rewriting System on
  Multisets (ARMS), Chemical Casting Model (CCM), Lambda-Calculus
  (AlChemy)
\item
  Arithmetic Operations (molecules are natural numbers, operations are
  arithmetic operators)
\item
  Autocatalytic Polymer Chemistries (polymers made from chains of
  monomer symbols, reactions are concatenation and cleavage)
\item
  Abstract Automata and Artificial Molecular Machines (seem similar)
  e.g., Turing
\item
  Assembler Automata--Avida, Tierra
\item
  Lattice Molecular Systems--\autocite{Ono2000},Madina2003,lattice
  molecular automaton (LMA), Autopoietic System
\end{itemize}

A \gls{achem} can be represented by
\textless{}\emph{S},\emph{R},\emph{A}\textgreater{}
\autocite{Dittrich:2001zr}:

\begin{itemize}
\item
  Set of Molecules (\textless{}\emph{S}\textgreater{})
\end{itemize}

Molecule representation e.g., labelled graphs

Properties of molecules e.g., energy calculation by Extended Huckel
Theory (EHT); salvation energies; reaction rates.

\begin{itemize}
\item
  Reaction rules (\textless{}\emph{R}\textgreater{}) (Chemistry)
\end{itemize}

Transformation from lhs reactants to rhs products. Many (most) reactions
are bidirectional; rates determined by concentrations. Conservation of
energy ? atoms are neither created or destroyed so reactions can be
represented solely by bond changes.

Chemical Abstract Service (CAS) database references approx 34 million
reactions in published work (1) Which ones to adopt? Which reactions
direct evolution to more interesting places?

Reactions may be pre-defined (if S represents real-world molecules) or
dynamically-determined using molecular properties--the problem of how
to select a reaction is addressed by the Reactor Algorithm (below.)

\autocite{Tominaga2007} showed for a particular artificial chemistry,
that it is computationally universal with only unimolecular and
bimolecular reactions.

\begin{itemize}
\item
  Reactor Algorithm (\textless{}\emph{A}\textgreater{}) (Physics)
\end{itemize}

Mechanism to select reactions, e.g., Through the introduction of a
reaction generator (A) the artificial chemistry triplet (S, R, A) is
completely defined \autocite{Lenaerts2009}. Or in
\autocite[sect. 4.1.3]{Faulconbridge2011}, The mixing component of an
Artificial Chemistry is an algorithm which describes the order of and
intervals between reactions, starting from an initial collection of
molecules; this can be termed a ``mixing space''.

\autocite{Faulconbridge2011} identifies three types of
\emph{mixing method} or reactor algorithm:

\begin{itemize}
\item
  Well-mixed/aspatial: with either discrete time (uniform probability
  distribution for selecting reactions) or continuous time
  (Gillespie1976) assuming the reactions are known in advance (not
  possible for a strongly constructive AChem)
\item
  nDimensional: grid with reactions with adjacent cells, or continuous
  where molecules have position and velocity. Major advantage is ability
  to simulate spatial affects; disadvantage is performance
\item
  Mixed scale: hierarchical spaces, such as aspatial cells within bigger
  grid, mostly for simulating biology (e.g., \autocite{Jeschke2008}). One possible
  advantage is potential for parallelization
\end{itemize}

Scaling can be an issue: as new products are generated, the space of
reactions and reactants can expand exponentially, and therefore
performance dives. The general problem is model reduction--see
\autocite{Radulescu2012} for a recent review; \autocite{Faulon2001} has
a well-received method.

Alternative classification in \autocite{Faulconbridge2011}:

\begin{itemize}
\item
  Symbolic--each molecular species labelled by a unique symbol; only
  explicit meanings
\item
  Structured Symbolic \eg \autocite{Hutton2002}--standard, hard to achieve
  emergence or support novel molecules
\item
  Sub-symbolic \eg RBN-World, NAC, ToyChem \autocite{Benko2005}
\end{itemize}

\subsection{Constructive chemistries}\label{constructive-chemistries}

Novel molecules can arise from artificial chemistry:

\quote{
A distinguishing feature of chemistry is that the changes of molecules
upon interaction are not limited to quantitative physical properties
such as free energy, density, or concentrations, since molecular
interactions do not only produce more of what is already there --
rather, novel molecules can be generated.}
{\autocite{Benko2009}}

A chemistry is \textit{constructive} \autocite{Fontana1994} if new components
may be generated through the action of other components. Both implicit
laws and implicit molecule definitions are required for constructive
chemistries:

\begin{itemize}
\item
  Explicit reaction laws = independent of molecular structure; implicit
  = laws must refer to structure. Often used for constructive
  chemistries
\item
  Explicit molecule definitions = from an fixed set of symbols; implicit
  = description for construction.
\end{itemize}

Constructive (weak and strong) initially defined in \autocite{Fontana1994}:

\quote{
We refer to models in which new agents are constructed in an unspecific
(essentially stochastic) fashion as \emph{weakly constructive}. This is
to be contrasted with a situation in which the encounter of two agents
\emph{implies} a \emph{specific} third one\ldots{}Models of this kind
will be termed \emph{strongly constructive}. The prime example of a
strongly constructive system is chemistry.}
{\autocite{Fontana1994}}

The commentary that immediately follows is also significant:

\quote{
A strongly constructive system that contains agent \emph{A} must cope
with the network of its implications. But, then, it also must cope with
the implications of the implications. And so on.}
{\autocite[217]{Fontana1994}}

A strongly constructive system is one which maintains closure, and in
which there is self-consistency and some form of logical structure
(implications.)

A mechanism for exploration in the EA sense--new products can be
generated, and new products can participate in reactions. One approach
is to pre-specify all possible reactions; another is to generate
reactions `on-the-fly' from the structures of the interacting molecules.
The first approach is well suited to simulating real chemistry as it
allows properties of reactions observed in chemical experiments to be
attached to their simulated equivalents. However, it does not allow for
arbitrary reactions, and it requires reaction properties to be
pre-specified--difficult for novel or artificial reactions. It could be
argued that the first approach is not in fact strongly constructive (in
the sense of \autocite{Fontana1994,Dittrich:2001zr}) as it is not completely
open-ended. However, \autocite{Hartenfeller2011} suggests that it is in fact
adequate.

\subsection{Applications in real-world chemistry}\label{applications-in-real-world-chemistry}

Modeling chemical reactions (\eg \autocite{Gibson:2000kx}). Emulators
are often used for backward chaining from a set of desired products to
identify a set of currently-available initial molecules, for example in
drug discovery (e.g., \autocite{Hartenfeller2011}). A second use is in
reaction network discovery, where the goal is to describe a closed set
of reactions and reactants from some initial reactants and reactions
(e.g., \autocite{Faulon2001}). Both applications effectively produce
static descriptions of dynamic processes, and are less useful for
exploring the changes in a network over time.

\autocite{Hartenfeller2012} suggests that
R=\{set of 58 reactions\} might meet requirements for de-novo drug
discovery, but based on S=\{10k-50k building-block molecules\}. Given
much smaller S, will this R still suffice?. Goal of identifying
collection of synthesis reactions that can be used by tools to construct
pathway from provided building blocks to desired compounds. Set of 58
reactions, 29 of which are ring-forming, implemented in reaction-SMARTS
and RDKit.Previous work deficient in its potential to generated
innovative chemo-types, because of the small number of ring-forming
reactions (only 3 ring-formations versus 29 in this work.) Why is a
small set of reactions (50-60) adequate? Most reliable, so transferrable
to bench-top; with reasonable collection of building-blocks (10k-50k)
can fill combinatorial space beyond ability to enumerate. Even with this
set need search mechanism to limit combinatorics.

The primary requirement is fidelity with real-world chemistry, which
requires either a library of empirically derived reaction definitions
and rates, or a model capable of accurately simulating
quantum-mechanical processes. The latter approach has been taken by a
family of Artificial Chemistries, beginning with
\autocite{Benko2003}, built on Extended H\"{u}ckel Theory with parameters
taken directly from chemical experiments and later extended (for example
in \autocite{Benko2005}) to a general purpose model with parameters
derived from theoretical chemistry. The model was used in
\autocite{Hogerl2010} for the study of the behaviour and topology of
chemical reaction networks, specifically Diels-Alder and Formose
reaction networks, and in a series of papers (e.g, \autocite{Flamm2010}
and \autocite{Ullrich2010}) for the examination of the evolution of
metabolic networks in early organisms using a simple model of RNA coding
for catalysts.

An artificial chemistry with the ability to create reactions
``on-the-fly'' given a set of possible reactants may discover more than
one possible reaction pathway between the same reactants and products.
The method used to choose one reaction pathway from the alternatives is
an important component of the artificial chemistry, and the mechanism
may be tuned or tailored to privilege or preferentially chose particular
types of pathways independent to other factors such as temperature or
concentration. In Chemistry the choice of reaction pathway is
fundamentally linked to those other properties and cannot be treated
independently.

Rather than from symbol manipulation, Chemical properties emerge from
fundamental laws. This gives a richness and depth that cannot easily be
equalled in a simulation. For example, the three-dimensional structure
of a molecule--the arrangement of the elements in space--drives even
the simplest chemical reactions. Any (base- or acid) reaction results
from the shift of charge from one region of a molecule to another,
revealing one region while shielding another from activity. Some of the
most complex reactions are shape-driven: the function of many enzymes
(biological catalysts) derives from their shape, and furthermore this
shape is often under regulatory control. A cell can in affect regulate
the activity of an enzyme by either blocking or unblocking the enzymes
active site with another protein. The prediction of the shape and hence
the function of an enzyme from its RNA transcript is perhaps the most
important problem in current molecular biology. In \gls{achem}, an
interesting, but highly simplified, attempt at this can be found in the
work of \autocite{Flamm2010} and \autocite{Ullrich2010}.

Investigating natural selection for chemical evolution
(\eg \autocite{Fernando:2007pf})

Reaction network discovery and Drug discovery

\begin{itemize}
\item
  Chemical Abstract Service (CAS) database references approx 34 million
  reactions in published work
\item
  Either from products back to reactants, or enumeration of all products
  from reactants
\item
  describe a closed set of reactions and reactants from some initial
  reactants and reactions (e.g., \autocite{Faulon2001}
\item
  Realism fundamental
\item
  There is an inbuilt tension between realism and speed in artificial
  chemistries. Realistic reaction selection must involve calculations of
  likelihood, where the current state of the art is to use quantum
  chemistry simulators to determine the properties of a particular
  reaction. Each reaction that fires alters molecular quantities, and in
  models based on a Gillespie-like method, requires the recalculation of
  the firing probabilities for all affected reactions.
\item
  The field has generally progressed from simpler, abstract models (such
  as \autocite{Fontana1992}) to more chemically-realistic ones
  \autocite{Suzuki2008a}.
\item
  Examples
\item
  \autocite{Hogerl2010} etc--Diels-Alder and Formose reaction networks
\end{itemize}

\subsection{Origins of Life}\label{origins-of-life}

Of the historic and ahistoric aspects of the Origin-of-life problem,
historic aspect may never be known \autocite{Pross2013}. Constrained by
what we do know, but many different pathways, and unless some record
somewhere (either geological or phylogenetic), actual path essentially
lost to history. So without evidence for historic aspect, not possible
to test by falsification, and hence can only be speculative.

Consensus forming that early life formed by chemoautotrophs with energy
from inorganic redox couples and biomass from CO\textsubscript{2}, and
that innovations in carbon-fixation created main branches in
tree-of-life \autocite{Braakman2012}. Initiation of selection marked by
\gls{ida}, probably from RNA world, followed substantially later by Last
Universal Common Ancestor (LUCA) \autocite{Yarus2011}, which, it is
important to note for clarity, was almost certainly not a single cell or
even species, but rather a construct of evolutionary genetics because of
the likely predominance of Lateral Gene Transfer (LGT) in archaic
biology (http://sandwalk.blogspot.ca/2007/03/web-of-life.html).
Self-replicating RNA enzymes shown in \autocite{Lincoln2009}, forming
basis of selective system (link to natural selection) (also see
\autocite{Cheng2010}, \autocite{Powner2009} for formation of RNA in
prebiotic conditions). Some elements of \gls{ida} thought still with us
in lineages of informational (for protein synthesis and RNA
transcription) and operational genes (for some standard cellular
processes) \autocite{Ragan2009}, for example the ribosome and
ribonuclease P (RNase P) \autocite{Wilson2009}. Next major transition to
Protein world (although predominance of RNA transcripts leads to
suggestions that should be called RNA-Protein world
\autocite{Altman2013})

Two alternative models for the step from abiotic to \gls{ida}: genetic
or replicators or RNA-first, and metabolism or protein-first. Both
metabolism and replication almost certainly required for \gls{ida}. A
self-sustaining autocatalytic network (in terms of a RAF set
specifically a ``set of molecules and reactions which is collectively
autocatalytic in the sense that all molecules help in producing each
other (through mutual catalysis, and supported by a food set).'')
generally considered essential \autocite{Pross2013}, but not sufficient
\autocite{Hordijk2011}. Both competing models--replication first and
metabolism first--build on that. Autocatalysis expressed by
self-replication of oligomeric compounds in replication first; by cycles
and network in metabolism first. In the broadest sense, life can be seen
as an autocatalytic process where an entity catalyses the production of
one or more descendant entities.

Metabolism-first privileges function, while replicator-first privileges
descent.

If life is metabolism plus information, then for metabolism-first where
lack template-based replication, replication is compositional (composome
- Vasas)

Common functions required in both protocells and minimal cells, but
approaches quite different. Protocells must build up from abiotic
conditions to point where known processes can take over, and to point
where we have historical evidence--bridge gap between abiotic
conditions and first hypothetical cells in record

Main issues with replicator-first model: big step from abiotic compounds
to template-based replication (although ribonucleotides conceivably
could form in pre-life conditions see \autocite{Powner2009}). Templates
encode information in biology, so require a encode/decode mechanism as
well as an information code to represent the product. This is a step
more complex than simpler duplication.

Main issue with metabolism-first model: shift from composome inheritance
to template-based; ability of composomes to fulfill heredity requirement
for natural selection.

Autocatalytic sets are proposed for bootstrapping (from less
differentiated systems to more complex ones), as a mechanism to increase
order and so counteract entropy, for stability, and as a unit of
competition. The catalytic properties of autocatalytic sets result in
bootstrapping, where the reaction rates in the set are ratcheted-up as
the product quantities increase. Autocatalytic sets form a complex or
dynamical system, where the feedback loops and interactions can result
in forms of temporal (waves and cycles) and spatial order (stable
structures.) Similarly stability follows from the formation of
attractors in the complex system that are resistant to pertubation.
Finally, as the set forms a coherent unit expressing certain properties,
such as the ability to maintain itself or to create copies of itself, it
can be seen as a unit of competition where success is measured by
life-time or by control of resources.

Some abiogenesis results fundamentally assume real-world chemistry and
conditions, a constraint that doesn't apply to Alife or artificial OEE,
and so is more restrictive than required. Other abiogenesis work such as
on properties of autocatalytic sets, is broader in applicability.
Genetic and catalytic properties of RNA make well suited to creation of
Alife \autocite{Cheng2010}

Autocatalysis is a source of dynamical behaviour--leads to
bifurcations, multistability, oscillators, attractors
\autocite{Plasson2010}

Some properties of RAF sets established by Hordijk and Steel have
application beyond origin-of-life: linear growth rate in level of
catalysis compared to size of molecules (n) is sufficient for RAF set to
form. For a RAF set to appear with high probability (P\textgreater{}0.5)
in even a model with n=20 (about 1 million molecular types) each
molecule needs to catalyze 1-2 reactions on average. For the given
model, need around 65,000 different molecule types (n=15-16) for RAF set
if use more realistic probability of catalysis of 1:1 million of a
molecule catalysing any particular reaction. \autocite{Hordijk2011}

Lateral or Horizontal Gene Transfer thought so common in early life that
no single common ancestor, but genes from multiple lineages combined
into all lineages today.\autocite{Ragan2009}

Real-world chemical processes are also important to modelling scenarios
for the origin of life or of other related areas such as the formation
of metabolic networks in the earliest protocells

In many cases though the specific focus is less on the bottoms-up model
from the most elementary elements, and more on task-based models of
processes where the particular starting point is predetermined by the
researcher--in the former, Kauffman's autocatalytic protein sets, and
Kaneko's protocell toy model; in the later, Ganti's chemoton.

Examples

\begin{itemize}
\item
  Lattice Artificial Chemistry \autocite{Madina2003,Ono2000}
\item
  the study of membrane formation and cell division assumes five
  different types of particles (some hydrophilic and some hydrophobic)
  that together form an autocatalytic cycle similar to those observed in
  biological cells.
\item
  SCL
\item
  Three types of particle are employed by the Substrate-Catalyst-Link
  (or SCL) chemistry of \autocite{Varela:1974qd,Suzuki2008}: the
  eponymous Substrate, Link and Catalyst. Cells are formed from links
  around a catalyst, with a single predefined reaction rule
  S+S+C\(\Rightarrow\) L+C and some straightforward constraints on
  movement of the particles in the matrix (for example, bonded Link
  particles cannot cross each other.)
\item
  \autocite{Flamm2010, Ullrich2010}--the evolution of metabolic networks in early
  organisms using a simple model of RNA coding for catalysts
\item
  NAC
\item
  \autocite{Dorin:2006fk} the focus is on an ecosystem, based on a set
  of atoms interacting in pre-specified ways that represent biological
  photosynthesis, respiration and biosynthesis (or growth). The goal is
  to explore the interactions in an ecosystem made up of a set of
  organisms pre-built to perform various defined roles.
\item
  \autocite{Gardiner2007}
\item
  string based chemistry to investigate protein metabolism evolution
  under genetic control. Three types of molecule--protein, gene and
  service molecule--react in particular ways associated with the types
  of interacting molecules. Type and pattern of molecules defines type
  of interaction.
\item
  \autocite{Fernando:2008xy},Fernando:2007pf
\item
  flow-reactor for evolution of metabolism in lipid aggregates based on
  predefined types and reactions.
\item
  Natural selection for chemical evolution
\item
  \autocite{Ganti:2003hl}
\item
  \autocite{Dyson1999}
\end{itemize}

\subsection{Alife}\label{alife}

Finally, in Artificial Life, Artificial Chemistries have been used in
the exploration of open-ended or creative evolution. Squirm3
\autocite{Hutton2009,Lucht2012,Hutton2002} adopts fixed molecule types,
and pre-defined reactions for replication and gene-sequence
transcription, and so although capable of interesting behaviour is not
capable of unlimited extension. Stringmol \autocite{Hickinbotham2011} -
a bacterial inspired microprogram chemistry--though does demonstrate a
rich heredity for open-ended evolution using string-matching to model
binding between sequences, and RBN-World \autocite{Faulconbridge2011}
shows that a form of Random Boolean Network, with the addition of a
bonding mechanisms to allow for composition and decomposition of RBNs,
can be used to build a chemistry capable of almost limitless extension
out of non-traditional components.

everything in our artificial world must be built from a common set of
raw materials; a loop connects the targets of selection with the
environment.

Other work, although superficially similar in that it models objects of
similar scale, uses different models for the objects and the world (see
\autocite{Sanchez-Dehesa:2008uq}) and so lack this loop.

\quote{
Erasing the distinction in simulation between organism and environment
allows a model to explore the exchange and transformation of matter and
energy.}
{\autocite{Dorin:2006fk}}

\paragraph{Desirable properties}\label{desirable-properties}

\autocite{Suzuki2003}:

\begin{itemize}
\item
  The symbols or symbol ingredients be conserved (or quasi-conserved) in
  each elementary reaction, at least with the aid of a higher-level man-
  ager.
\item
  An unlimited amount of information be coded in a symbol or a sequence
  of symbols.
\item
  Particular symbols that specify and activate reactions be present.
\item
  The translation relation from genotypes to phenotypes be specified as
  a phenotypic function.
\item
  The information space be able to be partitioned by semi-permeable
  membranes, creating cellular compartments in the space.
\item
  The number of symbols in a cell can be freely changed by symbol trans-
  portation, or at least can be changed by a modification in the
  breeding operation.
\item
  Cellular compartments mingle with each other by some random pro- cess.
\item
  In-cell or between-cell signals be transmitted in the manner of symbol
  transportation.
\item
  Symbols be selectively transferred to specific target positions by
  particular activator symbols (strongly selective), or at least
  selectively transferred by symbol interaction rules (weakly
  selective).
\item
  There be a possibility of symbols being changed or rearranged by some
  random process.
\end{itemize}

\autocite{Tominaga2007}--computationally universal with only uni- and
bi-molecular reactions

Desirable properties from \autocite{Faulconbridge2011}:

\begin{itemize}
\item
  Functional groups.
\item
  Conservation of energy.
\item
  \autocite[sec.4.4]{Faulconbridge2011} contains an interesting
  discussion mapping these desirable properties onto the emergent
  properties that are then required of an \gls{achem}.
\end{itemize}

Additional properties from \autocite{Hickinbotham2010}:

\begin{itemize}
\item
  Novelty and innovation, specifically the ability for new molecules
  introduced to the chemistry to take part in reactions without needing
  changes to the \gls{achem}.
\item
  Range of scales.
\item
  Dynamic environment.
\item
  Redundancy and degeneracy.
\item
  Emergent complex properties.
\item
  Unified molecular representation.
\item
  Stochasticity.
\item
  Emergent mutation rates.
\end{itemize}

\paragraph{Examples}\label{examples}

\begin{itemize}
\item
  Assembler Automata--Avida, Tierra
\item
  Geb \autocite{Channon:2001ly}
\item
  artificial organisms controlled by neural networks created by a
  developmental process from a bit-string genotype.
\item
  Individuals interact with the world through five predefined types of
  interaction generated by the neural network--reproduce (crossover and
  mutation of production rules), fight, turn anti-clockwise or
  clockwise, move forward.
\item
  RBN-World
\item
  Random Boolean Network, with the addition of a bonding mechanisms to
  allow for composition and decomposition of RBNs, can be used to build
  a chemistry capable of almost limitless extension out of
  non-traditional components
\item
  entities are described as a form of Random Boolean Network, with the
  addition of a bonding mechanisms to allow for composition and
  decomposition of RBNs.
\item
  A number of parameters affect the behaviour of the chemistry, and so a
  series of experiments sampled from the parameter-space, and then used
  a GA, to search for interesting variants as measured by non-catalysed
  loops (ideal measures of auto-catalytic sets and Hypercycles too rare
  for use as a measure) \autocite[section 8]{Faulconbridge2011}.
\item
  Ducharme et al \autocite{Ducharme2012}.
\end{itemize}

The approach taken is to model the energy changes associated with
reactions. The chemistry is spatial; atoms are arranged on a
2-dimensional grid and have velocity. When two atoms pass within a
particular distance, they interact.

The possible types of interactions are prespecified, with the type
chosen being driven by the atomic composition and energies of the
interacting atoms. Reactions are therefore between atoms rather than
molecules; a molecule in this chemistry is a combination of atoms
arranged in a particular structure, re-examined after each reaction to
form a stable configuration based on expectations from real-world
chemistry. Although computational costs are not reported, it seems
plausible that the calculation of intersections on a 2-dimensional grid
will be expensive for large molecular populations. Another cost comes
from the re-arrangement of molecules into energy-efficient
configurations. This spatial structuring enables the model to restrict
atomic interactions to those atoms that are accessible on a molecule,
but at the cost of additional modelling complexity.

\begin{itemize}
\item
  Stringmol
\item
  a bacterial inspired microprogram chemistry--though does demonstrate
  a rich heredity for open-ended evolution using string-matching to
  model binding between sequences
\item
  Squirm3
\item
  adopts fixed molecule types, and pre-defined reactions for replication
  and gene-sequence transcription, and so although capable of
  interesting behaviour is not capable of unlimited extension
\item
  Hutton
\item
  Fernando and Rowe
\item
  \autocite{Tominaga2009,Tominaga2007,Tominaga2004}
\item
  string-based pattern matching applied to examination of the
  feasibility of biochemical pathways
\item
  \autocite{Lenaerts2009}
\item
  modelling the topological evolution of chemical networks
\end{itemize}

\chapter{ToyWorld}\label{toyworld}

\section{Introduction}\label{introduction-3}

ToyWorld, our Artificial Chemistry for the exploration of emergent
behaviours, was first introduced in \autocite{Young2013}. The elements
of the model--Atoms, Molecules, Reactions, a Reaction Vessel--are
recognisable from real-world chemistry, but in highly simplified forms.

Although there is no need for ToyWorld to be faithful to standard
Chemistry, a degree of familiarity helps, but only in so far as the
analogy is consistent. Therefore we endeavour to maintain a basic
correspondence wherever possible. However, there is no requirement to
provide chemically-realistic results--our model cannot be used to
investigate real-world chemical behaviours.

There is also another reason. The model has many degrees-of-freedom, and
these must be constrained by parameter choice before the model can be
used in simulation. Some values are important to our thesis, and so are
considered true independent variables in our investigation. These are
examined fully. The remainder however are those to which the simulation
is insensitive, but still must be specified. For these we prefer
real-world values rather than arbitrary artificial values. In short,
where we consider something important, we investigate. Where we think it
less important, we use a consistent set of pre-existing values --
real-world chemistry.

The name has been chosen as both a homage or acknowledgement of ToyChem
\autocite{Benko2003}, and as a hint at the simulation's purpose:
creating an artificial world for exploration. That world, the ToyWorld
model, consists of:

\begin{itemize}
\item
  Analogues of physical elements such as atoms and molecules;
\item
  An overall energy model that describes the transformations that can
  occur between potential, kinetic and internal energy;
\item
  A physics model to describe how molecules interact within the reaction
  vessel; and
\item
  A chemical model that details the bond changes that can occur when two
  molecules collide.
\end{itemize}

All atoms, and therefore molecules and reactions, are contained within a
reaction vessel. ToyWorld provides a basic energy model, where molecules
have kinetic energy and bond breaking requires energy input and bond
formation releases energy. The reaction vessel, which provides the
strategies by which reaction reactants (or input molecules) and products
(output molecules) are determined, is described in detail in the
following section.

In our model, all reactions emerge solely from the properties of the
reacting molecules. For each reaction between two molecules we generate
a list of reaction alternatives by enumerating all possible bond
additions, bond subtractions, and changes in bond type between the
reactants. For example, the reactants H\textsubscript{2} and
O\textsubscript{2} generate three reaction alternatives: breaking of the
H-H bond, breaking of the O=O double bond, and a transformation of the
O=O double bond to a single bond. The reactants H\textsuperscript{+} and
OH\textsuperscript{-} give two alternative reactions: breaking of the
O-H bond (giving H+H\textsuperscript{+}+O\textsuperscript{-}) and
formation of a single bond between H\textsuperscript{+} and O to give
H\textsubscript{2}O.

\section{Atoms and molecules}\label{atoms-and-molecules}

ToyWorld is based on RDKit \autocite{rdkit}, open-source software for
cheminformatics. RDKit provides a number of useful capabilities
including format conversions to and from SMILES \autocite{smiles} and
graphical forms of molecules; standard sanity checks for molecular
structure, and molecular manipulations, but most importantly, is
well-tested and optimised for performance.

Molecules are modelled as an extension of standard RDKit \emph{Mol}
objects, constructed from RDKit \emph{Atoms} connected with
\emph{Bonds}. Standard Lewis dot structures built on the inherited
atomic properties are used to identify possible bonds, and a formal
charge model is used to record the charge changes associated with
modifications to the molecular structure caused by reactions.

The lowest level component in the ToyWorld model is the atom, and atoms
can be joined by bonds to form molecules. Reactions between molecules
are the only mechanism tom modify molecules provided by the model; a
reaction is simply the addition or subtraction of a single bond between
any two atoms in two molecules. ToyWorld provides a strongly
constructive chemistry \autocite{Fontana1994} where completely new forms
of molecules may be generated by reactions, and where the new molecules
may in turn take part in further reactions: the chemistry emerges from
the lower level atomic properties.

\section{Energy Model}\label{energy-model}

Energy within ToyWorld is found in only three standard forms -- kinetic,
internal and potential.

\begin{itemize}
\item
  Kinetic energy is the energy of motion, and equals $\frac{1}{2}mv^2$
  where $m$ is the mass of a molecule and $v$ its velocity.
\item
  Internal energy is an abstraction of vibrational energy or heat energy
  within a molecule. Internal energy, while incidentally consistent with
  real-world chemical models, is actually included in ToyWorld as a
  major mechanism to introduce stochasticity in reactions. Without
  internal energy, all excess energy following a reaction must be
  allocated to kinetic energy (following energy conservation), and so
  there is an iso-map between reaction and product kinetic energies.
  With internal energy, we can divert an arbitrary proportion into
  internal energy and therefore we have a stochastic multi-map.
\item
  Potential energy is the energy associated with the bonds between
  Atoms. Creating a bond reduces the potential energy of a molecule;
  breaking a bond increases it. The potential energy of a molecule is
  the sum of the potential energies of every bond in the molecule. The
  base state of no bonds is equal to $0$, and so molecules with bonds
  have negative potential energy. The specific value of each bond is
  absolutely determined by the atomic number of the two Atoms at either
  end of the bond, and the type of bond itself -- single, double or
  triple. ToyWorld provides a table of standard values in
  \cref{default_bond_energies}, based upon real-world chemistry.
  Unspecified bonds are given the average energy of specified bonds of
  the same bond type. Examples of molecules with associated potential
  energies are given in \cref{example_potential_energies}.
\end{itemize}

Our energy model enforces conservation of mass so reactions can be
represented solely by bond changes in RDKit. This follows the approach
taken in graph-based chemistries such as GGL/ToyChem
\autocite{Benko2003,Benko2005} where reactions are modelled as a series
of changes to graph edges, or bonds, only.

\begin{table}[t]
\begin{center}
\caption[Average Bond Dissociation Enthalpies]{Average Bond Dissociation Enthalpies in kcal per mole -- source: \url{http://www.cem.msu.edu/~reusch/OrgPage/bndenrgy.htm}}\label{default_bond_energies}
\begin{tabular}{@{}lp{2cm}p{2cm}p{2cm}@{}}
\toprule
Bond Type & Atom 1 & Atom 2 & Energy of both break and formation(simulation units)\\
\midrule
Single & H & H & 104.2\\
Single & C & C & 83\\
Single & N & N & 38.4\\
Single & O & O & 35\\
Single & H & C & 99\\
Single & H & N & 93\\
Single & H & O & 111\\
Single & C & N & 73\\
Single & C & O & 85.5\\
Single & N & O & 55\\
Double & C & O & 185\\
Double & C & C & 146\\
Double & N & N & 149\\
Double & O & O & 119\\
Double & C & N & 147\\
Double & N & O & 143\\
Triple & C & O & 258\\
Triple & C & C & 200\\
Triple & N & N & 226\\
Triple & C & N & 213\\
Quadruple & C & C & 200\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\begin{center}
\caption{Example potential energies calculated by the DefaultChemistry module using simplified bond energies}\label{example_potential_energies}
\begin{tabular}{@{}p{3cm}p{5cm}p{2.5cm}@{}}
\toprule
Molecule & SMILES & Potential Energy\\
\midrule
H$_2$O 			& [H]O[H] 								& -222.0 \\
H$_2$				& [H][H] 								& -104.2 \\
O$_2$ 				& O=O 									& -119.0 \\
N$_2$O$_4$ 	& [O-][N+](=O)[N+]([O-])=O 	& -434.4 \\
NO$_2$			& N(=O)[O] 							& -198.0 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}%

\subsection{Energy transformations}\label{energy-transformations}

The only energy transformations that occur in ToyWorld are those that
occur during reactions. The energy at the beginning of the reaction is
fully bound in the potential, internal and kinetic energies of the
reactants. At the moment of collision, that portion of the kinetic
energy due to the collision (kinetic energies of reactants less kinetic
energy of centre of mass) is transformed into internal energy 
in the combined reactants. If a bond forms in this phase
the freed potential energy is added into this pool of internal energy;
any bond that breaks transforms internal energy into increased potential
energy. Post-collision, a portion of the the pool of remaining internal
energy is transformed back into kinetic energy. The division may be all
to kinetic energy or all to internal or anywhere in between. All as
kinetic would represent a more elastic collision; if all remains as
internal energy, the collision is fully inelastic.

Reactions are modelled as head-on elastic collisions between two
reactants with changes to kinetic energy equalling the increase or
decrease in molecular potential energy associated with the creation,
destruction or change of order of bonds. Creation of a bond results in a
reduction of molecular potential energy and an increase to kinetic
energy; destruction results in the reverse. A change in bond type is
modelled as the sum of a bond creation and of a bond destruction. Total
energy in the system is always constant, and equal to the sum of the
initial kinetic energy of all molecules plus the sum of their potential
energies.

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}
[enode/.style={circle,draw=gray!50,inner sep=5pt,minimum size=30pt}]
\node[enode] (pe) at (6,4) {Potential Energy (PE)};
\node[enode] (ie) at (1,1) {Internal Energy (IE)};
\node[enode] (ke) at (11,1) {Kinetic Energy (KE)};
\end{tikzpicture}
\caption{Energy transformations in ToyWorld}
\label{fig:energy_transformations}
\end{center}
\end{figure}

Reactions conserve energy, and hence the total energy in the reaction
vessel should be constant. However, energy can be explicitly added to
and removed from the system from outside. Either case is modelled as a
uniform change in all molecular internal energies (heat and vibration).
This changes the size of the pool from which the kinetic energies of the
product molecules are determined after a collision or reaction. Adding
energy to the system increases each molecule's internal energy, which
increases the size of the pool of merged internal and kinetic energies
on collision or reaction, which increases the likely kinetic energies of
the product molecules. Removing energy from the system has of course the
opposite affect, down to the point where effectively all motion and
hence all reaction activity ceases.

\section{Reactant and product selection strategies}\label{reactant-and-product-selection-strategies}

A reaction may be seen as two stages in sequence: first, the choice of reactants from a population of possible reactant molecules (the Reactant selection strategy, denoted here by $S_\mathrm{Reactant}$), and second, the determination of products given that set of reactants (Product selection strategy, denoted $S_\mathrm{Product}$). The total energy (that is, potential + kinetic + internal) of the system is maintained, as is the total momentum of the molecules.

\section{Reactant selection strategies: selecting reactants for a reaction}\label{reactant-selection-strategies}

\autocite{Faulconbridge2011} describes two generic strategies for the
selection of reactants---spatial and aspatial---where the primary
difference is whether molecular position is a factor in reactant
selection. It is possible to further generalise this scheme by
considering other differentiating factors. Analogous with real-world
chemistry, a cumulative scheme presents itself starting with the pure
aspatial, or uniform probability strategy, and then proceeding through
the spatial strategy, based on molecular kinetics, to kinetics plus
intra-molecular and external forces such as electromagnetism. These
strategies can be viewed as being based on increasing derivatives of
position or location in the reaction vessel; from no position (uniform
selection), through fixed position (uninteresting as we cannot have a
sequence of reactions without motion) to the first derivative (velocity
or kinetic selection) and finally to the second derivative
(acceleration, or force selection.) Accordingly we adopt this more
detailed classification for the descriptions below.

\subsection{Uniform selection}\label{uniform-selection}

In a uniform selection strategy ($S_\mathrm{Reactant} = \mathrm{Uniform}$), reactants are chosen at random with equal (uniform) probability from the population: no property of a molecule has an effect on the selection. Conceptually we have a well-stirred reaction container with no intra-molecular forces.

\subsection{Kinetic selection.}\label{kinetic-selection.}

By contrast, in a kinetic selection strategy ($S_\mathrm{Reactant} = \mathrm{Kinetic}$) molecules have spatial position (and implicitly, velocity) within some assumed reaction vessel, and selection is determined by molecular position---molecules which are spatially co-located (that is, in collision) form a reactant set. Molecules move at constant velocity until they collide with something else (either another molecule or possibly a boundary of an explicit reaction vessel) and then either react, or bounce.  Currently in our work we assume that all molecules have a fixed and common size and shape (circular in two-dimensions), irrespective of molecular formula.

\subsection{Intra-molecular selection and external force selection}\label{intra-molecular-selection-and-external-force-selection}

More complicated forms, where molecular velocities are not constant, can
be generated by the introduction of some combination of intra-molecular
forces (such as electromagnetism) or external forces (such as gravity or
heat.)

\section{Product selection strategies: determining the products of a
reaction}\label{product-selection-strategies}

In the ToyWorld chemical model, all reactions arise solely from the
properties of the reacting molecules: it therefore defines a
\emph{strongly constructive} chemistry in the sense defined earlier. As
ToyWorld enforces conservation of mass, reactions can be represented
solely by bond changes. This is closely related conceptually to
graph-based chemistries such as GGL/ToyChem
\autocite{Benko2005,Benko2003}, RBN-World \autocite{Faulconbridge2011}
or NAC \autocite{Suzuki2006} where reactions are modelled as a series of
changes to graph edges. However, in ToyWorld, and RDKit, the graph is
implicit rather than explicit as it is in a graph-based chemistry.

For each interaction between two molecules we generate a list of reaction alternatives by enumerating all possible single bond additions, bond subtractions, and changes in bond type between the reactants. Each alternative is the result of a single one of these changes. For example, the reactants H\textsubscript{2} and O\textsubscript{2} generate three reaction alternatives: breaking of the H-H bond, breaking of the O=O double bond, and a transformation of the O=O double bond to a single bond. The reactants H\textsuperscript{+} and OH\textsuperscript{-} give two alternative reactions: breaking of the O-H bond (giving H+H\textsuperscript{+}+O\textsuperscript{-}) and formation of a single bond between H\textsuperscript{+} and O to give H\textsubscript{2}O. We restrict the options to those that can be generated by a single change to the bond structure of the reactants.

Each reaction alternative therefore can be completely described by the
pair of the products of the reaction (that result from the single bond
addition, subtraction or change) and the associated change in overall
potential energy, which of course will be the same as the potential
energy change of the single bond alteration.

Creation of a bond results in a reduction of molecular potential energy,
while bond destruction results in an increase. A change in bond type is
equivalent to a creation and then a destruction. The magnitude of the
change in potential energy, measured in arbitrary energy units, is taken
from a table of bond energies for each combination of atoms and bond
type (\cref{default_bond_energies}. The standard table is based upon
a simplification of real-world chemical bond energies. For example, the
creation of a H-H bond releases 104.2 units; the breaking of a C=O
double bond takes 185 energy units. \cref{example_potential_energies}
shows the potential energy for a sample of molecules.

How should we choose between alternative sets of possible products for
the same reactants? Various product strategies appear plausible: the
random choice of an alternative; the most complex alternative; least
complex; rarest; most common, and so on, but each strategy requires
effort to develop and evaluate.

We also consider a strategy with minimal bias: a Uniform selection
strategy ($S_\mathrm{Product} = \mathrm{Uniform}$), where every
alternative product set has equal probability of selection.

\subsection{Least Energy Strategy}\label{least-energy-strategy}

When following a Least Energy strategy
($S_\mathrm{Product} = \mathrm{LeastEnergy}$) we select a reaction by
choosing with uniform probability from a distribution of reaction
alternatives weighted by the total of the energy changes associated with
the bond changes. This biases selection towards the Least Energy
alternative; the strength of the bias is determined by the degree of the
weighting. Figure\cref{fig1} shows an example of the shift in products
that occurs as a result of this weighting as the overall quantity of
energy in the system is changed.

The reaction to fire is selected from the alternatives by generating a
cumulative probability distribution over the list of reaction options
where each option has a probability based on the relationship of the
energy required for the option versus the energy available.

We select a reaction to fire from the possible alternatives for the
reactants by probabilistic selection from the alternatives biased
towards options that release rather than consume energy. Fig. \cref{fig1}
shows an example of the shift in products that occurs as a result of
this weighting as the overall quantity of energy in the system is
changed.

Specifically, let $E_i$ denote the energy required for the bond change in the reaction option. If $E_i > 0$ the reaction is exothermic, or releases energy; otherwise it is endothermic, requiring energy to proceed.

We calculate a weighted value, $e_i$, based on the combination of $E_i$ and the available energy for the reaction, $E_{avail} > 0$, as follows:

\begin{displaymath}
e_i=
   \begin{cases}
     \lvert E_i\rvert,  &(1) \text{  if $E_i < 0$;}\\
     0,                 &(2) \text{  if $E_{avail} < E_i$;}\\
     E_{avail}--E_i,       &(3) \text{  otherwise.}
   \end{cases}
\end{displaymath}

Then, for reaction option $i$ of $n$ options, $p_i = e_i / \sum\limits_{i=1}^n e_i$, where $p_i$ is the probability of option $i$ being selected. A number is chosen from the uniform distribution $[0,1]$ and the selected reaction is found from the inverse of the CDF given by $p_i\text{ for all }i$ by searching for the reaction at that point in the CDF. This method has the property that the probability of a reaction being selected is proportional to its weight.

As a result, for exothermic reactions, highly exothermic reactions are preferred to slightly exothermic ones. For endothermic reactions, the available energy must exceed the energy required by the reaction, and the reaction is preferred according to the degree of the surplus. Note that an option where the energy required exceeds that available ($E_{avail} < E_i$) will have $p_i = 0$, and hence can not be selected. This behaviour is shown in \cref{fig:reaction_selection_weights}.

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}
[enode/.style={circle,draw=gray!50,fill=white,radius=0.5cm}]
\pgftext[at=\pgfpoint{2cm}{0cm},left,base]{\pgfimage[width=0.5\linewidth]{figures/reaction_selection_weights}};
\node[enode] at (5.5,6) {1};
\node[enode] at (7,4) {2};
\node[enode] at (8,3.2) {3};
\end{tikzpicture}
\caption{Energy transformations in ToyWorld}\label{fig:reaction_selection_weights}
\end{center}
\end{figure}

\subsection{Energy transformations during a reaction}\label{energy-transformations-during-a-reaction}

Molecules have kinetic energy; when they collide the form of the
interaction follows from the energy transformations between kinetic and
internal and potential energy that are preferred under the chemical
model; and finally, the trajectory taken by the resulting products of
the interaction is given by their final post-collision kinetic energy.

First, we determine the kinetic energy of the centre of mass of the
reactants. The available energy to drive the reaction is the total
kinetic energy of the reactants plus the internal energy of the
reactants less the kinetic energy of the centre of mass.

Consider the case where two particles of equal mass but opposite
velocity collide. The KE of the centre of mass will be zero (as it is
motionless) and the energy liberated by the collision will be the sum of
the kinetic and internal energies of the particles.

At the other extreme, consider two equal mass particles, travelling with
the same velocities. Intuitively, it is obvious that the only energy
released by their infinitesimally gentle collision is from their
internal energies, and this is confirmed by the calculation where the
kinetic energy of the centre of mass will equal the combined kinetic
energies of the reactants, leaving only the combined internal energies.

The available energy for bond modifications is calculated as the sum of
the kinetic energies of the reactants less the energy of their centre of
mass, plus any energies internal to the reactants. The final kinetic
energies of the products equals the sum of the initial kinetic energies
of reactants less the change in molecular potential energy from bond
changes and the change in internal energies. Total energy in the system
is always constant, and equal to the sum of the initial kinetic energy
of all molecules plus the sum of their potential energies and internal
energies.

The Reactor Algorithm selects one of the reaction alternatives by
choosing from a distribution of reaction alternatives weighted by
associated energy changes (as discussed in
\cref{product-selection-strategies}.)

\subsection{Setting product velocities and internal energies}\label{setting-product-velocities-and-internal-energies}

The final step in the reaction mechanism is to determine the velocities
and internal energies of the reaction products following the reaction.
Although the method is standard physics, there are two complications:
the number of products may or may not be the same as the number of
reactants, and the pre-reaction energy and post-reaction energy vary as
the reaction itself either consumes or liberates energy.

The only constraints are that velocities of the product molecules must
conserve momentum and total energy within the reacting system. We
recognize that within the frame of reference of the centre of momentum
of the reacting system, the vector sum of the momentum of the products
must equal zero. Therefore one possible solution to the product
velocities is to arrange their vector momentums according to simple
geometry: for two products, we arrange their momentums in a line (line
\cref{alg:2products} in \cref{alg:post_collision_adjustments}); for three
products, an equilateral triangle (line \cref{alg:3products}), and by
extension, for four products, a square and so on.

\begin{algorithm}
\KwData{Reactant molecules fully specified with energies and velocities, Product molecules without energies or velocities}
\KwResult{Velocities for each Product molecule and total Internal Energy for all Product molecules $\ni$ total energy and momentum conserved}
%$\text{sum of reactant kinetic energies}\leftarrow \sum\limits_{i=1}^n \text{kinetic energy}_i$\;
%$\text{sum of reactant internal energies}\leftarrow \sum\limits_{i=1}^n \text{internal energy}_i$\;
$\text{kinetic energy of CoM}\leftarrow \frac{1}{2}(\text{sum of reactant masses})(\text{velocity of CoM})^2$\;
\BlankLine
$\text{collision energy}\leftarrow \text{sum of reactant kinetic energies}+\text{sum of reactant internal energies}-\text{kinetic energy of CoM}$\;
\BlankLine
\For(\tcp*[f]{Transform into CoM frame}){$i\leftarrow 1$ \KwTo $\text{number of Products}$}{\label{alg:lab_to_CoM_frame}
    $v^\prime_i\leftarrow v_i-\text{CoM velocity}$\;
}
\BlankLine
\uIf(\tcp*[f]{Conservation of momentum implies all excess energy must go into internal energy}){Number of products = 1}{\label{alg:allocate_energies}
    $\text{Internal energy of Products}\leftarrow \text{collision energy}$\;
}
\uElse{
    $\text{ke}\leftarrow \text{random}([0,1])$\;
    $\text{Internal energy of Products}\leftarrow \text{collision energy}--\text{ke}$\;
}
\BlankLine
\Switch(\tcp*[f]{Find a set of momentum vectors that sum to zero...}){Number of products}{
\uCase(\tcp*[f]{One product}){1}{
    $mv^\prime\leftarrow (0,0,0)$\;
}
\uCase(\tcp*[f]{Two products}){2}{\label{alg:2products}
    $mv\leftarrow 2\text{ke}\prod\limits_{i=1}^n \text{mass}_i/\sum\limits_{i=1}^n \text{mass}_i$\;
    $mv^\prime_1\leftarrow (v^\prime_{i\theta}+\frac{\pi}{2},v^\prime_{i\rho}+\frac{\pi}{2},mv)$\;
    $mv^\prime_2\leftarrow (v^\prime_{i\theta}+\frac{3\pi}{2},v^\prime_{i\rho}+\frac{3\pi}{2},mv)$\;
}
\uCase(\tcp*[f]{Three products}){3}{\label{alg:3products}
    $mv\leftarrow 2\text{ke}\prod\limits_{i=1}^n \text{mass}_i/\sum\limits_{i=1}^n \text{mass}_i$\;
    $mv^\prime_1\leftarrow (v^\prime_{i\theta}+\frac{\pi}{3},0,mv)$\;
    $mv^\prime_2\leftarrow (v^\prime_{i\theta}-\frac{\pi}{3},0,mv)$\;
    $mv^\prime_3\leftarrow (v^\prime_{i\theta}-\pi,0,mv)$\;
    }
}
\BlankLine
\For(\tcp*[f]{Convert momentums to velocities...}){$i\leftarrow 1$ \KwTo $\text{number of products}$}{
    $v^\prime_i\leftarrow mv^\prime_i / \text{mass}_i$\;
}
\BlankLine
\For(\tcp*[f]{Transform back to standard frame}){$i\leftarrow 1$ \KwTo $\text{number of products}$}{\label{alg:CoM_to_lab_frame}
    $v_i\leftarrow v^\prime_i+\text{CoM velocity}$\;
}
\caption{Algorithm to set post-collision velocities and internal energies}\label{alg:post_collision_adjustments}
\end{algorithm}


Following a standard method, we first transfer the molecules from the
frame of reference of the reaction vessel into the centre of mass (CoM)
reference frame by subtracting the velocity of the CoM from each
particle (line \cref{alg:lab_to_CoM_frame}). Correspondingly, we also
adjust the energy of the collision by subtracting the KE of the CoM. We
recognize that in the CoM frame the vector sum of the momentums will be
zero; working in this frame reduces the number of vectors we must sum by
one (the momentum of the CoM itself.)

We then arbitrarily choose the proportion of total available energy to allocate to the product kinetic energies and assign the remainder to internal energy (line \cref{alg:allocate_energies}.) From the kinetic energy allocation we can determine the total scalar momentum of the products using $KE = 0.5 * velocity * momentum$, and arrange the vector momentums according to the geometry described earlier.

Finally, we convert from the CoM reference frame to the initial frame by
adding back the velocity of the CoM (line \cref{alg:CoM_to_lab_frame}).

This method satisfies our requirements of conservation of momentum and
energy for arbitrary numbers of reactants and products while being
computationally straightforward. The limitation is that product vectors
are arranged in regular and consistent, although reasonably realistic,
configurations. An improvement would be to perturb the geometry of the
vectors in the CoM frame to remove the regularity.

The outcomes for each reaction alternative from an example collision are
shown in \cref{fig:collision_diagrams}.

\tdplotsetmaincoords{60}{110}

\begin{figure}
\centering
\subcaptionbox{O=C=O + C$\rightarrow$ [O] + C + [C]=O}[0.5\textwidth]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.545078574376}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.545078574376}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.741455902351}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-1.00269224405}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{0.791487199406}{-24.6589855625}{-70.1992043245};
\tdplotsetcoord{O1}{0.790684881135}{-24.6841653158}{-219.830383842};
\tdplotsetcoord{O2}{0.342756745318}{-57.7497471497}{-145.082834065};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[O]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[H]C([H])([H])[H]};
\draw[color=blue,-stealth] (O) -- (O2) node[font=\tiny,color=black]{[C]=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\draw[dotted, color=black] (O) -- (O2xy); \draw[dotted, color=black] (O2) -- (O2xy);
\end{tikzpicture}}%
\subcaptionbox{O=C=O + C$\rightarrow$ C + [O][C]=O}[0.5\textwidth]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.545078574376}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.545078574376}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.741455902351}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-1.00269224405}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{0.964870972861}{170.402519717}{-86.502709589};
\tdplotsetcoord{O1}{0.690770768471}{-121.920061352}{-167.114195829};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[H]C([H])([H])[H]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[O][C]=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\end{tikzpicture}}%
\end{figure}\begin{figure}
\centering
\subcaptionbox{O=C=O + C$\rightarrow$ [O] + C + [C]=O}[0.5\textwidth][l]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.545078574376}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.545078574376}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.741455902351}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-1.00269224405}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{0.95444795957}{-20.4286793523}{-48.3296764651};
\tdplotsetcoord{O1}{0.953114678315}{-20.4573799628}{-241.693389093};
\tdplotsetcoord{O2}{0.239763118095}{-84.1834478971}{-145.082834065};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[O]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[H]C([H])([H])[H]};
\draw[color=blue,-stealth] (O) -- (O2) node[font=\tiny,color=black]{[C]=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\draw[dotted, color=black] (O) -- (O2xy); \draw[dotted, color=black] (O2) -- (O2xy);
\end{tikzpicture}}%
\subcaptionbox{O=C=O + C$\rightarrow$ C + [O][C]=O}[0.5\textwidth][r]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.545078574376}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.545078574376}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.741455902351}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-1.00269224405}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{0.683938755498}{116.950434333}{-109.96313961};
\tdplotsetcoord{O1}{0.613112203007}{-91.4698581679}{-158.027937122};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[H]C([H])([H])[H]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[O][C]=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\end{tikzpicture}}%
\end{figure}\begin{figure}
\centering
\subcaptionbox{O=C=O + C$\rightarrow$ [H] + [CH3] + O=C=O}[0.5\textwidth]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.13448625105}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.13448625105}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.182938074093}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-0.247392444315}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{1.0085637103}{-4.76020744144}{22.9882733079};
\tdplotsetcoord{O1}{0.174049248064}{-27.6898552083}{-202.938127977};
\tdplotsetcoord{O2}{0.113557100128}{-42.6716880361}{-145.082834065};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[H]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[H][C]([H])[H]};
\draw[color=blue,-stealth] (O) -- (O2) node[font=\tiny,color=black]{O=C=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\draw[dotted, color=black] (O) -- (O2xy); \draw[dotted, color=black] (O2) -- (O2xy);
\end{tikzpicture}}%
\subcaptionbox{O=C=O + C$\rightarrow$ [H] + [CH3] + O=C=O}[0.5\textwidth]{%
\begin{tikzpicture}[scale=3,tdplot_main_coords]
\coordinate (O) at (0,0,0);
\draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
\draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
\draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
\node at (1,1,0) {};
\tdplotsetcoord{CM0}{0.212817921117}{-35.9326309999}{-145.082834065};\tdplotsetcoord{CM1}{-0.212817921117}{-35.9326309999}{-145.082834065};
\draw[color=black,densely dotted,-stealth] (CM0)-- (CM1);\tdplotsetcoord{I0}{-0.289490563666}{53.4742770534}{-140.637846457};
\tdplotsetcoord{I1}{-0.391486455219}{-269.549896905}{-273.397556374};
\draw[color=red,-stealth] (I0) node[font=\tiny,color=black]{O=C=O}-- (O);
\draw[color=red,-stealth] (I1) node[font=\tiny,color=black]{[H]C([H])([H])[H]}-- (O);
\draw[dotted, color=black] (O) -- (I0xy); \draw[dotted, color=black] (I0) -- (I0xy);
\draw[dotted, color=black] (O) -- (I1xy); \draw[dotted, color=black] (I1) -- (I1xy);
\tdplotsetcoord{O0}{1.00268330145}{-7.57832855688}{10.7762512874};
\tdplotsetcoord{O1}{0.247189031344}{-30.8825003788}{-183.173163231};
\tdplotsetcoord{O2}{0.193079615497}{-39.6636579632}{-145.082834065};
\draw[color=blue,-stealth] (O) -- (O0) node[font=\tiny,color=black]{[H]};
\draw[color=blue,-stealth] (O) -- (O1) node[font=\tiny,color=black]{[H][C]([H])[H]};
\draw[color=blue,-stealth] (O) -- (O2) node[font=\tiny,color=black]{O=C=O};
\draw[dotted, color=black] (O) -- (O0xy); \draw[dotted, color=black] (O0) -- (O0xy);
\draw[dotted, color=black] (O) -- (O1xy); \draw[dotted, color=black] (O1) -- (O1xy);
\draw[dotted, color=black] (O) -- (O2xy); \draw[dotted, color=black] (O2) -- (O2xy);
\end{tikzpicture}}%
\end{figure}\label{fig:collision_diagrams}

\chapter{Model Validation}\label{model-validation}

\section{Introduction}\label{introduction-4}

The design outlined in Section
\cref{toyworld} leads to the following expectations, to be tested
experimentally.

\begin{enumerate}
\item
  Given two reactants, changing the reaction energy should result in
  different sets of reaction products.
\item
  Molecular quantities reach equilibrium---that is, the set of
  interacting molecules is constant, with fluctuations expected in
  quantities. We expect molecular concentrations to stabilize at
  non-extreme values (equilibrium rather than driven to an extreme)
  after some transition period from the initial conditions.
\item
  The equilibrium point depends on the energy of the system. Our energy
  model preferentially forms bonds at low energies, and breaks bonds at
  high. We expect the average length of molecules in the artificial
  chemistry to be greater at low energies than at high energies.
\end{enumerate}

These predictions were tested by two experiments: first, we examined the
reaction products produced at a range of reaction energies for four sets
of reactants. Second, for a given set of reactants, we ran the
simulation for 10,000 iterations at four successive initial average
kinetic energy levels---0, 67, 133, and 200 units per molecule---with
each molecule initially at quantity 100. The experiment was run first
with a reactant set containing N\textsubscript{2}O\textsubscript{4} and
2NO\textsubscript{2} (results in Fig.\cref{fig1}), and then with a
reactant set of H\textsubscript{2}, O\textsubscript{2} and H\textsubscript{2}O.

\begin{figure}
\begin{center}
    \caption{Molecular quantities over time for initial population of N\textsubscript{2}O\textsubscript{4} and 2NO\textsubscript{2} with initial average KE ranging from 0 to 200 units (only molecules with significant quantities are labelled; remainder appear as light-grey lines). Molecules represented in SMILES notation.}
    \includegraphics[width=\linewidth]{figures/results-test-N2O4a}
    \label{fig1}
\end{center}
\end{figure}

\section{Results and Discussion}\label{results-and-discussion}

Beyond the initial transition period, both reactant sets showed results
essentially consistent with equilibrium. Population variability was high
in both cases, but more so for the N\textsubscript{2}O\textsubscript{4}
and 2NO\textsubscript{2} reactant set. In that case, some molecules
never reached a relatively constant population level (gradient of a
best-fit population line remained significantly non-zero.) The
fluctuations in the quantities of the other molecules are expected
according to our criteria, and result from the inherent variability in
reaction selection which causes the quantities to oscillate around a
norm.

With both reactant sets the model produced a significant number of
molecules which would be considered unstable in real-world chemistry
(such as O\textsuperscript{-} and O.) This is likely an artifact of the method
we use to generate reaction options, where a bond-break plus
bond-formation reaction---moving through an intermediate unstable
ion---occurs in our model as two separate reactions. As all molecules
currently react with equal likelihood, significant time can elapse
before the intermediate product reacts to form a stable product.

Both reactant sets showed clear differences in population composition
between the four initial kinetic energy levels. In the
H\textsubscript{2}, O\textsubscript{2} and H\textsubscript{2}O reactant set, no reactions
occurred at the zero energy level. This is expected from our energy
model as only bond-formations are possible without free kinetic energy.
With reactants of H\textsubscript{2}, O\textsubscript{2} and H\textsubscript{2}O no bond formations are possible,
confirmed by examining the bond options returned by the model for the
six possible combinations of initial reactants. By contrast, the
reactant set N\textsubscript{2}O\textsubscript{4} and
2NO\textsubscript{2} at energy zero contains one possible bond formation
reaction (in SMILES, [O]N=O.[O]N=O to O=N[O][O]N=O)
which can proceed without free kinetic energy. This then releases a
product which can also react, and so on, thus explaining the different
results between the reaction sets.

\section{Conclusions}\label{conclusions}

Our base artificial chemistry appears to be at least compatible with the
requirements for the future exploration of open-ended evolution. The
model is simpler than comparable alternatives, and the energy and
reaction models produce results consistent with our predictions for the
system's behaviour (with the exception of achieving equilibrium with the
N\textsubscript{2}O\textsubscript{4} and 2NO\textsubscript{2} reactant
set). An aspatial approach does however come with restrictions. Most
obviously, as there is no concept of proximity in the chemistry, there
can be no boundaries or membranes or even basic distinctions between
\emph{inside} and \emph{outside}. This is critical in biology but it is
unclear if this is equally important in non-biological systems. We
expect that experimental comparison between the aspatial and spatial
approaches in the course of our exploratory experiments will help to
clarify this.

\chapter{Reactant and Product Strategies}\label{reactant-and-product-strategies}

\section{Introduction}\label{introduction-5}

In this section we explore the following research questions:

\vspace{0.3cm}
\begin{minipage}[l]{0.95\textwidth}
\begin{enumerate}[label=RQ\arabic*:]
\item Is there a quantitative difference between different reactant and product selection strategies?
\item Is there a combination of reactant and product selection strategies that leads to increased emergence as measured by cycles?
\item Is emergence significantly affected by the values of other parameters of an Artificial Chemistry, such as initial kinetic energy or bond energies?
\end{enumerate}
\end{minipage}
\vspace{0.3cm}

To the best of our knowledge, this is the first time that reaction and product selection strategies
in Artificial Chemistries have been experimentally compared.
Instead, the general approach of previous work, where there has been a quantitative evaluation,
has been to propose a particular strategy, build, and evaluate against the initial goals,
rather than against alternatives.

Our two primary factors, or independent variables, are $S_\mathrm{Reactant}$ and $S_\mathrm{Product}$.
We also introduce two secondary factors, overall reaction vessel energy ($E_\mathrm{Vessel}$) and
bond energy ($E_\mathrm{Bonds}$), to assess the sensitivity of the simulation to other parameters.
For simplicity of analysis, all of our factors are two-level, meaning they take one of two
possible levels, or values, in each run. The parameter values chosen for each level of
$E_\mathrm{Vessel}$ and $E_\mathrm{Bonds}$ were chosen as representative from a set of alternatives
used in initial exploratory experiments; in each case they allowed the simulation to run for an
extended period without running out of possible reactions (from lack of energy for example.)

\begin{table}
\scriptsize
\caption{Factors, or independent variables}\label{tbl:factors}
\begin{tabular}{p{1.4cm}p{2.2cm}p{4.4cm}p{5.5cm}}
\hline\noalign{\smallskip}
Factor & +1 value & -1 value & Description\\
\hline
\noalign{\smallskip}
$S_\mathrm{Reactant}$&		Kinetic&						Uniform&	See Section \cref{reactant-selection-strategies}\\
$S_\mathrm{Product}$&		LeastEnergy&					Uniform&	See Section \cref{product-selection-strategies}\\
$E_\mathrm{Vessel}$&		300&							100&		Initial kinetic energy of each molecule in the reaction vessel\\
$E_\mathrm{Bonds}$&		Single=50, Double=100, Triple=200& Simplified real-world chemistry. Average values for Single=77.7, Double=148.2, and Triple=224.3&	Energy required to break a bond of the given type\\
\hline
\end{tabular}
\end{table}

We concentrate on three related response, or dependent, variables---Number of cycles, Length of longest cycle, and Count of most common cycle. All three are derived from a reconstruction of the network of reactions that occur during each experiment run, where every edge represents a specific reaction connecting a particular set of reactants with a particular set of products. Note that the nodes in the constructed network capture specific molecules, rather than molecular types or species that share the same chemical formula (as would be more usual in the construction of a Reaction Network for real-world chemistry.)

We exclude all unique cycles, and all cycles with three or fewer elements (for example, where a molecule loses, then regains, an atom repeatedly). Unique cycles by nature are unlikely to be representative; very short cycles on the other hand are so common as to dominate other more interesting cycles in any analysis.

\section{Experiment Design}
The experiments follow a full factorial design over four factors ($S_\mathrm{Reactant}$, $S_\mathrm{Product}$, $E_\mathrm{Vessel}$ and $E_\mathrm{Bonds}$), each at two levels, run in a randomized order, with three (3) replicates of each combination of factors executed in sequence before beginning the next combination. The first replicate of each combination starts with a predefined random seed incremented by one for each successive replicate of the same combination. The factor levels used are given in Table \cref{tbl:factors}.

Each replicate used the same initial population of 800 molecules, made up of 100 molecules each of [H][H], O=O, [O-][N+](=O)[N+]([O-])=O, and N(=O)[O] and 200 molecules each of O and O=C=O (all represented in SMILES \parencite{smiles}.) This initial population is somewhat arbitrary, although reasonable; given that ToyWorld is a strongly constructive chemistry, we would expect that any differences between initial populations would reduce as the simulation proceeds.

For details of the Artificial Chemistry, see \parencite{Young2013}. The chemistry makes use of some low-level components from RDKit \parencite{rdkit}, open-source software for cheminformatics. RDKit provides a number of useful capabilities, including format conversions to and from SMILES and graphical forms of molecules; standard sanity checks for molecular structure, and molecular manipulations. In ToyWorld, atoms are closely based on real-world chemistry atoms, and in fact are implemented as wrappers around the Atom definitions provided by RDKit; we allow any atom type provided by RDKit. Bonds in ToyWorld are represented by RDKit bonds, but the addition or subtraction mechanism makes use of the parameterised ToyWorld energy model.

\section{Results}
All replicates completed a set of 20,000 reactions; given the initial population size of 800 molecules, and from the summary of results below, we believe that this captures a representative set of reactions. This also simplifies the analysis as we can assume a balanced set of treatments in the statistical sense (that is, the sample sizes for all treatments are equal).

\begin{table}[htbp]
\scriptsize
\caption{Summary of results}\label{tbl:results}
\begin{center}
\begin{tabular}{lrrr}
\hline\noalign{\smallskip}
Statistic&Number of cycles&Length of longest.cycle&Count of most common cycle\\
\hline\noalign{\smallskip}
\multicolumn{4}{c}{Reactions 4750 to 5000}\\
\hline\noalign{\smallskip}
Min.			&0.00	&0.00           	&0.00\\
1st Quartile	&0.00	&0.00        	&0.00\\
Median		&1.50     	&3.50        	&2.50\\
Mean		&219.06   &5.04        	&215.80\\
3rd Quartile	&91.25     	&7.75       		&96.00\\
Max.			&5704.00 &20.00         	&2728.00\\
\hline\noalign{\smallskip}
\multicolumn{4}{c}{Reactions 9750 to 10000}\\
\hline\noalign{\smallskip}
Min.			&0.00	&0.00		&0.00\\
1st Quartile	&0.00	&0.00		&0.00\\
Median 		&6.00	&4.00		&6.00\\
Mean		&62.10	&4.65		&169.21\\
3rd Quartile	&68.75	&8.25		&27.75\\
Max.			&526.00	&13.00		&6684.00\\
\hline\noalign{\smallskip}
\multicolumn{4}{c}{Reactions 14750 to 15000}\\
\hline\noalign{\smallskip}
Min.			&0.00	&0.00	&0.00\\
1st Quartile	&1.00	&3.00	&2.00\\
Median		&5.00	&4.50	&5.00\\
Mean		&27.17	&4.79	&42.27\\
3rd Quartile	&34.50	&7.00	&16.25\\
Max.			&237.00	&12.00	&862.00\\
\hline\noalign{\smallskip}
\multicolumn{4}{c}{Reactions 19750 to 20000}\\
\hline\noalign{\smallskip}
Min.			&0.00	&0.00	&0.00\\
1st Quartile	&0.00	&0.00	&0.00\\
Median		&3.50	&4.00	&4.00\\
Mean		&20.04	&3.90	&14.62\\
3rd Quartile	&20.25	&6.00 	&13.25\\
Max. 		&199.00    &12.00	&216.00\\
\hline
\end{tabular}
\end{center}
\end{table}%

A view of the results is given in Table \cref{tbl:results}: reaction networks built from the full dataset of 20,000 reactions can be too large for easy analysis. Instead, we choose to partition the reaction data into four equally spaced blocks of 250 reactions each and analyse each block independently.

%% begin.rcode loaddfc, echo=FALSE
%	load_dfc <- function(f){
%		colClasses <- c("NULL", "NULL", "factor","NULL","factor","factor", "factor","factor","integer","integer","integer")
%		df <- read.csv(f,colClasses=colClasses)
%		df$Partition.Start <- factor(df$Partition.Start, levels = c("4750", "9750", "14750", "19750"))
%		df
%	}
%% end.rcode

%% begin.rcode cyclesbypartition, echo=FALSE, fig.show='hold', fig.cap='Cycles by reaction partition (starting reaction number for each partition along x-axis)'
%	df<-load_dfc("results/EvaluatorActualCycles.out") 
%	par(mfrow=c(1,3),cex=0.5)
%	plot(df$Number.of.cycles~df$Partition.Start, main="Cycle numbers by Partition", ylab="Number of cycles")
%	plot(df$Length.of.longest.cycle~df$Partition.Start, main="Cycle length by Partition", ylab="Length of longest cycle")
%	plot(df$Count.of.most.common.cycle~df$Partition.Start, main="Cycle count by Partition", ylab="Count of most common cycle")
%% end.rcode
\label{fig:partitions}

%\begin{figure}[h]
%\centering
%	\includegraphics[width=0.95\linewidth]{figures/partitions}
%	\caption{Cycles by reaction partition (starting reaction number for each partition along x-axis)}
%	\label{fig:partitions}
%\end{figure}

% No R code...
\begin{figure}[h]
\centering
	\includegraphics[width=0.45\linewidth]{figures/PlotMolecularDiversity-strategies-12-0}
	\includegraphics[width=0.45\linewidth]{figures/PlotMolecularDiversity-strategies-16-1}
	\caption{Diversity for Replicates 12-0 and 16-1}\label{fig:diversity}
\end{figure}

\subsection{Analysis and Discussion}
Figure \cref{fig:cyclesbypartition} suggests that the first partition, representing the vessel a quarter of the way into its lifespan, is quantitatively different from the other three partitions, with a significantly greater range for all three response variables. Intuitively this corresponds with an initial period where the diversity in the reaction vessel rapidly increases from the limited starting set of molecules, as seen in some (e.g, Figure \cref{fig:diversity}) but not necessarily all of the replicates. Diversity here is measured by (average molecular quantity)$^{-1}$. All following sections therefore exclude data from the first partition of reaction numbers from 4750 to 5000.

%% begin.rcode reactantstrategy, echo=FALSE, fig.show='hold', fig.cap='Response by Reactant Strategy '
%	df <- subset(df,df$Partition.Start != "4750")
%	par(mfrow=c(1,3),cex=0.5)
%	boxplot(Number.of.cycles~Product.Strategy, data=df, main="Cycle numbers by Product Strategy", names=c("Uniform","Energy"), ylab="Number of cycles")
%	boxplot(Length.of.longest.cycle~Product.Strategy, data=df, main="Cycle length by Product Strategy", names=c("Uniform","Energy"), ylab="Length of longest cycle")
%	boxplot(Count.of.most.common.cycle~Product.Strategy, data=df, main="Cycle count by Product Strategy",names=c("Uniform","Energy"), ylab="Count of most common cycle")
%% end.rcode

%\begin{figure}[t]
%\centering
%\includegraphics[width=0.95\linewidth]{figures/reactant_strategy}
%\caption{Response by $S_\mathrm{Reactant}$}
%\label{fig:reactant_strategy}
%\end{figure}

%% begin.rcode productstrategy, echo=FALSE, fig.show='hold', fig.cap='Response by Product Strategy '
%	par(mfrow=c(1,3),cex=0.5)
%	boxplot(Number.of.cycles~Reactant.Strategy, data=df, main="Cycle numbers by Reactant Strategy", names=c("Uniform","Kinetic"), ylab="Number of cycles")
%	boxplot(Length.of.longest.cycle~Reactant.Strategy, data=df, main="Cycle length by Reactant Strategy", names=c("Uniform","Kinetic"), ylab="Length of longest cycle")
%	boxplot(Count.of.most.common.cycle~Reactant.Strategy, data=df, main="Cycle count by Reactant Strategy",names=c("Uniform","Kinetic"), ylab="Count of most common cycle")
%% end.rcode

%\begin{figure}[t]
%\centering
%\includegraphics[width=0.95\linewidth]{figures/product_strategy}
%\caption{Response by $S_\mathrm{Product}$}
%\label{fig:product_strategy}
%\end{figure}

\subsection{RQ1: Is there a quantitative difference between the different reactant and product selection strategies?}
From visual inspection of Figure \cref{fig:reactantstrategy}, there appears to be a significant difference between the Uniform and Kinetic reactant selection strategies for number and length of cycles. Kinetic reactant selection seems to result in significantly higher levels of emergent behaviour than Uniform reactant selection. Similarly, from Figure \cref{fig:productstrategy}, there is very little apparent difference between the two product strategies, Uniform selection and Least Energy selection.

We use ANOVA (Analysis of Variance) to further examine the relationship of $S_\mathrm{Reactant}$ and $S_\mathrm{Product}$ to the response variables using a two-factor with two-levels (2x2) model (degrees of freedom=1) with interaction effects. There is a highly significant difference (p\textless 0.001) between the Uniform and Kinetic reactant selection strategies when comparing the number of cycles (f-value=40.442) and length of cycles (f-value=361.891) (confirming the impression given by Figure \cref{fig:reactantstrategy}), although again without difference for the count of the most common cycle. The effect of $S_\mathrm{Product}$ on cycle number and length is also significant (f-value=4.050 and 5.705 respectively, p\textless 0.05) and there is a first-order interaction between $S_\mathrm{Reactant}$ and $S_\mathrm{Product}$ for number of cycles (f-value=4.011, p\textless 0.05).

%% begin.rcode reactantproductcombination, echo=FALSE, out.width='0.30\\linewidth', fig.height=4, fig.show='hold', fig.cap='Effect of the combination of Reactant and Product Strategies on Response Variables '
%	boxplot(Number.of.cycles ~ Reactant.Strategy+Product.Strategy, data=df,  log="y", ylim=c(1,6000),names=c("Uniform:Uniform","Kinetic:Uniform","Uniform:Energy","Kinetic:Energy"))
%	boxplot(Length.of.longest.cycle ~ Reactant.Strategy+Product.Strategy, data=df,  log="y", ylim=c(1,6000),names=c("Uniform:Uniform","Kinetic:Uniform","Uniform:Energy","Kinetic:Energy"))
%	boxplot(Count.of.most.common.cycle ~ Reactant.Strategy+Product.Strategy, data=df, log="y", ylim=c(1,6000), names=c("Uniform:Uniform","Kinetic:Uniform","Uniform:Energy","Kinetic:Energy"))
%% end.rcode

%\begin{figure}[t]
%\centering
%\subcaptionbox{Cycle Count by Strategy Combination %($S_\mathrm{Reactant}$:$S_\mathrm{Product}$)\label{fig:cycle_count}}[0.45\linewidth][r]{\includegraphics[width=0.45\linewidth]{figures/cycle_count}}
%\subcaptionbox{Cycle Length by Strategy Combination %($S_\mathrm{Reactant}$:$S_\mathrm{Product}$)\label{fig:cycle_length}}[0.45\linewidth][l]{\includegraphics[width=0.45\linewidth]{figures/cycle_length}}
%\subcaptionbox{Count of Most Common Cycle by Strategy Combination %($S_\mathrm{Reactant}$:$S_\mathrm{Product}$)\label{fig:cycle_common}}[0.45\linewidth][r]{\includegraphics[width=0.45\linewidth]{figures/cycle_common}}
%\caption{Effect of the combination of $S_\mathrm{Reactant}$ and $S_\mathrm{Product}$ on Response Variables}
%\end{figure}

\subsection{RQ2: Is there a combination of reactant and product selection strategies that leads to increased emergence as measured by cycles?}

From Figure \cref{fig:reactantproductcombination} it is clear that there is no significant relationship between strategy and the number of occurrence of the most common cycle. However, it seems that such a relationship does exist for the number and length of cycles, with the strongest effect as a result of $S_\mathrm{Reactant}$, and a lesser effect from the choice of $S_\mathrm{Product}$.

We conclude that the greatest levels of emergence are likely to be seen with the combination of $S_\mathrm{Reactant} = \mathrm{Kinetic}$ and $S_\mathrm{Product} = \mathrm{LeastEnergy}$.

\subsection{RQ3: Is emergence significantly affected by the values of other parameters of an Artificial Chemistry, such as initial kinetic energy or bond energies?}

We constructed a two-factor with two-levels (2x2) ANOVA model (degrees of freedom=1) with interaction effects to examine the relationship of the independent variables $E_\mathrm{Vessel}$ and $E_\mathrm{Bonds}$ to the response variables, and applied it to our dataset (summarised in Table \cref{tbl:results}). $E_\mathrm{Bonds}$ is significant (f-value=4.221, p\textless 0.05) to number of cycles. No other significant relationships exist.

\section{Conclusions}
The choice of $S_\mathrm{Reactant}$ is critical to the behaviour of an emergent Artificial Chemistry; $S_\mathrm{Product}$ on the other hand appears to have a lesser effect on the emergence of cycles in our experiments. Furthermore, $S_\mathrm{Reactant} = \mathrm{Kinetic}$ is more effective for cycle emergence than $S_\mathrm{Reactant} = \mathrm{Uniform}$.

The most significant limitation of our analysis overall is that the values chosen for the high and low values of $E_\mathrm{Bonds}$ make it impossible to determine the cause of the difference observed in RQ3. There are two alternative explanations: first, the energy required to make or break bonds is simply different between the two factor levels; second, in the low factor level, based on real-world values, the bond make and break energies for even a single bond vary depending on the atoms involved, while in the high factor level these values are consistent for all bonds of the same degree. To distinguish between the two explanations we would need at least the average levels at each degree to be the same for each factor; this is a suggestion for a future experiment.

\chapter{Discussion}

\part{The Search for Creativity}\label{the-search-for-creativity}

\chapter{Introduction}

The parameters themselves become endogenized, in the case of \emph{Fitness} into an implicit relative fitness and for \emph{Fidelity} into the outcome of the copying/variation mechanism.

``Fitness determines context''--in contrast to Part 2, the entity affects its environment

Fitness is relative to a context or niche. Creativity is where the
entity changes niche or context--either by moving to a new niche, or
creating or modifying one

Creativity requires a feedback loop between entity and environment--a
common representation for both so that they can interact (and entities
can interact with other entities, and elements within the enviroment
with other elements\ldots{})

Other open areas--workings of selection

  Hints--e.g., Taylor, Van Valen\ldots{}--regarding resource
  competition, when individuals are embedded (required for single set of
  omnipotent rules)

Hypothesis--Resource competition as the mechanism of selection

\begin{itemize}
\item
  Does this simplify/remove choices? (as we earlier saw with
  reproduction/replication?)
\item
  Addition/Removal follows automatically
\end{itemize}

\section{Requirements for chemistry}\label{requirements-for-chemistry}

\section{Previous work}\label{previous-work-1}

\section{Match to requirements}\label{match-to-requirements}

\section{Chemistry selection}\label{chemistry-selection}

\quote{
Standish (2003), which is that openendedness depends fundamentally on
the continual production of novelty.}
{\autocite{Soros2014}}

\section{\texorpdfstring{What is interesting? Require formal defn for ``proof''\ldots{}}{What is interesting? Require formal defn for proof\ldots{}}}\label{what-is-interesting-require-formal-defn-for-proof}

Levels are interesting in biology (should defn encompass biology or
focus on CS?)

Interesting seems to be different search space, not just bigger space.
What about better search of same space? Is that interesting?

\quote{\ldots{}the processes associated with the major transitions are an
automatic consequence of mutation and selection, due to the generation
of higher levels of selection due to spatial self-organization.}
{\autocite{Hogeweg1998}}

\begin{itemize}
\item
  although somewhat quantified by scope (spatial structures e.g.,
  waves/spirals) and lack of fourth of element of Maynard-Smith:1995lw -
  ``transition from limited inheritance to universal inheritance''
\end{itemize}

Evolutionary mechanism from earlier is made up of variation and
selection elements

Evolutionary mechanism must be itself capable of improvement

\begin{itemize}
\item
  Levels of selection requires changes in mechanism
\item
  And levels are correlated with surprise--major transitions in
  evolution (Maynard-Smith:1995lw) (although specifics here are
  biologically derived)
\item
  e.g., Mechanism of evolution must be itself evolvable; individuals and
  environment interact to give new ways of producing new individuals.
  Dynamics required for novelty-generation (Nellis2014)
\item
  Evolvable mechanism; implies embodied, or endogenous--mechanism must
  be capable of evolvable under same conditions as shown earlier for
  evolutionary potential
\item
  Inheritable variation under selection

  \begin{itemize}
  \item
    If intrinsic selection, then mechanism for
    inheritance/variation/selection should be expressed in same language
    as other evolvable elements--a common rule set or chemistry. If
    different, then cannot have interactions between other elements and
    evolutionary mechanism--limited EvoEvo
  \end{itemize}
\end{itemize}

Show that given 1) OEE evolutionary system from above 2) capable of
self-modification--process evolution, capable of ``interesting''

Sufficient condition

\textit{Hypothesis 4}: Endogenous selection and variation under evolutionary control sufficient for ``novelty-generation'' in evolution

From earlier Hypothesis 3, V+S-\textgreater{}E

Endogenous selection by resource competition

World = Elements + Interactions

More complex interactions emerge (constructive)

\begin{itemize}
\item
  Some compositions actively evolve (strong selection), all others
  potentially affected (weak selection and no selection)
\item
  How do compositions form and how are they maintained? Some rule in
  world must impose a form of bias or asymmetry to allow differences to
  develop. In biology, locality of effect is one example
\item
  Compositions rely on external-to-them elements and lacking those,
  cannot maintain themselves. Growth and maintenance results from
  success in resource competition
\end{itemize}

Variation under evolutionary control

\begin{itemize}
\item
  A' = f(A), if f() fixed then must be capable of generating all future
  variation. Alternative is a f() which is not-fixed, but is itself
  capable of variation. What type of variation?
\item
  Needs to be novel and open-ended itself!
\item
  EvoEvo simplest--same mechanism
\item
  Shared mechanism for I and V--capable of introducing some V during I
\item
  V must be exposed to S
\item
  Feedback loop--V refined by S
\item
  Prediction: V changes over time. In more detailed models, form of V
  may change over time (not possible without an EvoEvo mechanism)
\item
  Prediction: correlation between parent and offspring properties is highest
  in unchanging (stable) environments, and lowest in varying ones. In
  systems without variation under evolutionary control, correlation is
  unchanging. (in previous section, correlation modelled as under
  evolutionary control with degree of correlation as endogenous
  parameter)
\end{itemize}

\subsection{Tests for evolutionary activity}\label{tests-for-evolutionary-activity-1}

Sustainability may be undecidable--Wiedermann:2005ys

Alife community--Channon, etc

Population measures e.g., Vasas2015

Artificial selection

Moran's test

Search mechanism--thermodynamic or ergodic search vs evolutionary
search (implied by Rasmussen2004)

Local entropy (implied by Adami2015)

Measure of OEE -
http://link.springer.com/article/10.1007\%2Fs11084-012-9309-y

\chapter{Discussion and Conclusions}\label{discussion-and-conclusions}

We have shown that:
\begin{itemize}
\item Selection and Variation are sufficient for Evolution in an Artificial System
\item Such a system is capable of adjusting to changing environments without preselection of parameters
\item Such a system can be implemented in an Artificial Chemistry
\end{itemize}

And therefore that open-ended evolution is possible in an Artificial Chemistry.

\TODO{ link OEE to selection and variation from work in \cref{the-search-for-creativity}}

Requires a series of steps--akin to OOL where single-step
astronomically unlikely (single RNA strand probability about 10E-60,
based on 100 monomers--\autocite{Pascal2013})

\section{Step 1--demonstration of Autocatalytic activity}\label{step-1---demonstration-of-autocatalytic-activity}

Previous work

\begin{itemize}
\item
\autocite{Gordon-Smith2013}--Quite complex systems for replication demonstrated
in SimSoup-2, handcrafted molecules
\item
\autocite{Bagley1990}
\item
GARD--\autocite{Segre1998}
\item
\autocite{Ono2002}--Boid-like rules in Lattice Artificial Chemistry leads to
membrane formation. Fixed types of particles with associated
hydrophobic/hydrophilic/neutral class (with orientation). Fixed
reaction paths to form an autocatalytic set of reactions
\item
\autocite{Huning2000}
\end{itemize}

\section{Step 2--demonstration of selection between ACS}\label{step-2---demonstration-of-selection-between-acs}

Test of evolution under selection versus drift- artificial selection -
show response to selective pressure

\section{Step 3--Evolution in an artificial chemistry}\label{step-3---evolution-in-an-artificial-chemistry}

\section{Alternative explanations}\label{alternative-explanations}

\begin{itemize}
\item
Thermodynamics
\item
Experimental/Model bias (e.g., discovered bias of GARD inheritance
model)
\end{itemize}

\section{Tests for evolutionary activity}\label{tests-for-evolutionary-activity}

\begin{itemize}
\item
Sustainability may be undecidable--\autocite{Wiedermann:2005ys}
\item
Alife community--\autocite{Channon:2006st}, etc
\item
Population measures e.g., \autocite{Vasas2015}
\item
Artificial selection
\item
Moran's test
\item
Search mechanism--thermodynamic or ergodic search vs evolutionary
search (implied by \autocite{Rasmussen2004})
\item
Local entropy (implied by \autocite{Adami2015})
\item
Measure of OEE--\autocite{Markovitch2012}
\end{itemize}

\section{Tests for sufficient conditions}\label{tests-for-sufficient-conditions}

\begin{itemize}
\item
Remove a condition (e.g., \autocite{Soros2014})
\end{itemize}
