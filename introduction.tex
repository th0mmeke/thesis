\chapter{Introduction}\label{introduction}

\settowidth{\epigraphwidth}{Wonderful life : the Burgess Shale and the nature of history}
\epigraph{%
Without hesitation or ambiguity, and fully mindful of such palaeontological wonders as large dinosaurs and African ape-men, I state that the invertebrates of the Burgess Shale, found high in the Canadian Rockies in Yoho National Park, on the eastern border of British Columbia, are the world's most important animal fossils. Modern multicellular animals make their first uncontested appearance in the fossil record some 570 million years ago--and with a bang, not a protracted crescendo.}%
{\textit{\\Wonderful life : the Burgess Shale and the nature of history}\\\textsc{Stephen Jay Gould}}

Biological evolution has produced ecosystems of staggering variety and range, occupying essentially every viable niche on, above, and within the Earth from a common origin in the prebiotic world many millions of years ago. This has been far more than a working out of a single theme; instead a hugely impressive radiation of form and function has left that original ancestor far behind. The only connection that remains is the unbroken lineage of genes that links that earliest ancestor to every organism alive today.

This is not a work about that evolution. Nor does this thesis directly address the artificial equivalent of that explosion through evolution by natural selection, the subject of creative \gls{oe}--a topic that has had steady attention, without notable success\footnote{Recent reviews in \cite{BanzhafBaumgaertnerBeslonEtAl2016}}. Instead, our focus is on one of the necessary elements for \gls{oe}, the mechanism for inheritance and variation within artificial evolutionary systems.

\Gls{oe}, as a form of evolutionary system, requires three elements--variation, inheritance and selection\footnote{Other related formalisms exist--see \cref{evolution-by-natural-selection} for a discussion. None fundamentally replace these elements however.}. Selection is the mechanism by which possibilities are pruned, and focus is maintained. Selection guides evolution by directing the search into promising areas, acting upon the phenome, or the form developed from a genome under the influence of the environment. However, it is not a generator of new ideas. 

Novelty instead is the outcome  of the second element, variation. Variation extends the search into different areas, and creates new alternatives for selection to evaluate. The final element, inheritance, elevates evolution beyond random search--instead of starting afresh at each generation, inheritance causes the search to be cumulative, with gains preserved. Variation and inheritance together are the source of new forms; each variation is a modification of the previous generation, forming a spreading tree of related entities when viewed over time. Unlike selection, variation and inheritance are processes that act upon the genome directly.

Creative \gls{oe} is motivated by the gap observed between the products of biological evolution, and those produced by artificial evolutionary systems. The term itself, originating with \textcite{Taylor2001}, captures the essential elements: first, the products of artificial evolution are missing ``impressiveness'' or the capacity to surprise; the most common reaction is that they lack creativity. A creative system is one that produces artefacts that are novel, surprising and valuable \parencite{Boden2004}; properties that are desirable not only in the ``creative'' fields of art, literature, music and the like, but also in more prosaic areas such as engineering and indeed artificial evolutionary systems. Second, artificial evolution lacks ``open-endedness'', or the capacity to continue to produce new variants.

Thus we can draw a distinction between creative \gls{oe} and simple \gls{oe} (as suggested by \textcite{Taylor2001}) as the difference between endless uninteresting variation and something more distinctive, indeed surprising. \footnote{This has recently been nicely formalised by \textcite{BanzhafBaumgaertnerBeslonEtAl2016} in terms of models and meta-models, with a direct conceptual links to Boden.}

We can now relate inheritance and variation to the requirements for creative \gls{oe}. As the model underlying a creative \gls{oe} system must be capable of change, so must the inheritance and variation mechanism that implements that model. It cannot be static and predefined, but must evolve itself. In fact, the mechanism must be a target of evolution just as the broader organism is; instead of two systems--the organism as a whole and the evolutionary system--there must be one. In other words, the inheritance and variation mechanism must be endogenous, or inherent to the model, rather than separate and imposed from outside.

In this work we explore two related aspects of an endogenous mechanism for inheritance and variation. The first research question (RQ1) is: how can such a mechanism evolve? In \cref{part2} we describe a simple evolutionary model to show how inheritance arises from variation and selection, and how it can evolve under a variety of selection environments (or environmental forms.) Second, how can this mechanism be embedded in a system to provide the underpinnings for creative \gls{oe} (our second research question, RQ2)? In \cref{part2} we begin to answer this through a \gls{achem} that we suggest is capable of supporting an endogenous inheritance and variation mechanism, and therefore of creative \gls{oe}. This is however an ambitious goal, more suited to a research program than a thesis, and we leave for future work the implementation of the mechanism, instead establishing a foundation in a chemistry compatible with the goal.

\begin{DRAFT}

	
Constitutive definitions of \gls{oe}
Emergent systems easy to code but produce surprising results. Cannot predict results from rules (or in fact easily predict rules from desired results. One-way function, akin to encryption) \parencite{Nellis2014}

Much of this work falls under the banner of \gls{alife}, both weak and strong as distinguished by \textcite{Langton1989}: ``study the phenomena of life, not by simulating life as it is (weak AL) but by instantiating life as it could be (strong AL)''

First, a distinction: open-ended evolution refers to a goal or result, while evolution-of-evolution, or EvoEvo, describes a process or mechanism. The first is constitutive, the second causal. In artificial systems, the most influential formulation of the problem overall remains that of \textcite{Bedau:2000mi}:

\quote{A key challenge is whether digital systems based on symbolic logic harbor the same potential for evolutionary innovation as physical systems. A preliminary challenge is to unlock the full potential of evolution in digital media. Many believe that digital life today falls far short in this regard, and this issue is starting to be approached quantitatively.}{\textcite{Bedau:2000mi}}

\parencite{Taylor2001} like others makes a distinction between \gls{oe} and ``kinds of evolutionary innovation''. He gives as an example of limited innovation the emergence of parasitism in \textcite{Ray1991} as direct result of system design and initial seeding conditions. In this reading, ``fundamentally new'' (labelled by Taylor as ``creative'') means new ways of sensing the environment and interacting with it \parencite{Taylor2001}.

\quote{by open-ended evolutionary capacity we understand the potential of a system to reproduce its basic functional-constitutive dynamics, bringing about an un-limited variety of equivalent systems, of ways of expressing that dynamics, which are not subject to any predetermined upper bound of organizational complexity (even if they are, indeed, to the energetic-material restrictions imposed by a finite environment and by the universal physico-chemical laws)}{\cite{Ruiz-Mirazo2004}}

\begin{compactitem}
	\item An open-ended evolutionary system must demonstrate unbounded diversity during its growth phase.
	\item An open-ended evolutionary system must embody selection.
	\item An open-ended evolutionary system must exhibit continuing (``positive'') new adaptive activity.
	\item An open-ended evolutionary system must have an endogenous implementation of niches.
\end{compactitem} \textcite{Maley1999} (considered ``rather abstract'' by \textcite[p.341]{Hutton2002}).

Darwinian machine is fundamentally self-referential - products of evolution affect process of evolution - lots of examples \parencite{Watson2015}

``openendedness depends fundamentally on the continual production of novelty.'' Standish, in \parencite{Soros2014}

``we would like the evolutionary system, like life, to continue to produce individuals of increasing complexity and diversity.''--although note, following McShea, that much of life is single-celled and hasn't become much more complex in billions of years \parencite{Maley1999}

\quote{by open-ended evolutionary capacity we understand the potential of a system to reproduce its basic functional-constitutive dynamics, bringing about an un-limited variety of equivalent systems, of ways of expressing that dynamics, which are not subject to any predetermined upper bound of organizational complexity (even if they are, indeed, to the energetic-material restrictions imposed by a finite environment and by the universal physico-chemical laws.}{\cite{Ruiz-Mirazo2004}}
\end{DRAFT}

\section{Creativity in artificial systems}\label{creativity}

\begin{DRAFT}
Perpetual production of novelty
unbounded evolution
The potential size and complexity of the individuals' phenotypes should be (in principle) unbounded.  \parencite{Soros2014}
Unlimited heredity--number of possible heritable types should astronomically exceed individuals in population \parencite{Vasas2015}.

continual production of complexity
essence of life

\autocite{VonNeumann1966} as reviewed in \autocite{Taylor1999} (Lack of environmental emphasis)
Von Neumann's architecture for how ``complicated machines could evolve from simple machines''

\begin{enumerate}[label=\Alph*]
	\item A \emph{constructive} machine that takes a description and builds a new machine;
	\item A \emph{copying} machine to make a copy of a description, and 
	\item A \emph{control} machine to sequence the other two machines - first the copying machine, then constructive, then finally to link the new machine to its description.
\end{enumerate}

Von Neumann makes a fundamental distinction between a description of a machine and the machine itself; in biological terms, the architecture separates between genotype (description) and phenotype (the machine.) However, because some of the function of the three machines (A, B, C) are provided by the world there is a limit to how self-referential the system can be, and hence how much its function can evolve over time.

Taylor states ``I would suggest that the reproducing programs in Tierra and similar systems can also sensibly be analysed in terms of von Neumann's architecture.''

Just a few years later, \autocite{Waddington2008} as reviewed in \autocite{Taylor:1999sc}--Originally published in ``Towards a Theoretical Biology, Vol. 2'' in 1969

Also Genotype (G) and Phenotype (Q*) based, where Q* associated with an environment (E from Ej)

For \gls{oe} need: Ej infinite numbered set, and sufficient Qs for Q*s for all those Ejs

Q*s are part of Ej satisfies condition one {[}recursive?{]}

Second is an emergent one - ``it is not sufficient to create new mutations which merely insert new parameters into existing programmes; they must actually be able to rewrite the programme''
- key distinction between \gls{oe} and creative evolution

\parencite{Maley1999}
Focus on diversity (to make progress), as measured by new adaptive activity, $A_new$. Some suggestions previously that diversity is bounded (at minimum, by number of molecules available for biosphere, also by energy and minimal populations and probably other things), and plateaus (punctuated equilibrium). Indicates two time constants - a fast expansion to use available resources, with a slower rhythm of innovations to create and enter new adaptive spaces.

Maley builds up the requirements by beginning with almost the simplest model possible (called Urmodel 1) where the only changes in a population from generation to generation are 1-bit flips in a 32-bit genotype. This results in a neutral fitness landscape, and the (unsurprising) result was unbounded diversity but no heritable effect on fitness.

Selection was added to Urmodel 2, based on the hamming distance ``dissimilarity'' between the species competing for an available niche (or vacant location on a 256x256 grid)) where there is selective advantage for the most dissimilar, justified by biological example of niche overlap theory. However, $A_new$ trended towards $0$.

Urmodel 3 moved to a model where fitness is determined by the degree of match (number of bits with same value) between a parasite and a single fixed ``host'' bit pattern. Maley claims that Urmodel 3 shows unbounded activity using measure $A_new$--``the first known artificial evolutionary system demonstrating unbounded evolutionary activity''. But in Maley's words this is probably not a unique or even significant result--``The only trick is to defer the point when the model hits its true asymptotic behaviour for long enough that the growth dynamics of the model are themselves asymptotic in some sense'' \parencite{Maley1999}.

Note that the niches in Urmodel 3 are imposed from outside, rather than arising endogenously. This leads to Urmodel 4, where co-evolution is added to Urmodel 3 by letting hosts mutate. Urmodel 4 also showed ``unbounded`` evolutionary activity.

No model however resulted in ``surprise''. Maley believes this is because of a lack of complexity, illustrated through a nice biological metaphor: ``A puddle of inert, multicoloured and diverse algae would not be nearly so inspirational as the rain forest.''



\Textcite{Taylor2001,Taylor:1999sc} discuss creativity in \gls{oe} in depth and argues that, for it to be possible, the replicators must \parencite{Hutton2004}:
\begin{compactenum}
	\item Be fully embedded in their arena of competition 
	\item Have rich, unlimited interactions between each other and with their environment 
	\item Initially replicate implicitly, rather than using some encoding of the replication process, and 
	\item Be constructed entirely of `material' components, allowing the possibility of different encodings of information. (\quote{the very stuff from which they are constructed is a valuable resource of matter and energy}{\textcite[s3.6]{Taylor2001}})
\end{compactenum}

``the potential for a large degree of intrinsic adaptation'' \parencite{Taylor2001}

\parencite{Soros2014} presents four necessary conditions for \gls{oe} (it is left open if these are also sufficient conditions):
\begin{compactenum}
	\item A rule should be enforced that individuals must meet some minimal criterion (MC) before they can reproduce, and that criterion must be non-trivial
	\item The evolution of new individuals should create novel opportunities for satisfying the MC
	\item Decisions about how and where individuals interact with the world should be made by the individuals themselves.
	\item The potential size and complexity of the individuals' phenotypes should be (in principle) unbounded.
\end{compactenum}
Along with these specific conditions, \parencite{Soros2014} assumes certain general conditions: a ``good'' genetic representation, a ``sufficiently large world for every individual to be evaluated'', and a seed or starting point.

Minimal conditions for \gls{oe} \parencite{Vasas2015}:
\begin{compactenum}
	\item Very rich combinatorial generative mechanism \eg organic chemistry.
	\item Unlimited heredity--number of possible heritable types should astronomically exceed individuals in population (Maynard-Smith:1995lw).
	\item Inexhaustible fitness landscape--implies rich, dynamical environment.
	\item Cannot state in advance possible pre-adaptations
\end{compactenum}

General difficulty with grounding these definitions, as the terms on which they rely are themselves only loosely defined (although commonly understood). \parencite{BanzhafBaumgaertnerBeslonEtAl2016}. As \textcite{Gayon2010} notes, many fundamental things generally taken for granted, such as ``life'' and ``energy'', are hard to define. 

``We therefore arrive at two contrasting definitions. On the one hand, there is the idea of continual (unbounded) creation of \emph{novelty}, which appears to be necessary but not sufficient. On the other hand, there is the notion of continual (unbounded) creation of \emph{complexity}, which appears to be sufficient, but not necessary for OE.'' \parencite{BanzhafBaumgaertnerBeslonEtAl2016}
\end{DRAFT}

Creativity is related to exploration--how we explore and what we find--and is a general problem in \gls{alife}, Artificial Evolution, and in fact life in general. That no standard definition exists is a good indication that this is a hard problem. Intuitively, creativity is something that causes a sense of surprise or wonder in an observer. Creative systems and products are beautiful; interesting; impressive; novel; surprising; transformative.  For example, in \textcite{Lehman2012}, creativity is related to beauty and interestingness--while impressiveness is time-independent, interestingness often drops over time.

Following \textcite{Boden2004}, a starting point might be that creativity is a \emph{process} that results in \emph{artefacts} with the properties of novelty, surprise, and value. It's clear that to \textcite{Boden2004} not all novelties are creative; creativity means something more than change-for-change's sake. Creativity is the ability to come up with ideas or artefacts that are \begin{inparaenum}[1)]\item new--a jump not just extension,\item surprising--statistically unlikely, or unexpectedly belonging to a class, or thought impossible--a sense of wonder and \item valuable--a sense of a deeper meaning; fitness or appropriateness or usefulness \end{inparaenum}\parencite{Boden2004}. Perkins interprets \textcite{Boden2004} as saying that creativity is a search plan that breaks or changes the rules (or representation or landscape) so as to make previously inaccessible (or hard to access or isolated) regions of the search space (the ones that contain new, surprising and valuable items) accessible (or rich) \parencite{Perkins1995}. For example, moving from individual organisms to assemblages or aggregations (changing the level of selection--a rule change) would certainly be seen as novel, surprising and valuable.

For \textcite{Dorin2009} creativity is primarily connected to rareness--if something was rare but is now less rare in another framework, then the framework is more creative. Value (and incidentally appropriateness) is explicitly denied \parencite[p.18]{Dorin2009}, in part because of the difficulty of defining value in a general way. Novelty is important, but comes as a consequence of the introduction of a new framework which generates novel patterns. In \textcite[p.16]{Dorin2009} though, novelty is introduced as the time lag between successive more creative frameworks--humans become habituated to ideas, so novelty means surprise. Surprise is therefore equated with one (``unlikely'') of Boden's three ideas of surprising (``unlikely or unfamiliar'', ``unexpectedly belongs to a class'', and ``thought impossible.'')

\emph{Novelty} can be seen as either historical novelty (new to all) or psychological novelty (new to self), where the latter is more interesting as a superset of the former (and removes historical chance.)\footnote{I would argue that in biology, the identification of novelty has been essentially subjective based upon a reading of the historical record to identify patterns and changes in those patterns. Any measures, such as proposed by \textcite{McShea1998,Maynard-Smith:1995lw,Walker2012} are post-hoc.}

Almost by definition, novelty is difficult to identify. A straightforward definition would suggest we're interested in changes--identifying significant changes seems reasonable, but changes in what? Pre-identifying characteristics to monitor limits our scope, and our imagination, to those characters. How do we create a list of existing characters as a baseline? How could this be done objectively? And what about completely new characters? How could we ever describe those ahead of time? In other words, if we can describe it ahead of time, it isn't novel. A second related issue is one of \textit{reification}--if these characters emerge from lower-order operations, rather than being specified, then we need a mechanism to identify, capture, and describe each new one.

Systems that are only random are not creative (\eg Racter in \textcite[p.16]{Dorin2009})--it seems that novelty is not enough, and random variation within bounds that do not change is quickly seen as normal and not novel. 

Next, changes are obviously relative, so we must identify what we are comparing--within a population there is inherent variation. Do we compare to an average, or to a range? And presumably the population is undergoing evolution, and therefore there will be some underlying trends. To be significant, any change should be more than just an acceleration (or diminution) of a pre-existing trend. Finally, at what point can we declare a novelty? On its first appearance? Perhaps though the putative novelty is maladaptive, and doesn't last beyond one generation. Perhaps it doesn't even last that long: the individual isn't viable. Hereditary lineages (\eg Fitness Transmission \parencite{Miconi:2007xh}) might be one approach to this. For all of these reasons, any objective measure of novelty based on characters seems in practice impossible. This problem applies to both physical characteristics, such as cellular membranes, or legs, and processes, such as cross-membrane transport or genome replication.

The outcome of a workshop held in the summer of 2014 on ``Open-ended Novelty'' \parencite{BanzhafBaumgaertnerBeslonEtAl2016} , attended by many of those whose works are referenced elsewhere in this thesis, provides a satisfying way to formalise novelty by making the idea of \textcite{Boden2004}, that novelties ``break the rules'', explicit in the form of models and meta-models. By proposing a standard meta-model to describe any \gls{oe} system, be it living or non-living, \textcite{BanzhafBaumgaertnerBeslonEtAl2016} can then define three levels of novelty by their effect on the model of the system itself and the overall meta-model:

\begin{itemize}
	\item Type-0 or \emph{variation}. Novelty occurs within the scope of the existing model, by changing the values of existing variables.
	\item Type-1 or \emph{innovation}. From changes to the model itself, either new types or relationships.
	\item Type-2 or \emph{emergence}. By changes to the meta-model, either in the form of new meta-types or meta-relationships between meta-types.
\end{itemize}

The significance of this formulation is that novelty for almost the first time becomes non-subjective and measurable (assuming agreement on the overall meta-model, and that a model for the specific system can be defined.)

\emph{Surprise} is almost as difficult to define as creativity, as it is also relative in general to an observer, but previous work has successfully equated surprise with rarity (\eg \textcite{Kowaliw2009}). Related to surprise, is impressiveness or ``rare and hard-to-recreate'' \parencite{Lehman2012}: an asymmetrical transformation in that is easy to recognise yet hard to create, and so similar to NP-completeness, which can be verified but not solved in polynomial time \parencite{Lehman2012}.

\emph{Valuable} seems at first problematic: valuable to who? Introducing an observer into the definition freights in subjectivity and the associated practical difficulty of capturing the observer's assessments. A pragmatic answer might instead be that it is valuable to itself--valuable ideas are ones that are successful, or in other words, that are of high fitness. 
\footnote{\Textcite{Bringsjord2000} argues that Boden is wrong in two ways: first, that computer creativity does not shed much insight into human creativity (citing Searle's Chinese Room to demonstrate that shuffling symbols, as a creative AI would do, doesn't add much to our understanding), and second, that Boden's formulation of a rule change is too weak. Systems (citing BRUTUS) that are not considered creative, do rule changes of the form proposed by Boden. Instead, these changes must be more special than simple first-order logic. The first criticism is not relevant to us (echoed by \textcite{Dorin2009}), but the second holds.}

%\subsection{Elements/Embodiment/Emergence}
%\begin{DRAFT}
%Open-ended evolution can be seen as the outcome of evolution in an open-ended system (\eg Chemistry), where an open-ended system has effectively unrestricted representation: the number of possible types must be much larger than the number of individuals. In the terminology of \textcite{Szathmary:2006ty}, these are unlimited replicators, in contrast to limited replicators where the number of possible types is less than the number of individuals. Without this property all possible types can be generated in a finite time, and the system will either reach stasis or begin to repeat. 
%
%Not all open-ended systems necessarily support evolution, but in those that do, our intuition suggests that open-ended evolution produces increasing complexity, increasing diversity, accumulation of novelty and continual adaptation \parencite{Lehman2012}.
%
%A ``good'' genetic representation \parencite{Soros2014}
%
%\Textcite{VonNeumann1966}
%
%
%\textcite{Pattee1995a}: complementary matter (\eg genotype, hardware, brain, physical laws) and symbols (\eg phenotype, software, mind, measurements) aspects (referred to as \textit{Semantic Closure}) define life. Matter is objective and compressible; symbols are subjective and incompressible, specific to the individual's purpose. 
%
%Semantic Closure is also a necessary condition for \gls{oe}. The argument is as follows: evolution requires self-replication. Self-replication requires both matter and symbols. In essence, \quote{"the organism's measurement,memory, and control constraints must be \textit{constructed} by the genes of the organism from parts of the artificial physical world.}{\textcite[p.29]{Pattee1995},emphasis in original} 
%
%\textcite[p.29]{Pattee1995}: three fundamental types of knowledge must be represented in models of \gls{alife}: laws, initial conditions, and statistics. Given that initial conditions are measurements, the implication is that \quote{any form of artificial life must be able to detect events and discover laws of its artificial world.}{\textcite[p.29]{Pattee1995}}
%
%\quote{Additionally, from an epistemological point of view, Pattee (1995b) points out that symbolic information (such as that contained in an organism's genes) has ``no intrinsic meaning outside the context of an entire symbol system as well as the material organization  that constructs (writes) and interprets (reads) the symbol for a specific function, such a classification, control, construction, communication\ldots''. He argues that a necessary condition for an organism to be capable of creative open-ended evolution is that it encapsulates this entire self-referent organisation (Pattee refers to this condition as semantic closure). From this it follows that organisms should be constructed ``with the parts and the laws of an artificial physical world'' Pattee (1995a) (p.36). In other words, the interpretation (phenotype) of the symbolic information (genotype) of an artificial organism should be constructed and act within the artificial physical environment of the system. Additionally, if the system is to model the origin of genetic information, then the genotype itself must also be embedded within the environment; that is, the complete semantically-closed organisation `the entire organism' must be completely embedded within the physical environment. }{\textcite[p.82]{Taylor2001}}
%
%``From the point of view of the evolvability of individuals, the more embedded they are, and the less restricted the interactions are, then the more potential there is for the very structure of the individual to be modified. Recall that this is one aspect of my definition of creative evolution. Sections of the individual which are not embedded in the arena of competition are `hard-wired' and likely to remain unchanged unless specific mechanisms are included to allow them to change (and the very fact that specific mechanisms are required suggests that they would still only be able to change in certain restricted ways).'' \textcite{Taylor2001} 
%
%
%Resources must be ``(a) a vital commodity to individuals in the population; (b) of limited availability; and (c) that individuals can compete for (at either a global or local level). This resource can usually be interpreted as en\eg space, matter, or a combination of these.'' \parencite{Taylor2001}
%
%Everything in our artificial world must be built from a common set of raw materials; a loop connects the targets of selection with the environment. Other work, although superficially similar in that it models objects of similar scale, uses different models for the objects and the world (see \parencite{Sanchez-Dehesa:2008uq}) and so lack this loop.
%
%Smith-Szathmary hypothesis that major transitions in biology are characterised by new ways of transmitting information. Adami follows this direction in stating ``It is probably more appropriate to say that evolution increases the amount of information a population harbours about its niche (and therefore, its physical complexity).'' \textcite{Adami2002}
%
%Bottom-up models for open-ended evolution leverage the richness of underlying environment--less information in entity definition, more in environment definition. Similar to biology, where physics and chemistry underpin living organisms, where definition of minimal cell many orders of magnitude simpler than the working out of the chemical and physical rules that it relies upon. Top-down models assume a knowledge of the necessary elements.
%
%Need something emergent. \parencite{Nellis2014}
%
%``A phenomenon is embodied within a world if there exist some mechanisms within the world that enable the phenomenon to emerge.'' \textcite{Nellis2014}
%Quick defined embodiment in terms of two dynamical systems mutually affecting each other--no need for a physical world. System modifies environment. Doesn't address if system is constructed from environment. Autopoiesis does say though that system built from environment. But autopoiesis talks about maintenance not evolution. \parencite{Nellis2014}
%
%Contains an interesting discussion mapping a list of desirable properties onto the emergent properties that are then required of an \gls{achem} \parencite{Faulconbridge2010, Faulconbridge2011}
%
%
%
%\end{DRAFT}
%
%\subsection{Selection}
%\begin{DRAFT}
%Individuals and environment mutually affect each other and ``the potential for a large degree of intrinsic adaptation'' \parencite{Taylor2001}
%Some support the Red Queen hypothesis \parencite{VanValen1973}:
%\begin{itemize}
%\item Competition between individuals for resources is the primary source of intrinsic selection pressure. \parencite{Taylor2001}
%\item ``the most important aspect of an organism's environment are the other organisms with which it interacts'' \parencite{Maley1999}
%\item Ray makes similar arguments \parencite[p.23]{Ray1991} in favour of interactions with others to ``provide the primary driving force in evolution.'' (rather than independence and isolation as in an \gls{ea}) 
%\end{itemize}
%
%Selection implies limits on fidelity--at least one perfect copy on average at each replication required \parencite{Eigen1977}
%
%How does selection at one level get suppressed, and introduced at the next?  \parencite{Watson2015}
%
%``The term self-evolution should refer to an evolutionary process within a population system where the components responsible for the evolutionary behaviour are (only) the individuals of the population system itself. Every variation is carried out by the individuals. Selection pressure is generated implicitly through interaction among the individuals and not by external agents. The system must not contain a component that can be identified as a fitness function or global operators performing selection or variation (\eg crossover).'' \textcite{Dittrich1998}
%Cannot state in advance possible pre-adaptations \parencite{Vasas2015}
%Decisions about how and where individuals interact with the world should be made by the individuals themselves. \parencite{Soros2014}
%A rule should be enforced that individuals must meet some minimal criterion (MC) before they can reproduce, and that criterion must be non-trivial\parencite{Soros2014}
%The evolution of new individuals should create novel opportunities for satisfying the MC \parencite{Soros2014}
%A ``sufficiently large world for every individual to be evaluated'' \parencite{Soros2014}
%``the potential for a large degree of intrinsic adaptation'' \parencite{Taylor2001}
%Inexhaustible fitness landscape--implies rich, dynamical environment. \parencite{Vasas2015}
%
%\end{DRAFT}
%
%\subsection{Replication, reproduction and inheritance}\label{replication-reproduction-and-inheritance}

\section{Background and context}\label{background-and-context}

There has long been interest in understanding how biological evolution generates robust, novel, creative outcomes, unlike those seen in current artificial evolutionary systems. This has led to a drive towards understanding the fundamentals of biological evolution rather than in the historical inspiration of specific biological elements and structures, many of which are contingent and perhaps even arbitrary; certainly complex.
	
This \namecref{background-and-context} expands upon prior work that is relevant, but not specific, to our thesis to provide context and background.

\subsection{Life}\label{life}

\epigraph{%
In the world of human thought generally, and in physical sciences in particular, the most important and most fruitful concepts are those to which it is impossible to attach a well-defined meaning.}%
{\textsc{\\H.A. Kramers}}

Natural evolution is the best example we have today of a transformative, adaptive improvement process. Many fields would benefit from a robust general optimisation heuristic for use when exact methods are not possible, and the example of natural evolution is our current gold-standard. 

As stated by many (\eg \textcite{Pascal2013, Malaterre2015}) a definition of life is elusive, and probably not useful. There is no definite boundary between the living and the non-living; as \textcite{Pascal2013} goes on to explain, the most likely scenario for the origin of life is that there was a series of intermediates, of increasing degrees of ``aliveness''. The corollary is that it is hard therefore to imagine a clear cut transition between non-living and living. Think of a present day virus--is it alive, or not? Reproduction is generally thought of as a requirement for life, yet viruses cannot reproduce without co-opting the necessary machinery from an independent host. 

\Textcite{Fernando:2007pf} presents a partial compendium of definitions which illustrates the range of opinion:

\quote{
...at least one of the following outcomes: ‘open-ended evolution’ (Bedau et al., 2000); the origin of basic autonomy, i.e. a dissipative system capable of the recursive generation of functional constraints (Ruiz-Mirazo et al., 2004); a process ultimately capable of the production of nucleic acids or other modular replicators with unlimited heredity potential (Maynard-Smith and Szathmary, 1995; Szathmary, 2000); identification of ``the course of evolution by which the determinate order of biological metabolism developed out of the chaos of intercrossing reactions'' (Oparin, 1964); the coupled cycling of bioelements (Morowitz, 1968, 1971); the maximization of entropy production by a biosphere (Kleidon, 2004); the minimal unit of life (Ganti, 2003a, b); or an autopoetic unit (Maturana and Verela, 1992)?}{}

One interesting distinction between living and non-living comes from \textcite{Rasmussen2004}--non-living systems explore a state-space through thermodynamics and so, in a sense, through a random ergodic search. Living systems however almost universally employ evolution. Another information-theoretic view, from \textcite{Adami2015}, is that living systems can preserve information on a much longer timescale than non-living things. Given the relationship between information and entropy, the statement in \textcite{Schrodinger1944}, that metabolism is how ``living matter evades the decay to equilibrium'', seems very similar.

\begin{mdframed}[style=box, frametitle={The orgins of life}]

The evolution of life was almost certainly contingent, and there is an absence of evidence from early stages \parencite{Pross2013}. There were many possible pathways, and unless some record remains somewhere (either geological or phylogenetic), the actual path is essentially lost to history. So without evidence for historic aspect, it is not possible to test hypotheses by falsification, and hence they can only be speculative.

However, a consensus is forming that early life began with chemoautotrophs fuelled by energy from inorganic redox couples and biomass from CO\textsubscript{2}, and that innovations in carbon-fixation created the main branches in the tree-of-life \parencite{Braakman2012}. Initiation of selection marked by \gls{ida}, probably from an RNA world, followed substantially later by  the so-called Last Universal Common Ancestor (LUCA) \parencite{Yarus2011}. it is important to note for clarity, that LUCA was almost certainly not a single cell or even species, but rather a construct of evolutionary genetics because of the likely predominance of Lateral Gene Transfer (LGT) in archaic biology\footnote{http://sandwalk.blogspot.ca/2007/03/web-of-life.html}. Lateral or Horizontal Gene Transfer is thought to have been so common in early life that there was no single common ancestor, but rather genes from multiple lineages intermixed during this early stage into all lineages today \parencite{Ragan2009}.

Self-replicating RNA enzymes are shown in \textcite{Lincoln2009}, forming the basis of a selective system  (also see \textcite{Cheng2010,Powner2009} for formation of RNA in prebiotic conditions). Some elements of \gls{ida}  are thought to still bewith us in lineages of informational (for protein synthesis and RNA transcription) and operational genes (for some standard cellular processes) \parencite{Ragan2009}, for example the ribosome and ribonuclease P (RNase P) \parencite{Wilson2009}. The next major transition was to the Protein world, although predominance of RNA transcripts leads to suggestions that it should more accurately be called the RNA-Protein world \parencite{Altman2013}.

Two alternative models exist for the step from the prebiotic world  to \gls{ida} (\cref{major-stages-early-life}): replication- or genes- or RNA-first, and metabolism- or protein-first. Both metabolism and replication were almost certainly required for \gls{ida} however. A self-sustaining autocatalytic network (in terms of a RAF set, specifically a ``set of molecules and reactions which is collectively autocatalytic in the sense that all molecules help in producing each other (through mutual catalysis, and supported by a food set).'' \parencite{Hordijk2011}) is generally considered essential \parencite{Pross2013}, but not sufficient \parencite{Hordijk2011}. 

Both competing models--replication-first or metabolism-first--build on that. Autocatalysis, as expressed by self-replication of oligomeric compounds in replication-first; by cycles and networks in metabolism-first. In the broadest sense, life can be seen as an autocatalytic process where an entity catalyses the production of one or more descendant entities. From another perspective, metabolism-first privileges function, while replicator-first privileges descent.

The main issues with the replicator-first model are the sizeable step required from abiotic compounds to template-based replication (although ribonucleotides conceivably could form in pre-life conditions \parencite{Powner2009}). Templates encode information in biology, so require a encode/decode mechanism as well as an information code to represent the product. This is a step more complex than simpler duplication. By contrast, the main issue with the metabolism-first model concerns the shift from composome\footnote{See discussion in \cref{variable-replicators}.} inheritance to template-based, and the ability of composomes to fulfil the heredity requirement for natural selection.

The origin of life can be seen as the transition from chemistry to biology, and is analogous to our goal of transitioning from simple uninteresting systems to systems which evolve. However, the usefulness of the similarity is limited: the primary postulates of origins of life research are not our postulates. Some abiogenesis results fundamentally assume real-world chemistry and conditions, a constraint that doesn't apply to \gls{alife} or artificial \gls{oe}, and so is more restrictive than required. Other abiogenesis work such as on properties of autocatalytic sets, or the suitability of the genetic and catalytic properties of RNA for \gls{alife} \parencite{Cheng2010}, is broader in applicability.
\end{mdframed}

\begin{figure}
	\begin{center}
		\begin{tikzpicture}
		\node (prebiota) at (0,0) {prebiota};
		\node (metabolism-first) at (2,-1) {metabolism-first} edge [<-] (prebiota);
		\node (replication-first) at (2,1) {replication-first} edge [<-] (prebiota);
		\node (ida) at (4,0) {IDA} edge [<-] (replication-first) edge [<-] (metabolism-first);
		\node (luca) at (7,0) {LUCA} edge [<-] (ida);
		\end{tikzpicture}
	\end{center}
	\caption{Major stages in the evolution of early life}
	\label{major-stages-early-life}
\end{figure}

\paragraph{Living organisms are supremely well suited to their environments, and can adapt to environmental changes.}

Adaptation of organisms to their environments occurs in the main on two different time-scales.

Evolution by \gls{naturalselection} acts over a period of generations on populations of individual organisms. Changes are therefore relatively gradual, and many generations can pass before a change such as a beneficial mutation becomes ubiquitous in a population, for example the 300-500 generations observed for targeted modifications in lactose processing in \emph{E. coli} \parencite{Dekel:2005fk}. In contrast, gene regulatory effects act during the life cycle of a single individual, either during development to affect morphology, or during the adult lifespan in reaction to seasonal or other environmental cues. These regulation-driven changes are not in themselves heritable, but they can be assimilated back into the population by influencing the organisms fitness under natural selection (\eg,\cite{Baldwin:1896ly,Dennett:2003ve,Paenke:2009xe,Paenke:2007ve}).

Similar effects can be seen by another adaptive mechanism that operates on individuals during their lifespan: learning, where behavioural adaptations can also lead to genetic change (\eg \cite{Hinton:1987vy}.)

\paragraph{Natural selection, acting on populations, is the primary driver for long-term adaptation.}

The year 2009 saw the celebration of the 150\textsuperscript{th} anniversary of the publication of \emph{On the Origin of Species}, the explanation of evolution by natural selection, with extensive coverage in scientific and popular media. The terms are therefore fairly well known to many people, but what exactly does \emph{natural selection} mean? To quote from \textcite{Futuyama:1979tg}, \gls{naturalselection} is the ``differential survival and reproduction of genotypes''.

Let's examine each of the components in this idea in turn.

\emph{Differential survival.} Living organisms are constantly engaged in an intimate relationship with their environments. Indeed, according to the theory of \emph{autopoiesis} \parencite{Varela:1974qd}, organisms are defined by this engagement: to be alive means maintaining oneself against the surrounding environment. In general the more effectively the organism is able to do this, the more likely it is to survive. However, in practice survival for an individual may be affected by random events. Scholarship winning students can be killed by drunk drivers. Sardines in shoals flash and turn, yet sharks still manage to grab one or two from the shoal effectively at random. Averaged over a population however these chance events balance out; a succession of random trials leads to a skewed distribution of fitness away from the less able.

\emph{Reproduction.} In this sense, reproduction simply means inheritance. Only those characteristics that can be passed on from one generation to the next are relevant. One implication of this is that the only traits of an organism that matter to natural selection are those that are apparent while the organism can reproduce. Altruism and kin-selection, where an individual acts to increase the fitness of a related individual, are interesting for the light they shed on this implication.

\emph{Genotypes.} An organism's \gls{genotype} is the heritable information that defines an individual, of which the great majority is encoded in DNA (some \emph{epigenetic} information is inherited through DNA-methylation, maternal protein concentrations and other mechanisms. However, generally DNA remains the primary source.)

How then does natural selection unfold in practice? Although an organism is defined by its genotype, its survival is based not on the raw genotype, but on the expression of the genotype--called the \gls{phenotype}--that participates in the interaction with the environment. There is not necessarily a direct one-to-one mapping between genotype and phenotype; for example, environmental triggers during development can switch the phenotype in different directions (a phenomenon called \gls{polyphenism}.)

This indirect mapping (the \gls{gpmap})enables a number of important mechanisms significant to the operation of natural selection: first, changes in the genotype (caused by mutations for example) may build up independent of phenotype changes--the idea of \emph{neutral mutations} \parencite{Ohta:1996vn,Ohta:2002ys,Ohta:1973kx}. Examples from studies of RNA secondary structures (the physical folding of RNA molecules) show that many closely-related RNA sequences can produce the same RNA folding \parencite{Fontana:1993zn}. Adjacent changes often have little effect on structure.

Second, by extension, \textcite{Gavrilets:1997qt,Gravner:2007yd} have shown that in cases involving many gene loci under well-defined conditions there is a path between viable phenotypes that requires only neutral mutations.

Third, behaviours such as learning, rather than purely genetic mechanisms, can influence the form of the mapping from genotype to phenotype in combination with natural selection, as illustrated by the Baldwin effect \parencite{Baldwin:1896ly} and other examples of genetic assimilation \parencite{Hinton:1987vy,Siegal:2002qn,Waddington:1942jb}.

Finally, a \gls{grn} provides another mechanism to modify the \gls{gpmap} and hence to guide natural selection.

\paragraph{Novelties often arise from new regulatory connections rather than changes to genes.}

The phenotype in living organisms is many orders of magnitude more complex than the genotype, and the methods used during development to build this complexity are many and various.

Cells call upon a complex of regulatory processes to regulate the expression of genes, and hence to control development and morphology (as well as to implement the cell's basic machinery.) The majority of these processes (at current understanding) use proteins to affect the initiation of transcription (the production of RNA from a segment of DNA, the first stage of gene expression); most commonly the presence of a specific protein promotes or inhibits the transcription of a related gene, and without transcription the gene is silenced.

One step further along the protein production chain, editing and splicing of the RNA products of transcription in eukaryotes prior to translation (the production of a protein from mRNA) is also under regulatory control. Many alternative proteins from one transcript can be produced, triggered by environmental or other influencing factors; each alternative protein will have a different effect on cell function. As a result, in biological organisms, the phenotype is not uniquely determined by the genotype, and a genotype contains the potential for multiple different phenotypes under the influence of the environment (a phenomenon known as plasticity).

Patterns of gene expression can also have an effect outside the originating cell, a phenomenon crucial for development. Signalling proteins produced by a cell act as regulators on the machinery of surrounding cells, and epigenetic mechanisms allow signal effects to be long-lived so that a pattern of expressions may be inherited by daughter cells without the continued presence of the original signals. These mechanisms determine the fate of new cells during development: a cell originating in an area bathed in a particular combination of signalling molecules will develop in a specific manner (such as into a skin cell); another cell in another area will receive different signals, and therefore develop in a different way.

The development of an organism from a zygote is thus controlled by a pattern of gene expression under the combined influence of a regulatory mechanism and the environment.

As the regulatory mechanism is constructed from proteins and RNAs encoded in the genotype, and as the genotype is generally under the influence of natural selection, evolution also acts upon gene regulation, and hence upon development. The study of the evolution of the developmental mechanism itself is known as Evolutionary Development, or, more colloquially, EvoDevo. Although development was overshadowed  in the public eye for many years by genetics, its origins go back to almost the beginning of the modern understanding of molecular biology as seen in this quotation:

\quote{According to the strictly structural concept, the genotype is considered as a mosaic of independent molecular blue-prints for the building of individual cellular constituents. In the execution of these plans, however, co-ordination is evidently of absolute survival value. The discovery of regulator and operator genes, and of repressive regulation of the activity of structural genes, reveals that the genotype contains not only a series of blue-prints, but a co-ordinated program of protein synthesis and the means of controlling its execution.}{\parencite[p354]{Jacob:1961ys}}

\Textcite{Prudhomme:2007ax} believe that evolutionary novelties more commonly arise from changes or additions of regulatory \emph{connections} than from the development of \emph{new} genes or regulatory elements; that is, from changes to the network topology rather than from additions to the network elements. The underlying implication is that novelties are therefore new compositions of pre-existing elements, rather than being constructed ``\textit{de novo}'', and that production of novelties may be relatively rapid. Connection changes may happen quickly; by comparison, new genes may take many generations.

A network formed by the regulatory interactions between genes and the transcriptional products of those genes (\glspl{tf}) is known as a Gene Regulatory Network, or \gls{grn}.

\paragraph{Modularity is an emergent property of GRNs.}
Even a superficial look at a set of randomly sampled insects reveals a striking level of similarity between supposedly distantly related species. Mouthparts, segmented bodies, wings and halteres springing from body segments, antennae on head segments; common patterns abound. The explanation lies in the action of a group of homeotic (or body-patterning) genes, which act in concert to impose a structure on the developing body plan. These regulatory genes, amongst the first patterning genes to be identified and characterised, in fact appear in very similar forms across all animals and hence appear to share a common evolutionary origin, and to be highly conserved \parencite{Shubin:2009vw}. 

As an example of their action, the gene \emph{Ubx}, or \emph{Ultrabithorax}, is involved in development of the \gls{metathorax}, one of the body segments, in \glspl{arthropod} \parencite[pg. 696-697]{Watson:2008fm}. Changes in \emph{Ubx} expression in combination with expression of a second homeotic gene, \emph{Scr}, are responsible for some of the differences in body plan between two arthropod groups--brachiopods, and isopods. In brachiopods, the expression of \emph{Ubx} suppresses \emph{Scr} in the leading thorax segment leading to development there of swimming legs. In isopods however, \emph{Ubx} expression has been lost in that segment, and instead a feeding appendage develops \parencite{Watson:2008fm}.

The morphological patterns that result from changes in expression of the \emph{hox} homeotic genes are textbook examples of modularity caused by regulatory networks. These networks in general appear to take a characteristic form, sometimes called a `medusa' network \parencite{Kauffman:2004zi,Aldana:2007da} where a regulatory head controls the expression of many functional genes. For example, in \gls{drosophila}, the medusa has a head of around 80 genes \parencite{Aldana:2007da}. \Textcite{Davidson:2006wi}, using the well-characterised sea urchin genotype, identifies GRN `kernels' of interconnected and hence very robust regulatory genes that control essential functions such as heart development and the endoderm specification in sea urchins. 

\subsection{Evolution by Natural Selection}\label{evolution-by-natural-selection}

\epigraph{%
	Biological evolution consists of change in the hereditary characteristics of groups of organisms over the course of generations. Groups of organisms, termed populations and species, are formed by the division of ancestral populations or species, and the descendant groups then change independently. Hence, from a long-term perspective, evolution is the descent, with modification, of different lineages from common ancestors.}{\textit{``Evolution, Science, and Society: Evolutionary Biology and the National Research Agenda'', Working Draft, 28 September 1998}}

``We take it as given that biology instantiates ENS'', but that doesn't mean that the algorithm of biology is in fact \gls{ens} \parencite{Watson2012}. Adaptation in biology appears to precede Natural Selection, so adaptation is possible without \gls{ens} \parencite{Watson2010}, and other forms of evolution have been proposed in artificial systems (\eg the compositional evolution of \textcite{Arthur2009}), and on the boundaries of \gls{ens} in the domain of living systems (\eg composomal inheritance and \gls{hgt}). These variants are examined in the following \namecref{alternatives-to-evolution-by-natural-selection}, but here we concentrate on canonical \gls{ens}.

\Textcite{Godfrey-Smith2007} examined a number of summaries of \gls{ens} taken from the most influential items in the literature. The purposes of the summaries varied, but have interest for us ``as attempts to capture some core principles of evolutionary theory in a highly concise way.'' Incidentally, as an illustration of the difficulty of definitions, although the usual aim for a summary is to ``give conditions that are sufficient ceteris paribus for a certain kind of change occurring'', \textcite{Godfrey-Smith2007} notes that the scope of most summaries is somewhat ambiguous. They can be read as either being discriminatory--``this process is ENS''-- or alteratively in providing conditions that will result in \gls{ens} when it is assumed that the meaning of \gls{ens} is clear. In other words, these are alternative \emph{constitutive} or \emph{causal} readings; in the example of \textcite{Godfrey-Smith2007}, ``becoming pregnant\ldots{}{[}versus{]} being pregnant''.

The main examples selected by \textcite{Godfrey-Smith2007} are:

\begin{enumerate}
\item ``Owing to this struggle for life, any variation, however slight and from whatever cause proceeding, if it be in any degree profitable to an individual of any species, in its infinitely complex relations to other organic beings and to external nature, will tend to the preservation of that individual, and will generally be inherited by its offspring. The offspring, also, will thus have a better chance of surviving, for, of the many individuals of any species which are periodically born, but a small number can survive. I have called this principle, by which each slight variation, if useful, is preserved, by the term of Natural Selection, in order to mark its relation to man's power of selection.'' \parencite{Darwin1859}
\item ``If there is a population of entities with multiplication, variation and heredity, and if some of the variations alter the probability of multiplying, then the population will evolve. Further, it will evolve so that the entities come to have adaptations....'' (Maynard-Smith, in \textcite{Griesemer2001})
\item ``Any entities in nature that have variation, reproduction and heritability may evolve'' \parencite{Lewontin:1970mc} and ``1. Different individuals in a population have different morphologies, physiologies, and behaviors (phenotypic variation). 2. Different phenotypes have different rates of survival and reproduction in different environments (differential fitness). 3. There is a correlation between parents and offspring in the contribution of each to future generations (fitness is heritable).'' \textcite{Lewontin:1970mc}
\item ``...evolution will occur whenever and wherever three conditions are met: replication, variation (mutation), and differential fitness (competition)'' \parencite[quoting Daniel Dennett]{Ofria2004}
\end{enumerate}

\Textcite{Godfrey-Smith2007} concludes that the core requirement for \gls{ens} is some ``combination of variation, heredity, and fitness differences'', although he identified a number of differences between the summaries. For example, the most commonly cited summary is \textcite{Lewontin:1970mc}, but unusually that formulation states that ``fitness is heritable'' whereas typically phenotypic heredity (as appears in Lewontin's later 1980 summary) is stated as sufficient for a trait to evolve. 

These differences are also discussed in \textcite{Griesemer2001}, in particular with reference to the variations between the concept of inheritance in Darwin which includes both heritability (a capacity) and inheritance (a process carrying the capacity); Lewontin, which stresses the heritability while assuming inheritance, and Maynard Smith's multiplication which is actually about inheritance; his heredity is both \parencite{Griesemer2001}.

\subsection{Extensions and alternatives to evolution by natural selection}\label{alternatives-to-evolution-by-natural-selection}

Quite apart from these differences in interpretation, the major theoretical difficulty in a literal application of \gls{ens} to artificial systems is captured succinctly by \textcite{Griesemer2005}: ``Darwin's theory of evolution by natural selection is restricted in scope. One sense in which it is restricted is that it refers to organisms.'' Organisms are not defined, but the context and scope is clearly biological. 

Although \gls{ens} is usually only discussed within the context of modern day biology, as we've seen when examining the definition of \gls{ens} there is nothing exclusively biological about the standard formulations. By abstracting the concepts of variation, selection, and in particular inheritance or heredity, a generalised form of \gls{ens} can be developed that goes beyond the usual biological readings. 

A specific example lies in the transition from non-living to living things. One of the main problems (of several) in extending \gls{ens} to prebiotic entities is that the meaning of the foundational elements--such as variation, selection, heredity, multiplication--are generally couched in biological terminology. For example, heredity is often discussed in terms of alleles, or traits, rather than in more general ways. However, there must have been some point at which the prebiotic processes transitioned to \gls{ens}, and it is clear that this transition did not happen abruptly in a population of fully-fledged modern organisms. The pathway to \gls{ens}, the evolution of evolution itself, is one of the main open problems in origins-of-life research, and it involves an extension of those foundational elements back into the prebiotic world.

This example of the transition to life is an example of \emph{extending} \gls{ens}; two further examples are now presented that are \emph{alternatives} to \gls{ens}: DaisyWorld, or regulation without selection, and evolution in an entirely non-living domain--the evolution of technology. 

\Textcite{Arthur2009} describes a mechanism for the evolution of technology, where evolution is used in the sense of ``all objects of some class are related by ties of common descent from the collection of earlier objects''. Evolution in technology occurs by using earlier technologies as building blocks in the composition of new technologies, and these new technologies then become building blocks for use in later technologies, and so on. Arthur calls this ``combinatorial evolution.'' But what is the starting point? How is this regression grounded? Arthur proposes that the capture and harnessing of natural phenomena starts each lineage and provides new raw components for inclusion in later technologies. \Textcite{Bourrat2015} comments that distributive evolution (where only the distribution of elements changes, as result of selection or drift) cannot result in novelties: Arthur's answer is that novelty comes from this incorporation of new phenomena from the source, the natural world.

Evolution is related to innovation: in fact, Arthur claims that by understanding the mechanism by which technologies evolve we will understand how innovations arise. In other words, innovations arise as the result of an evolutionary process, rather than \textit{de novo} from the brain of a designer. Darwinian evolution, or natural selection, is not appropriate for technology. Arthur quotes from Samuel Butler's essay ``Darwin Among the Machines'' to illustrate the impossibility of the slavish adoption of biological models: ``{[}t{]}here is nothing which our infatuated race would desire more than to see a fertile union between two steam engines\ldots{}''.

The first obstacle to a more general scope is the existence of innovations such as the jet engine, laser, railroad locomotive, or QuickSort computer algorithm (to name Arthur's examples.) Innovations seem to appear without obvious parentage; they do not appear to be the result of gradual changes or adaptations to earlier technologies. Arthur's answer is to look inside the innovation and to recognise that each is made up of recognisable components or modules; the key lies in the nature of heredity in technology. Technologies are formed by combining modules of earlier technologies. These groupings start as loose assemblages to meet some new function, but over time become fixed into a standard unit (for example, the change in DNA amplification mechanisms from assemblages of laboratory equipment to standard off-the-shelf products.)

However, even Arthur in his rejection of ``Darwinian evolution'', describes a process that still relies on selection, variation and inheritance. This is not the case in our final example, DaisyWorld.

\Textcite{LovelockMargulis2011} propose DaisyWorld as a example of an alternative to selection for regulation, based on two feedback loops. \Textcite{Saunders1994} explains how regulation can emerge in DaisyWorld without selection--the planet's temperature is adjusted to meet the conditions for maximum growth of the daisies without the daisies adapting to the planet. ``As a result, regulation, one of the most fundamental and necessary properties of organisms, appears without being selected for. What is more, it appears as a property not of the daisies, on which natural selection may have acted, but of the planet, on which, as Dawkins rightly points out, it could not'' \parencite{Saunders1994}.

The fundamental insight in DaisyWorld is that individuals modify environments (that is, niche construction): the daisies adapt the planet (specifically the temperature for maximum growth) to suit themselves, rather than adapting themselves to the planet; and in fact there's little benefit to adaptation by the daisies to the planetary conditions. As \textcite{Saunders1994} states, ``the ability to withstand a greater variability is not the result of Darwinian adaptation. On the contrary, it exists because of the absence of Darwinian adaptation.''

\subsection{Evolutionary Algorithms}\label{ea}

The field of \glspl{ea} originated in the exploration of abstract biological evolution, but rapidly diverged into problem-solving and optimisation \parencite{De-Jong:1993gy,DeJong2006}. The fundamental differences between the biological original and the modern canonical \gls{ea} include:

\begin{compactitem}
\item \Glspl{ea} search through a fixed space, and hence cannot surprise by 'kind', only by 'degree' (\eg \cite{Nellis2014})
\item The evolutionary process in an \gls{ea} is not itself subject to evolution; it is designed, and a large body of literature exists to guide the implementer in the choice of algorithms to employ.
\item Fitness in an \gls{ea} is measured by an explicit \emph{objective function} whereas in biology fitness \emph{emerges dynamically} through continuous interaction with the environment. ``The difference is that we require a system with the potential for a large degree of intrinsic adaptation for open-ended evolution, rather than a system where the selection of individuals is determined by an externally-defined fitness function'' \parencite{Taylor2001}.
\item \Glspl{ea} conduct a series of discrete trials of fitness, rather than a continuous evaluation.
\item Individuals in a \gls{ea} do not interact other than indirectly through a pre-designed selection mechanism.
\item \Glspl{ea} are not dynamical systems unlike the biological original. Dynamics required for novelty-generation \parencite{Nellis2012}.
\item Natural organisms must replicate themselves to pass on genetic information--``final arbiter of fitness'', and interaction with other organisms and with environment \parencite{Ofria2004}.
\end{compactitem}

The first two of these differences are significant; \glspl{ea} are not capable of open-ended, or evolutionary, evolution. However, recent research in biology continues to be applied to improve the performance of \glspl{ea} in their core function of optimisation:
\begin{compactitem}
	\item Redundancy and degeneracy--\eg \cite{Whitacre:2010qy}
	\item Novelty--\eg novelty search \parencite{Lehman:2008cr}
	\item EvoEvo, or the evolution of evolution--\eg the remainder of this work.
\end{compactitem}

A summary of some relevant work in the application of \glspl{grn} to \glspl{ea} to improve the robustness and modularity of solutions can be found in \cref{applications-of-grn-in-eas}.

A form of re-unification between biological evolution and artificial evolution has been attempted in works such as \textcite{Paixao2015}, based on ``models in theoretical population genetics and in the theory of evolutionary computation.'' For example: ``Some EDAs can be regarded abstractions of evolutionary processes: instead of generating new solutions through variation and then selecting from these, EDAs use a more direct approach to refine the underlying probability distribution. The perspective of updating a probability distribution is similar to the Wright--Fisher model.'' \parencite{Paixao2015}

\subsection{Artificial Life}\label{alife}
The well-known description of \gls{alife} by \textcite{Langton1989} is that it is ``life-as-it-could-be'', rather than ``life-as-we-know-it''. The main conference series in \gls{alife}, also called ``Alife'', uses as a tagline ``Synthesis and simulation of living systems'', which is closely related to the definition of \textcite{Bedau:2007ga} : ``an interdisciplinary study of life and life-like processes, whose two most important qualities are that it focuses on the essential rather than the contingent features of living systems and that it attempts to understand living systems by artificially synthesising simple forms of them.''

Interestingly, these later two definitions place the study of living things as a subfield within \gls{alife}, appropriately as many consider life to be the only working example we have of the most interesting ideas in \gls{alife}, including open-ended, or creative, evolution.

Technological imperatives have driven the division of \gls{alife} overall into three largely independent subfields: \emph{hard}, \emph{soft} and \emph{wet} \gls{alife}, where hard \gls{alife} is implemented in physical hardware (made from atoms), soft \gls{alife} in computer software (from bits) and wet \gls{alife} from biological compounds. Soft \gls{alife} has the great advantages of relative cheapness and immediacy, and consequently is the basis for the majority of current research, including this work.

Good summaries of the state of the field can be found in \textcite{Aicardi2010,Aguilar2014}; \textcite{Aguilar2014} also contains a comprehensive history from early (classical Greek) interest in automata, through the publication in 1818 of Mary Shelley's ``Frankenstein; or, The Modern Prometheus'', which revitalised interest in artificial creatures in modern times, to the consensus view of the birth of the modern field of \gls{alife} in 1987 with the first ``Workshop on the Synthesis and Simulation of Living Systems'' in Sante Fe, New Mexico.

\section{Guide to this work}

This work is structured in four logical parts. This introduction contains material general to the entire thesis and establishes the overall research context; the second block of material (the first formal part, labelled \cref{part2}) moves to theory, specifically the meaning of fidelity and inheritance, and an experimental investigation of a theory-based model; the third (labelled \cref{part3}) to consideration of how a system based on this model might be implemented in an artificial system, specifically our \gls{achem}, ToyWorld; and the final, fourth, part (in \cref{thesis-conclusions}) closes the work with conclusions, a general discussion, and some thoughts on future work.

The first part begins with this introductory chapter to motivate the work and place it in context. The overall impetus for study of inheritance and heredity in artificial systems comes from the age-old dream of self-improving systems, more recently expressed in the research topic of \gls{oe}. \Gls{oe} increasingly is seen as an evolutionary, emergent, process and as such we can identify a set of fundamental elements considered either necessary or sufficient for \gls{oe} in artificial systems. One of those elements is replication, and establishing the means by which an evolutionary replicator can emerge in an artificial system drives the work described in this thesis.

\Gls{oe} is in turn inspired and, in some sense, in competition with the achievements of evolution in living systems. \Cref{life} provides a broad but necessarily shallow overview of relevant ideas from modern biology, to explain this link with \gls{oe} and to give some sense of what might be possible should the goal be reached. Evolution in general and the specific evolutionary mechanism behind the richness of biology, \gls{ens}, are described in \cref{evolution-by-natural-selection} . As a second example of an evolutionary process that fits within the broad umbrella of evolution in general, compositional evolution is introduced in \cref{alternatives-to-evolution-by-natural-selection}, foreshadowing its importance to discussions of the origins of life and the evolution of technology.

The field of \gls{alife} is almost synonymous with \gls{oe}; the spirit and historical context of the field permeates this work, and so a brief introduction in \cref{alife} seems appropriate.  Of the wide variety of approaches to \gls{alife}, one in particular, \glspl{achem}, seems most promising for \gls{oe} in \gls{alife}, as seen concretely in the number of relevant works reviewed in \cref{previous-work} specific to replication and replicators in artificial chemistries. The structure follows the broad classification proposed by \textcite{Zachar2010} which moves from simple multipliers through to informational replicators and finally reproducers. Most existing work can be placed in the earlier categories; very few systems are capable of emergent replicators, and none at all meet the requirements needed for reproducers.

\Cref{previous-work} reviews the previous literature on the relationship between inheritance and heredity in artificial evolutionary systems. Excluded are purely theoretical models; the most relevant are covered instead in \cref{previous-work-p2} of \cref{part2} where they inform the discussion in that \namecref{part2} of our own proposed model. Instead, \cref{previous-work} summaries work organised into four related categories of reproducing entity, or replicators, defined by \textcite{Zachar2010}. Of these, only systems in the last category, Informational Replicators, meet the requirements for a evolving evolutionary mechanism.

In \cref{methods} we outline our motivating research questions (\cref{research-questions}), discuss the various tools available to us (simulations, models and experiments) and finally describe the methods selected for use in the remainder of this work.

The next part, labelled \cref{part2}, begins, as already noted, with a review of some specific work on the theory of evolutionary systems (\cref{previous-work-p2}) before introducing our simulation model for exploring inheritance, variation and selection (\cref{base-model}), and the hypothesis (\cref{h2}). With a parameterised simulation model such as ours, it's important to understand the implications of parameter value choices on the simulation's behaviour, and so in \cref{practical-considerations} and \cref{reducing-the-parameter-space-for-the-experiments} we do just that. With that to guide us, we then conduct two sets of experiments, using the simulation model, to test our hypothesis under, first, stable environmental conditions (\cref{experimental-test-of-h2-under-fixed-conditions}), and then second, under changing conditions (\cref{model-behaviour-in-changing-environments}.)

In \cref{part3} we move to implementations, and an exploration of the second of our research questions: how an evolutionary inheritance mechanism might be implemented in an artificial system. The test bed is our \gls{achem} ToyWorld, previously described in \textcite{Young2013, Young2015}, and again in this work in \cref{toyworld}. As in the previous part, the behaviour of ToyWorld is driven by certain parameter choices, and two of those most important of those choices--the strategies for reactant and product selection--are investigated in \cref{reactant-and-product-strategies}.

Finally, in \cref{thesis-conclusions} we summarise and draw conclusions, and make some suggestions for future work.

\section{Previous publications}\label{previous-publications}

A version of \cref{reactant-and-product-strategies} was published as \textcite{Young2015}, and material from \cref{model-validation} appears in \textcite{Young2013}.

The source code for ToyWorld, the \gls{achem} from \cref{toyworld}, has been released under an GNU GPL v2 open source licence and is available from GitHub (see \cite{toyworld}).

\section{Contributions}\label{contributions}

This thesis makes the following novel contributions:

\begin{compactenum}
\item As far as is known, the first review of inheritance and heredity in artificial chemistries (in \cref{previous-work}.)
\item A parameterised model of the emergence of inheritance from variation and selection, incorporating an exogenous property for fitness suggested by \textcite{Bourrat2015} (\cref{part2}.)
\item Experimental tests of the sensitivity of this model to the choice of parameter values (\cref{reducing-the-parameter-space-for-the-experiments}.)
\item Using this model, a demonstration by simulation that heredity emerges under a variety of environmental forms (\cref{model-behaviour-in-changing-environments}.)
\item Demonstration by simulation that \emph{erroneous copying} \parencite{Zachar2010}--variation plus inheritance--can result in informational replicators (\cref{part2}.)
\item A new constructive\gls{achem}, ToyWorld, compatible with the model in \cref{part2}  (\cref{toyworld}.)
\item Comparison of four options for reaction and product selection strategies in an \gls{achem} (\cref{reactant-and-product-strategies}.)
\end{compactenum}

\chapter{Previous work}\label{previous-work}

\epigraph{%
It may appear that the properties one would have to assign to a population of self-reproducing elements in order to obtain Darwinian evolution are of a spectacular simplicity. The elements would only have to: (1) Be self-reproducing and (2) Undergo hereditary changes (mutations) in order to permit evolution by a process based on the survival of the fittest.}%
{\textit{\\Nils Barricelli}}

Our focus in this work is on the nature of inheritance and heredity, specifically \emph{endogenous} inheritance mechanisms that can evolve. We’ve seen in \cref{evolution-by-natural-selection} that heredity is one of the foundational elements of \gls{ens}, the means by which information from one generation can be leveraged in the next. This form of inheritance, from parent to child, is called \emph{vertical} inheritance, and the type of entity that participates in and enables vertical inheritance (that is, can form a child) is called a \emph{replicator}. 

Now, in the transfer from the prebiotic world to the biotic one, it’s clear that the proportion of information held by a predecessor that could be passed on to its successor increased somehow, probably over many generations, from ``none'' to ``nearly all'', as seen today; \textcite{Vasas2012a} links heritability to the correlation between the parent and child entities. Heredity is therefore likely a matter of degree, rather than being a binary relationship; related or not-related, and that opens up the possibility that a series of gradual changes might over time transform a very poor replicator into a very good one. Of course, one possible process for this transformation might well be \gls{ens}.

This progression is comprehensively explored in \textcite{Zachar2010}, extending earlier work by \textcite{Szathmary1999,Szathmary:2006ty}: 

''The simplest of replicators is the \emph{exact replicator}, which is \emph{non-informational}, and any change made to it causes a change in the phenotype. If a variation can arise in the structure in such a way that it does not change equivalence of the entity, then it is a \emph{variable} replicator, with more than one stable state. If such changes can be passed on to the offspring then the replicator is \emph{informational}. If the non-heritable part is constructed by a developmental process, then the replicator is a \emph{reproducer}.'' \parencite[p.21]{Zachar2010}

Interestingly, in biology at least, the starting point for the development of modern replicators might have began with \emph{horizontal} inheritance. By analogy, it’s easy to envisage another kind of information transfer where information is passed from one entity to another horizontally, without replication or a parent-child relationship. In today’s biology, \gls{hgt} provides one mechanism for this; in the early prebiotic world, the horizontal transfer of information might have occurred through a number of conceptually similar mechanisms (such as auto-catalytic cores) that didn’t involve genes. 

First, the changes from horizontal inheritance can be heritable in a vertical sense. In organisms with genes, by definition under \gls{hgt} the change becomes part of the target’s genome and hence heritable. In entities without genes, there are forms of entity where the horizontal change can be subsequently inherited vertically. Second, mechanisms for horizontal transfer don’t require replication (by definition). Therefore it’s possible that they can act as a precursor for the development of replication. That is, horizontal inheritance plus selection might be sufficient for replication, rather than replication being required for inheritance.

But what exactly does it mean for something to be a replicator? Is a rock that erodes to form grains of sand a replicator\footnote{In \textcite{Bourrat2015} rocks are given as examples of \emph{persistors}, unable to reproduce and subject to only a ``weak'' form of selection for hardness}? Is a set of autocatalytic reactions that splits into two replicating? \Textcite{Dawkins1976} was the first to define replicators, including a range from biological genes to non-biological ideas (memes) in the scope of the definition. Many other definitions and formulations followed as various properties or features were examined. 

%\textcite{Hogeweg1998} describes attractor-based heredity (inheritance of state) and storage-based. 
%
%Heredity by \textcite{MaynardSmith1999} is modular or holistic--if module changed only that module changes in descendants, holistic change part changes whole. Believe it true that unlimited heredity implies modular. \textcite{Szathmary1999} calls these digital and holistic, and adds phenotypic replicators whereby ``phenotype or function of one object is translated to the other, without any modular copying effect” 
%
%Modular/template heredity has advantages of: mutations hereditary; problems of correct copying \parencite{Eigen1977}

Relatively recently, \textcite{Zachar2010} saw a need to reexamine the definition primarily to resolve issues of discrimination between entities which are clearly replicators or not replicators, and those which are borderline, and between biological replicators and non-biological or cultural ones. 

%A regenerating/recreating entity can produce at least one entity equivalent to it. It is possible that the original entity immediately decomposes (that is, cannot be the subject of further turns of the cycle), causing sequential replacement, although this is the most simple of regenerating entities. If it can effectively increase the number of entities equivalent to itself, then it is autocatalytic and is a \emph{replicator}.

\Textcite{Zachar2010} conclude that a replicator is ``any autocatalytic entity for which there is a selection process defined'', using autocatalysis in the general sense of a cyclical process that increases $A$ by $X + A\rightarrow 2A + Y$, and where selection has the fairly standard definition of ``a process, acting on a particular population of entities in a particular environment, which sorts entities according to their phenotypes.'' \parencite[p.21]{Zachar2010}

In the following \namecrefs{non-informational-exact-replicators} we review some pertinent previous work following the classification developed in \textcite{Zachar2010}\footnote{With the exception of \emph{reproducers}, replicators with a developmental component, as these build upon informational replicators and, as we shall see, we lack complete informational replicators.}.

\begin{DRAFT}
Initially replicate implicitly, rather than using some encoding of the replication process  \parencite{Taylor2001}
Be constructed entirely of `material' components, allowing the possibility of different encodings of information. (\quote{the very stuff from which they are constructed is a valuable resource of matter and energy}{\textcite[s3.6]{Taylor2001}})
Very rich combinatorial generative mechanism \eg organic chemistry \parencite{Vasas2015}

``An important property of most strong AL systems is that they contain the ability for self-reference. For instance, Ray's Tierra organisms are able to read, copy, and modify their own code. In Fontana's algorithmic chemistry every object is a character string able to process other objects by using the lambda-calculus that maps the character string into an (active) function. The dualism inherent in those systems can be traced back to Godel who defined a mapping of mathematical statements into natural numbers `` that allowed self-reference, to Turing's universal machine, and to von Neumann's stored program computer .''\parencite{Dittrich1998}

``We want to embody the copying program, implementing it as a phenomenon resulting from the interaction of multiple mechanisms in a world. This will allow the copied strings to vary and evolve, and will also allow the copying process to vary and evolve.'' \textcite{Nellis2014}

Major transitions in evolution not possible without self-referentiality - unit of evolution must change between levels e.g., from molecules to cells. This was/is accomplished by changing the way that individuals interact, from competition to cooperation (fitness change) to form the next level,; independent replication before transition, must replicate as part of a larger whole afterwards. 
Fundamental for complexity - complexity associated with levels  \parencite{Watson2015}
\end{DRAFT}

\section{Exact replicators}\label{non-informational-exact-replicators}

Autocatalysis by definition is replication; where the entities are molecules, the autocatalytic reaction $X + A\rightarrow 2A + Y$ replicates the molecule $A$ (\eg \textcite{Lifson1997}). Evolutionary variation though is difficult as the exponential growth of the main autocatalytic product will generally overwhelm the molecules produced by any side reactions unless 1) they are also autocatalytic, and 2) the solution contains the molecules needed to initiate and maintain this alternate autocatalysis cycle.

%GGL/ToyChem \textcite{Benko2003,Benko2005}                        	&Atoms/Molecules&&None--at level of reactions\\	
%RBN-World \textcite{Faulconbridge2011}                            	&Boolean Networks and Graphs& None--at level of ACS\\

\emph{RBN-World} \parencite{Faulconbridge2011} is a \gls{achem} where the entities take the form of \gls{rbn} \parencite{Kauffman:1969ne}, with the addition of a bonding mechanisms to allow for their composition and decomposition. The resulting form of \gls{rbn} is called a bonding \gls{rbn} or bRBN.  Larger structures are formed by ``bonding'' two independent bRBNs at each bRBNs bonding node. ``All reactions are between two reactants; it is assumed that more complicated reactions can be expressed as a series of two-reactant reactions with intermediate structures.'' The choice of reactants is described as ``Gillespie-like'', and essentially random, uncorrelated in any way with reaction energies or rates \parencite[chap.8]{Faulconbridge2011}.

Each bRBN is a synchronous RBN, made up of a number of nodes, each with an initial state (\emph{true} or \emph{false}) assigned randomly and with an input/output matrix assigned randomly. Finally k(=2) inputs are established per node. The bonding method uses ``cycle length as the bonding property and equality as the bonding criterion....bonds only exist between bRBNs that have the same cycle length.'' After initial bond formation the algorithm recalculates cycle lengths, and checks again for equality. This might result then in decomposition (records are kept of composition operations so that the reverse decomposition can be easily done.)

A number of parameters affect the behaviour of the chemistry, and so a series of experiments sampled from the parameter-space, and then used a GA, to search for interesting variants as measured by non-catalysed ``loops'' (as the preferred measures of auto-catalytic sets and hypercycles are too rare for use as a measure) \parencite[chap.8]{Faulconbridge2011}. 

The development of RBN-World involved many design choices, some essentially adhoc, such as the choice of \gls{rbn}: ``...the choice to use RBNs as the sub-symbolic representation in RBN-World was based on limited information. As a discrete dynamical system that is computationally tractable yet also spans a wide range of behaviours, RBNs met the appropriate criteria. It is not expected that RBNs are the best representation however; others may be more suitable for particular emergent properties.''

The model for \emph{Chemical Evolution by Natural Selection} in \textcite{Fernando:2008xy,Fernando:2007pf} is driven by origin-of-life objectives (``the evolution of chemical networks that lead to autonomous systems''), and takes the form of a simulation of laboratory experiments of lipid aggregates (``phase separated ‘individuals’, e.g. liposomes'') in a reactor. The molecules and food molecules (that make up an autocatalytic cycle) share a common representation and underlying chemistry, while replication, unlike the (rare) autocatalytic cycles in \textcite{Faulconbridge2011}, is by `` division by externally imposed agitation, i.e. replication rather than self-replication.''. New molecular species (evolutionary variation), comes from chemical ``avalanches'' initiated when two existing species are chosen at random, tested to see if a reaction between them is possible by thermodynamics, and then the resulting products seeded in the reactor at low concentrations. These new reactions may be autocatalytic, or the products may complete a food-set for another autocatalytic reaction, or they may enable a side-reaction from an existing autocatalytic one. Introducing these new species into the reactor therefore can trigger the rapid formation of a series of novel products--a chemical avalanche.

\section{Variable replicators}\label{variable-replicators}
\Textcite{Ganti:2003hl} and  \textcite{Eigen1971} showed that distinct, organisationally different alternative autocatalytic networks in the same environment might compete, and the fittest would prevail. A number of models have been proposed since where autocatalytic networks form stable components that can be inherited in a modular fashion in a process called compositional inheritance. There are however some difficulties in establishing anything more than a ``variable replicator'' (in the term of \textcite{Zachar2010}) with these models, namely inconsistent inheritance and a limited potential for information transmission.

%\emph{Reflexively Autocatalytic Polymer Networks} (RAPN) \parencite{Kauffman1986} are evolvable. First, likelihood of such networks higher than expected (Hordijk and Steel) and second, in \textcite{Vasas2012} putting these networks into compartments (so not well-stirred) then can do directional selection.

In the most well-known of these models, the graded autocatalysis replication domain (\emph{GARD}) model \parencite{Segre1998}, highly catalytic molecules determine the properties of the compotype (compositional genotype), and these are not necessarily inherited equally. Instead a child may or may not inherit one of these molecules and so its properties may be similar to or very different from its parent \parencite{Vasas2015, Vasas2012, Vasas2012a}. Information fidelity varies widely; the Eigen threshold \parencite{Eigen1971} applies, and mutation rates overwhelm selection \parencite{Vasas2015, Vasas2012, Vasas2012a}.

Mutations in cycles \parencite{Vasas2012a} are generally not heritable as mutant copies are rarely functional in autocatalysis. This makes mutation problematic as a source of variation in these autocatalytic systems; one alternative to mutations is avalanches, to form new cycles from the side-products of the base networks (as discussed in the previous \namecref{non-informational-exact-replicators})

% Any molecule in the core will produce the remaining species in both the core and periphery \parencite{Vasas2012a} and so reconstitute the core,

The model of \parencite{Vasas2015, Vasas2012, Vasas2012a}, based on GARD, tests the hypothesis that compositional inheritance is possible where there is a parent-offspring correlation in molecular composition. The mechanism is autocatalytic cores made up of one or more linked autocatalytic loops to provide the compotype. The core forms an attractor, where one core equals one attractor, but multiple cores are required of course for selection. Multiple cores (produced by inhibition \textcite{Vasas2012a}) provide multiple attractors, but the attractors must be stable for selection to be stable and meaningful. Unlike GARD, which generates only single-core networks, the model in Vasas is capable of multiple cores, but as cores are the equivalent of a single bit of heritable information, it's hard to see core-based inheritance being capable of unrestricted heredity--there are practical limits to the number of stable cores that can co-exist in a system. 

\section{Informational replicators with shortcut replication mechanism}s

A \emph{shortcut} is one where the replication mechanism is directly implemented by the experimenter rather than being an emergent property \parencite{BanzhafBaumgaertnerBeslonEtAl2016}. As the mechanism is external to the entities it is not under selection; shortcut replicators are not, by our definition, open-ended. One of the most well-known systems with a shortcut replication system is Squirm3.

\emph{Squirm3} \parencite{Hutton2007,Hutton2002}. An artificial system capable of life-like \gls{oe} (creativity), initially developed with the goal of testing the hypothesis of \textcite{Taylor2001} \parencite[p.341]{Hutton2002}.

All elements in Squirm3 are constructed from atoms defined by \emph{types} (e.g., a, b, c\dots) and \emph{states} (e.g., 0, 1, 2\dots). Atoms and hence molecules are located on a 2D grid, and molecules cannot overlap or pass through each other. All reaction rules, such as \emph{R1: e8 + e0 $\rightarrow$ e4e3}, are pre-specified (\textcite[p.4]{Hutton2007} and \textcite[p.49]{Faulconbridge2011}), and a shortcut consisting of a set of eight rules in \textcite{Hutton2002} is sufficient to replicate single molecules. As all reaction rules, including those responsible for replication and hence inheritance, are exogenous to the model, the inheritance mechanism in Squirm3 is not evolvable. 

Individual entities in \textcite{Hutton2002} are simply single molecules. \Textcite{Hutton2007} introduces cells made up of a collection of molecules and bounded by a membrane of a particular atom type of limited reactivity; the membrane is intended to allow individuals to benefit from innovations by protecting their internal reactions from others. With a greatly increased set of predefined reaction rules (34 rules now for the cell replication shortcut, extended further again in \textcite{Lucht2012}), each cell has the capacity for division and mutation (through the stochastic application of equally applicable rules).  Selection is purely by indirect competition for the raw materials (atoms in the environment) required by the reactions in a cell; interactions between individuals are purely through this indirect competition (niche construction without direct interaction). Cells in \textcite{Hutton2007} are incapable of making use of resources from other cells (as they are effectively protected by non-reactant membranes) and so an intermittent exogenous mechanism (``floods'') is used to return the atoms in a number of randomly chosen cells to the environment.

\section{Informational replicators or self-replicators}
Replication in these systems emerges from the base (or in the terminology of \textcite{BanzhafBaumgaertnerBeslonEtAl2016}, level-0) rules without being directly specified by the experimenter or designer. However, even here most systems still shortcut this by providing a seed or universal ancestor (\eg \textcite{Ofria2004}) that contains a working module for replication that can be then modified (for good or ill) in each subsequent generation.

\subsection{String-manipulation systems}

A new object in Fontana's Algorithmic Chemistry, or \emph{AlChemy} \parencite{Fontana1992}, is defined as the interaction expression, $h$, of two randomly chosen objects $f$ and $g$, if, and only if, the interaction expression contains at least one variable and one primitive operator, and is shorter than some maximum length \parencite[p.173--p.180]{Fontana1992}. New objects in AlChemy are therefore the children of two parents.

How then is the interaction expression $(('f)('g))$ between $f$ and $g$ evaluated to produce $h$? AlChemy is a form of pure LISP (with some ``minor idiosyncrasies''), based on toyLISP, with six primitive operators defined in \textcite[p.205]{Fontana1992}. The interaction expression is defined in \textcite[Definition A.9, p.204]{Fontana1992} as $V[(('f)('g)),()] = (V[f,(a\leftarrow g)])$, using the notation $V[e,L]=v$ to mean the expression $e$ with the ``association list'' $L$ (a list of value assignments between atoms and expressions) evaluates to $v$. The result $h$ is described as $f(g)$ and the process as $f+g \rightarrow (('f)('g)) \rightarrow h + f + g$.

Clearly reproduction in AlChemy is self-referential--the process to construct a child object is defined in the code of the parent objects. Unfortunately, inheritance doesn't follow straightforwardly as the reproduction process is unusual in two important ways. First, each new object has two parents, rather than one, as is more common in artificial systems (although not necessarily in biology.) Producing new objects as some function of the two parents in turn means that the relationship between parents and child is not a straightforward mutation or other syntactic difference, but rather a complex functional relationship. What this means for the relationship between the parent's fitness and the child's fitness is not obvious. It seems that the fitness differences in AlChemy might be more extreme than in other systems where parent and child have a more straightforward relationship.

%\begin{DRAFT}
%\parencite{Fenizio2000}:
%
%Original AlChemy reactions of form $A+B\rightarrow C$ where $C$ replaces an existing element.
%
%This system generates $A+B\rightarrow C_1+C_2...C_n$ where $C$ is a multiset of size $n$. Done by modifying the original K rule to detach x2 and eliminate both original elements (like reactants in chemistry)
%
%Uses combinators rather than lambdas
%
%To prevent from stopping (out of elements) added modification where randomly add/remove some elements
%
%Combinator first combines (appends) elements, each element other than first bracketed. Then each 1-term combinator applied to string, where it makes specific changes \eg K x1x2s0-\textgreater{}x1s0 (s0 is remaining substring, may be null). Apply until no further reductions possible (that is, in normal form). Two combinators are equivalent if can be reduced to same combinator (and previously noted that order is not important--same results regardless of order).
%
%Free pool of atoms for conservation of ``mass''
%
%\parencite{Fenizio2001}:
%
%Experiment to show spontaneous formation of autopoietic cells, with a focus on ``identity as an entity separated from its environment'', that is, membrane formation. Graph used to model spatial structures: ``an artificial chemistry (AC) is embedded in a graph, with each molecule being a vertex of the graph and possible interactions being allowed only along the edges of the graph''. Molecules are composed of atoms taken from a 
%
%``Molecules are built from a substrate of elements called atoms. There are seven types of atoms ($I, K, W, R, B, C, S$), each with a different function. The total number of atoms in the reactor is kept constant during a run. Free atoms (not bounded in molecules) are separately stored and form a global pool.''
%
%As the rules for the combinations of two molecules are predetermined (the reaction mechanisms are described in \textcite{Fenizio2000}), this model is not by our definition open-ended.
%\end{DRAFT}
\subsection{Automata}

\emph{Coreworld} \parencite{Rasmussen1990}, inspired by the early computer game ``Core War''\footnote{See \url{corewar.co.uk}}, set segments of simplified assembly code into competition in core memory. The assembly command to copy values from one memory location to another can spontaneously introduce errors into the copying, and hence can introduce evolutionary variation. However, as reviewed by \textcite{Ofria2004}, the system ``collapsed into a non-living state'' perhaps because organisms could copy over each other in the shared system memory \parencite{Ofria2004}.

In \emph{Tierra} \parencite{Ray1991} mutations are introduced during replication by randomly flipping bits during the copy operation (at a given rate of generally between 1 bit flip per 1,000 and 2,500 instructions copied). This rate is set by the experimenter, and is not evolvable. Mutations can also be introduced by the copy algorithm itself; as it is an algorithm defined in the organism in standard Tierra instructions (and hence fully embedded), mutations in the algorithm during a copy will be inherited by the child. The initial copy algorithm is part of the 80-instruction ancestral creature documented in \textcite[app.C]{Ray1991}.

In Ray's words, ``...this approach involves engineering over the early history of life to design complex evolvable organisms, and then attempting to create conditions that will set off a spontaneous evolutionary process of increasing diversity and complexity of organisms'' \parencite[p.3]{Ray1991}. As Taylor criticises though, the problem with ``engineering over'' is that we don't understand the natural examples well enough to engineer them at all \parencite{Taylor2001}

Tierra has been the testbed for a number of other works. For example, \textcite{SugiuraSuzukiShioseEtAl2003} converted Tierra into a string manipulation systems, introducing a set of 140 regular-expression based rewriting rules where each rule encoded one or more of the original 32 Tierran instructions. The initial rewriting ruleset was manually generated by the experimenters, although details are unclear. Unlike in Tierra where the instruction set is fixed during a run, the rewriting ruleset for each organism itself could evolve through a separate genetic algorithm. This algorithm removed the least applied rules and inserted the same number of new rules generated by mutating (through duplication, removal and addition of operations) a selection of the most applied rules. Although results support the claim that the ruleset as well as the genome evolves, the use of a separate genetic algorithm for ruleset evolution artificially separates the ruleset from the genome--the feedback loop from genome back to ruleset is broken.

Tierra was also the starting point for \textcite{Taylor2001, Taylor:1999sc} to explore the creation of \gls{alife}, by adding cell regulation, parallel processes and energy modelling \textcite[p.4]{Taylor:1999sc}.

% Seed (proto-DNA) must itself be an indefinite heredity replicator {[}assumes that this is minimal starting point, rather than that this itself may evolve{]} \parencite{Taylor2001}
%Assume that early stages see A+B implicitly encoded in the environment, essentially as simpler than explicit mechanism, but little justification given beyond ``At the early stages of an evolutionary process, however, we would not expect there to be mechanisms for explicitly decoding the proto-DNA\ldots{}'' \parencite{Taylor2001}

\emph{Avida} \parencite{Ofria2004}, introduced in the summer of 1993, based on Tierra with improvements including better metering and measuring, and a 2D lattice or well-stirred reaction vessel topography (unlike the shared linear memory of Tierra, for example). ``In principle, the only assumption made about these self-replicating automata in the core Avida software is that their initial state can be described by a string of symbols (their genome) and that they autonomously produce offspring organisms. However, in practice our work has focused on automata with a simple von Neumann architecture that operate on an assembly-like language inspired by the Tierra system.''

Like Tierra, the automata engine in Avida is based on a Turing tape-like metaphor, with instruction, read, write, and flow control heads that can be moved forward and backwards through memory using relative rather than absolute addressing. Instructions are grouped into instruction sets, with the default set containing 26 instructions, and by definition every program is valid. Each organism runs on its own virtual automata; the only interaction between organisms is via resources in the shared environment and through competition for virtual machine CPU cycles based on ``merit'' or fitness. Direct Tierra-style interactions by insertion of code into another organism is not enabled by default, but is possible through configuration. Phenotypes take the form of computations (entities take in resources, perform computations that result in merit, and perhaps produce output or by-product resources): ``\ldots by inputting numbers from the environment, performing computations on those numbers, and outputting the results. The organisms receive a benefit for performing specific computations associated with resources'' \parencite{Ofria2004}. Interestingly, the resources in the environment are not the same as the elements of the organisms (instructions from an instruction set.) Avidan organisms are not fully-embedded in their environment.

New organisms are created asexually by the parent first allocating memory for a child. The parent's read-head is placed at the beginning of its code, the write-head placed at the start of the newly allocated memory and successive h-copy instructions copy the instruction from the read-head to the write-head and advance both. Either once all instructions have been copied (or perhaps before) h-divide splits the child from the parent (all instructions between read-head and write-head go to the child) and starts both executing in a clean state. Variation is introduced through mutations which can be introduced through either h-copy (the write-head writes a random instruction rather than the instruction at the read-head) or in h-divide (a single random instruction may be deleted or added from the child code). Both forms of mutation happen with a fixed probability set by the inventor: COPY\_MUT\_PROB, INS\_MUT\_PROB, and DEL\_MUT\_PROB for h-copy, and DIVIDE\_INS\_PROB, DIVIDE\_DEL\_PROB for h-divide. However, there is a second, evolvable source of mutations during replication--the replication process itself is embedded in the organism, as a set of instructions, and so changes to this algorithm during the copy will persist in the child. The self-replication algorithm is initially defined in the ancestral organism used to seed a run, and as documented in \textcite[A1.3]{Ofria2004} consists of 15 instructions.

Avida is extremely configurable, but provides little guidance or theoretical justification for any particular configuration. Indeed this variability allows it to function most usefully as a general testbed for experiments, \eg ``in one experiment we wanted to study a population that could not adapt, but that would nevertheless accumulate deleterious or neutral mutations through drift'' \parencite{Ofria2004}.

\emph{Amoeba-II} \parencite{Pargellis2001} shares similar features to Tierra and Avida in that it is an instruction-set based automata, but unlike in those systems, replication spontaneously emerges in Amoeba-II. The replication process requires four steps: ``register initiation, memory allocation, copying of the parent's instructions to the child (embryo), and division where the child is initiated as a cell on its own'' \parencite[p.69]{Pargellis2001}. Two specific instructions in the parent's genome are required at a minimum. The MALL command allocates memory, and a virtual CPU to the child, and DIVD performs the division. Register initiation and copying of the parent's instructions can be done in a variety of different ways, and a good deal of the interest of the Amoeba-II system lies in the evolvability of these mechanisms. Selection is by efficiency of replication, where faster and more efficient replicators replace less efficient ones in the population (as \textcite{Pargellis2001} says, ``Amoeba has only one task: replication'') and so there is selective pressure from the least efficient, but functioning, mechanisms in the direction of improved replication. To complement selection and inheritance, variability is provided by the DIVD instruction, which introduces a mix of instruction substitutions, deletions and insertions into the child's program at a fixed but low rate.

Inheritance in Amoeba-II is almost too effective. Fit organisms rapidly evolve into extremely rapid reproducers and out-replicate all other entities, leaving a mono-culture \parencite{Pargellis2001}.

%\begin{DRAFT}
%\textcite{Dittrich1998}:
%A simulation approach towards ``dynamic phenomena, especially on the emergence of prebiotic evolution'', based on an artificial chemistry.
%
%Introduction of \textless{}\emph{S},\emph{R},\emph{A}\textgreater{} classification scheme for artificial chemistry, elaborated in \parencite{Dittrich:2001zr}, where in this work \emph{S} are `` binary strings with a constant length of 32 bits'', \emph{R} are of the form $s1+s2 \rightarrow s3$, and \emph{A} ''simulates a well-stirred tank reactor with mass-action kinetics, which assures that the probability of a collision is proportional to the product of the concentration of the colliding objects'' (based on earlier work by Fontana and Kauffman.)
%
%A=``1. Select two objects s1,s2 from the soup randomly, without removing them. 2. If there exists a reaction s1 + s2 to s3 and the filter condition f (s1,s2,s3) holds, replace a randomly selected object of the soup by s3.'', s1 and s2 are not consumed, rather they act as catalysts. Chosen as this shown capable of hypercyclic organisation
%
%Asn automata reaction with a set of operations (six common logic operations, \eg OR, and nine computational instructions), represented as 4-bit sequences, to generate s3 from s1, s2. Automata is a deterministic FSA, running s1 on s2 to produce s3. As Dittrich observes ``The first noticeable property is that the structure of the product s3 is similar to its 'parents' s1; s2. This indicates that there is a correlation between s1, s2, and s3 that is a prerequisite for evolution.''
%
%Passive self-replicators ($s1 + s2 \rightarrow s2$) are relatively common (approx. 30\% of randomly generated strings), while active self-replicators ($s1 + s2 \rightarrow s1$) are very rare (around 0.004\%).
%\end{DRAFT}
\subsection{StringMol and GraphMol}

\emph{StringMol} \parencite{Hickinbotham2011} and the related \emph{GraphMol} \parencite{Nellis2012, Nellis2014} explore computational novelty through embodiment: ``Our aim is to improve novelty-generation algorithms by making their biological models richer.'' No measure is proposed for novelty--the author's state that they're not even sure it is possible, but an informal definition is used that sees novelty as the outcome of increasing embodiment \parencite[p.87]{Nellis2012}. 

The base elements in StringMol are single-character symbols, each representing a microcode instruction. These combine to form molecular microprograms (the strings of the name). The general arrangement feels very similar to that in Avida or Tierra, complete with a variety of points (instruction, read, write, and flow for iterations).

GraphMol instead is graph based. ``The world defined by GraphMol contains chemicals (represented as graphs) that bind to each other via multiple binding sites, and then run simple computer programs (encoded in the graphs) that modify the binding of these chemicals.''. No explicit rationale for graphs is presented, other than as a natural extension from StringMol given the stated importance of a rich binding mechanism. 

An underlying principle in both systems is that the mechanism of evolution must be itself evolvable; functions such as template copying must be embodied mechanisms in the world so that they can be affected by evolution, and so evolved.  Crucially therefore both StringMol and GraphMol have embodied template copying, where a ``replicase'' molecule can copy another following the algorithm in \cref{alg:stringmolgraphmol}.

\begin{algorithm}[ht]
$\text{i} \leftarrow \text{start(string B)}$\;
\While{$\text{i not at-end(string B)}$}{
	$\text{string A(i)} \leftarrow \text{char-copy(string B(i)))}$\;
    $\text{i} \leftarrow \text{next i}$\;
}
\caption[The algorithm for template copying used by StringMol and GraphMol]{The algorithm for template copying used by StringMol and GraphMol, taken from \textcite{Nellis2014}}
\label{alg:stringmolgraphmol}
\end{algorithm}

Each of the four functions in \cref{alg:stringmolgraphmol}--\emph{start}, \emph{at-end}, \emph{char-copy} and \emph{next}--can be either ``crisp'' (\ie perfect) or embodied (variable, and subject to evolution). As an example of an embodied function, StringMol's \emph{start} function uses pattern-matching to determine the binding region between the replicase and the other string where the replicase should begin copying. By changing the subsequences in either string the strength of bind can be varied, with a corresponding shift to the beginning of the copy region even though the pattern-matching algorithm itself does not change (it is a ``level-0'' component in the terminology of \textcite{BanzhafBaumgaertnerBeslonEtAl2016}). The same pattern-matching is used in StringMol's \emph{at-end} function to determine the end of the region to be copied.

StringMol includes an embodied \emph{start} and \emph{at-end}, crisp \emph{next}, and stochastic \emph{char-copy}; GraphMol has an embodied \emph{start}, \emph{at-end} and \emph{next}, with a crisp \emph{char-copy}. The primary difference between the two is opposite approaches to \emph{next} (``in order to investigate a method of embodying the copying process that would be completely different from Stringmol's.'' \parencite[p.145]{Nellis2012})

These mechanism differences (the choices of which functions are crisp and which are embodied) result in different outcomes for the overall system: ``Stringmol exhibits macro-mutation and two chemical copying; GraphMol exhibits two types of quasispecies, cooperative and parasitic. These two systems use the same domain (emergent evolution) and metamodel (machines copying strings), but different computational models.'' Other combinations would presumably show different behaviour again.

Despite the use of the term ``embodied'', those functions that aren't ``crisp'' are not in fact fully self-referential as the pattern-matching algorithm itself remains unaffected by evolution. There is also a disconnect between the properties of the targets of the matches and the functioning of the algorithm. The algorithm matches on the symbol, and is completely unaffected by the meaning or properties of those symbols, the underlying microcode. In this it differs significantly from a completely endogenous system such as a biological replicase, where the match or bind is actioned by the same chemical rules that construct and maintain the replicase from component atoms.

%Runtimes for GraphMol are in weeks to months.

\section{Discussion}

Of the forms of replicator examined here, only full informational replicators provide both the representational range and the evolutionary flexibility to meet our requirements. Exact replicators lack heredity as they cannot pass on variability to their descendents; variable replicators are capable of only a limited number of distinct states and so suffer from a restricted representational range; informational replicators based on shortcuts are evolutionarily restricted in that their inheritance/heredity mechanism is not evolvable. 

The informational replicator systems reviewed in this \namecref{previous-work} take different pathways towards full replication, but none fully satisfy all conditions. It is also clear that most previous works lack an overall theoretical background. This observation has been made many times from the earliest days of \gls{alife}--``simulations that are dependent on ad hoc and special-purpose rules and constraints for their mimicry cannot be used to support theories of life'' \parencite{Pattee1988}--through to more recent times: ``This weakness is not specific to Tierra, but is shared by most, if not all, of the other Tierra-like systems which have emerged over the last decade\ldots{}'' \parencite{Taylor2001}. It is this weakness that we seek to address in \cref{part2}.

%Although the advantages of a distinction between genome and phenome are discussed by many, including \parencite[section 7.2.3]{Taylor1999} and indirectly \textcite{VonNeumann1966}; there is no inherent dependency on this in \gls{ens}. Early evolution may have involved the inheritance of complete portions, or components, of the phenome before the advent of a distinct genome, while research into \gls{hgt} (\eg \textcite{Ochman2000,Pace:2008vi,Ragan2009}) has shown that not only was component transfer between species a major driver of early evolution, but a horizontal component-based mechanism continues to exist even in many of today's organisms that have a genome built from DNA. 
%
%So in discussing inheritance we can already distinguish between two forms--holistic and digital, based on the absence or presence of a genome (respectively)--and direction, either horizontal or vertical, where horizontal inheritance does not involve reproduction or replication (and so can cross between species), while vertical inheritance does (and cannot). Typically we refer to holistic inheritance as compositional inheritance, digital horizontal inheritance as \gls{hgt},  while digital vertical inheritance is the mechanism usually assumed when discussing \gls{ens}.

%\begin{DRAFT}
%The EvoEvo project \footnote{\url{http://evoevo.liris.cnrs.fr/about-evoevo-project/})} an Information and Communication Technologies initiative funded by the European Commission, begins at a higher level biological starting point (genotype-phenotype mappings). The project presupposes microbial evolution, ``at the level of genomes, biological networks and populations'', with a focus on four specific properties of a genotype-phenotype mapping--Variability, Robustness, Evolvability and Open-endedness. Later work is planned to remove the biological specificity to provide a framework for applying EvoEvo to ICT problems. Along with development of a model founded on the ``genotype-to-phenotype mapping and the fitness landscape'', the project states that make use of \emph{Aevol} \parencite{Knibbe:2006vn,Knibbe:2007kx} to model developmental processes in micro-organisms
%\end{DRAFT}

\chapter{Approach}\label{methods}

In this \namecref{methods} we explain the approach taken in the remainder of this work: a summary of the motivating research questions, a discussion of the appropriate methods ()experiments, models, and simulations) that may be deployed in an attempt to answer these questions, and finally a description of the specific methods to be used in \cref{part2,part3}.

\section{Research questions}\label{research-questions}

This work overall is motivated by the following two research questions:

\begin{enumerate}[label=RQ\arabic*:]
	\item If we assume that for pragmatic reasons inheritance must itself be evolved from simple beginnings, rather than designed in, then what is the mechanism by which this might be achieved?
	\item Can such a mechanism then be successfully demonstrated in an artificial system?
\end{enumerate}

How then are we to approach these research questions? The field of artificial life is synonymous with simulation \parencite[chap.2]{Aicardi2010}. In other forms of science however practitioners make use of a number of other tools, including experiments and mathematical models. Each method is well suited to some types of questions, and inappropriate for others. Is the presumption of simulation justified for our research questions?

\section{Experiments}\label{experiments}

In the biological sciences, experiments are clearly a source of empirical data (that is, derived from the subject of investigation.) This is not so clear in artificial systems as our subject is instead a program; a model or simulation. Whether this remains a source of empirical data rather depends on your interpretation of the epistemological meaning of a simulation or model, as discussed in the next subsection.

\section{Models, and simulations}\label{models}

%\quote{It is seldom the case in biology that a model is derived deductively from a more fundamental quantitative theory, with the possible exception of population genetics which has its foundations in evolutionary theory.}{\cite{Krakauer2011}}
Models can be ``useful stop-gaps'' towards a theory, by providing a testable body of data for experiments and predictions \parencite{Krakauer2011}, and may be constructed either bottom-up or top-down, increasing in specificity by the successive application of constraints \parencite{Krakauer2011}, and fall into two main groups, although there are many types and forms (for example, eleven types in ecology according to \parencite{Jorgensen2008}):

Mathematical models, based on reduction, abstraction and simplifying assumptions (\eg Fisher's famous equation describing the changes in allele distribution under selection assumes independent genes--although extending this to realistic cases remains an open problem \parencite{Schuster2011}). Emergence and dynamic behaviours are important, and yet they are hard to capture with mathematical models relying on reduction \parencite{Ferrer:2008hv}

Simulations by contrast are holistic and bottom-up, and encompass variability so that the diversity of the results is closer to that seen in real systems \parencite{Ferrer:2008hv}. They have a unique ability to explore systems encompassing emergence and self-organisation. Biology, and by plausible extension, biologically-based systems, stand alone in the pervasiveness of emergence \parencite{Bersini:2006ve}, and the interconnection of levels of analysis, \eg behaviour can influence gene expression, and genes can affect behaviour \parencite{Krakauer2011}.

\section{The epistemological nature of simulations}\label{the-epistemological-nature-of-simulations}. Simulations seem to fall somewhere in between thought experiments or abstract models, and experiments. They are also relatively novel; common use has only come with increased access to digital computers. Consequently the nature of simulation--what can be claimed as a result of simulation, and what role may be played legitimately by simulation in scientific discovery--is a hot topic for philosophers of science. As might not be unexpected, two opposing positions have been commonly taken, plus a synthesis that claims the middle ground.

\emph{Simulations are only programs}. That is, a computational means to solve analytically intractable equations \parencite[31]{Winsberg2010}, ``...a high-speed generator of the consequences that some theory assigns various antecedent conditions'' \parencite[quoting from Dennett]{Eldridge}, producing nothing new (just consequences of what is ``fed in''\parencite{DiPaolo2000}). In this sense, simulations are not empirical.

In this view simulations are aimed at answering specific questions, or analysing particular scenarios. The more accurate the simulation however, and hence the more valuable the results, the harder it is to generalise to other cases. It is hard to understand the behaviour of complicated simulations, and the causes of particular behaviours of interest may be unclear if there are many variables in play. For this reason \textcite{MaynardSmith1974} prefers the use of simple models, designed to illuminate the ``causes of differences of behaviour between different species or systems'' rather than ``assertions which are true of all systems or of all species.''

\emph{Simulations are instance of the thing itself}. That is, the simulation is not a shadow but as real as the thing itself; the Animats\footnote{\url{https://en.wikipedia.org/wiki/Animat}} are actually alive, and therefore instances of biology.\footnote{And this way leads us to the claims of Strong \gls{alife}--the simulation is actually alive.} The simulation is a stand-in for the real world, and you can perform experiments on it as would any other system \parencite[31]{Winsberg2010}. Simulations certainly have elements of uncertainty and error, like experiments. As \textcite{Adami2002} says, describing Avida, ``These organisms, because they are defined by the sequence of instructions that constitute their genome, are not simulated. They are physically present in the computer's memory and live there. The world to which these creatures adapt, on the other hand, is simulated\ldots''

\emph{Simulations are model builders}. As identified by \textcite[p.31]{Winsberg2010}; or an ``Opaque Thought Experiment" \parencite{DiPaolo2000}. In one sense, simulation is like theory as it is about ``manipulating equations'' and ``developing ideas'', but also like experiments as it concerns ``fiddling with machines'', ``trying ideas out'' and ``watching to see what happens.'' \parencite{Winsberg2010}

In this view, simulation is a form of Kuhn's theory articulation or ``model building''--making principles apply to local, concrete systems in the real world: ``Simulation is a process of knowledge creation'' \parencite[6]{Winsberg2010}. Following this third way, simulations might be seen as a source of new hypotheses \parencite{Eldridge}. Similarly, Taylor as summarised in \textcite{Webb2009}, argues for ``pure exploration'' or ``exploratory tools'' that do not need justification, and that may be used to generate ``new questions to ask, new terms to employ, or different models to construct.''

\section{Methods}\label{approach}

\epigraph{%
An important ``Platonic'' conception is that abstraction is a mental process by which properties are thought of as entities distinct from the concrete objects in which they are instantiated. On an alternative, Aristotelian conception, abstraction is the mental process of subtracting certain accidental properties from concrete objects so as to regard objects in a manner closer to their essential natures.}%
{\textsc{\\\textcite{Griesemer2005}}}

To investigate our first and second research questions, we turn to simulation and experiment. Simulation is a natural fit for the exploration of emergent systems where we expect complex behaviour from simple rules, while experimental tests are the strongest method we have to examine specific claims. As explored earlier in this \namecref{methods}, logical argument or theorem-proving is difficult to apply to model-based systems; thought experiments lack the strength we require, while experimentation is both feasible and, assuming correct design, rigorous. 

The combination of a simulation to provide a testable system and experiments to test specific predictions driven by a hypothesis provides a methodologically sound form in which to pursue these two research questions. If a simulation that accurately represents the system in the hypothesis behaves in a way that matches our predictions, the hypothesis is supported. On the other hand, if the behaviour doesn't align with the predictions, our hypothesis will be rejected.

Now to specifics. The approach taken in both \cref{part2,part3} of this work is driven by the first and second research questions, respectively, and follows this common process:
\begin{enumerate}
	\item Relevant previous work is used to identify areas where the research question is well understood, and those areas where further work would be beneficial.
	\item We construct a simulation model that attempts to capture our understanding of the problem for the areas where further work is needed.
	\item Based the context formed by the research question and the previous work, we form a hypothesis of how the simulation should function if our understanding is correct.
	\item We then proceed to test this hypothesis by experiment using the simulation.
\end{enumerate}

Each simulation model is parameterised. Parameters are elements in the simulation model that can take different possible values, where the different values may (or may not) lead to quite distinct behaviour of the simulation. The purpose is two-fold: first, to allow for the investigation of a range of models simply by changing parameter values (rather than changing entire models) so as to broaden the \emph{scope} and hence the applicability of the results, and first, to permit us to test the \emph{sensitivity} of the simulation to different parameters overall. The combination of these two allows us to robustly justify the scope and strength of any claims that arise from the experiments.

Parameters are elements of the simulation. Each in the most general case may take a wide, perhaps indefinitely large, set of values, and so for practicality in experimentation, we need a way to limit the size of the parameter space defined by the combination of every value of every parameter. We do this by first testing the response of the model to each parameter and identifying those that do not make a statistically significant difference; the set of these parameters, those to which the model is insensitive, allows us to establish the scope over which all claims will hold. Those to which the model is on the other hand sensitive, or responsive, are tested separately in all later experiments; any claim must be made conditional on the particular level of each of these parameters. This process particularly applies to \cref{part2} where the number and range of parameters in the simulation model is relatively large; the specific method for identifying sensitive and insensitive parameters and the outcomes when applied to the simulation model are described in detail in \cref{reducing-the-parameter-space-for-the-experiments}.

Second, we use statistical ``design of experiments'' (\eg \cite{Montgomery2009}) methods to simplify the number of separate experiments needed. There are many approaches to this, but they mostly fall into two standard groups. First are response-surface methods which sample from the parameter space in a particular fashion to effectively construct an analysable function, or response-surface, from parameter values to response-variable values that approximates to some degree the behaviour of the model. The emphasis is on the shape of the response-surface; the parameter values are randomly chosen.

The usual alternative is some variant of a factorial design, where each parameter of interest is represented by a factor taking some small number of values, or levels (two levels being most common) and the analysis model constructed from runs that systematically work through a series of combinations of factors at different levels. The emphasis here is on the response given particular factor, and hence parameter, values. A further form of this is a fractional-factorial design, where we in effect take a well-defined sample of factor levels to further reduce the number of level combinations while still maintaining an acceptable level of experimental strength.

In both \cref{part2,part3} the relatively small number of representative levels required, combined with the reasonable speed of the simulation, is such that we can remain with the simpler full-factorial design.

Note that there are some differences between the design of experiments in the physical world and in simulations, with the most significant being the sources and understanding of experimental errors. In simulation, experimental runs are exactly reproduceable, absent any dependency on factors external to the simulation. Variation is explicitly introduced usually through a random number generator, which can be seeded to produce the same sequence of numbers again and again. This means that the practice in real-world experiments of ``blocking'' to control external variation is not required in simulation experiments. However, \gls{replicate}s where the same combination of factor values is run several times each with a different random seed value, remains valuable, but in this case less to control for experimental error and more to record the variation across a series of runs and the sensitivity of the model to parameter settings.
