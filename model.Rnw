<<setup, include=FALSE>>=
library(knitr)
library(cowplot) # styling of plots, extension of ggplot2
library(gridExtra) # grid layouts for ggplot2
library(lattice) # needed for bwplot etc
library(english) # convert numbers to words
library(xtable) # required for print.xtable.bootabs function
opts_chunk$set(fig.path='generated_figures/')
knit_hooks$set(pdfcrop = hook_pdfcrop)

load.results.simple <- function(t) {
colClasses <- c("numeric","numeric","integer","integer","numeric","numeric","factor","factor","factor","factor","factor","factor","factor","factor")
read.csv(t, colClasses=colClasses)
}

load.results <- function(t) {
colClasses <- c("integer","integer","numeric","numeric","integer","integer","numeric","numeric","factor","factor","factor","factor","factor","factor","factor","factor")
df <- read.csv(t, colClasses=colClasses)
df$ecf <- factor(df$ecf,levels=c(0,1,5,10)) # place into natural order
df
}
@

\chapter{Inheritance under Varying Environments}\label{models-of-inheritance}

Heredity describes the similarity of an offspring to its parent, and depending on the context, can refer to the correlation for either a single property or to a group of properties shared between offspring and parent. There is a subtle difference between heredity, heritability and inheritance; inheritance is a process, while heredity is the relationship between entities linked by inheritance, and can be measured by heritability. As noted by \textcite{Griesemer2005}, ``[o]ne must clearly distinguish between heredity (a relation), heritability (a capacity), and inheritance (a process).''

Heredity is related to variation: low variation implies high heredity and vice versa. For the single property (or in biology, trait) case, when the value for a parent's property and an offspring's property are identical we say we have complete or full inheritance and the heritability of the trait or property is $1$. Conversely, if there is no relationship there is no heritability and the entities are effectively unrelated. We can extend the measure to a group of properties simply by averaging the degree of similarity for each property across all properties.

\section{Evolution by Natural Selection}\label{evolution-by-natural-selection}

As has been said before, ``[w]e take it as given that biology instantiates ENS'' \parencite{Watson2012}, but that doesn't mean that the algorithm of biology is in fact \gls{ens}. Adaptation in biology appears to precede Natural Selection, reinforcing that adaptation is possible without \gls{ens} \parencite{Watson2010}, and other forms of evolution altogether have been proposed in artificial systems (\eg the compositional evolution of \textcite{Arthur2009}), and on the boundaries of \gls{ens} in the domain of living systems (\eg composomal inheritance and \gls{hgt}). These variants are examined later, but first we concentrate on canonical \gls{ens}.

\Textcite{Godfrey-Smith2007} examined a number of summaries of \gls{ens} taken from the most influential items in the literature. The purposes of the summaries varied, but have interest for us ``as attempts to capture some core principles of evolutionary theory in a highly concise way.'' Incidentally, as an illustration of the difficulty of definitions, although the usual aim for a summary is to ``give conditions that are sufficient ceteris paribus for a certain kind of change occurring'', \textcite{Godfrey-Smith2007} notes that the scope of most summaries is somewhat ambiguous. They can be read as either being discriminatory--``this process is ENS''-- or alteratively in providing conditions that will result in \gls{ens} when it is assumed that the meaning of \gls{ens} is clear. In other words, these are alternative \emph{constitutive} or \emph{causal} readings; in the example of \textcite{Godfrey-Smith2007}, ``becoming pregnant\ldots{}{[}versus{]} being pregnant''.

The main examples selected by \textcite{Godfrey-Smith2007} are:

\begin{enumerate}
	\item ``Owing to this struggle for life, any variation, however slight and from whatever cause proceeding, if it be in any degree profitable to an individual of any species, in its infinitely complex relations to other organic beings and to external nature, will tend to the preservation of that individual, and will generally be inherited by its offspring. The offspring, also, will thus have a better chance of surviving, for, of the many individuals of any species which are periodically born, but a small number can survive. I have called this principle, by which each slight variation, if useful, is preserved, by the term of Natural Selection, in order to mark its relation to man's power of selection.'' \parencite{Darwin1859}
	\item ``If there is a population of entities with multiplication, variation and heredity, and if some of the variations alter the probability of multiplying, then the population will evolve. Further, it will evolve so that the entities come to have adaptations....'' (Maynard-Smith, in \textcite{Griesemer2001})
	\item ``Any entities in nature that have variation, reproduction and heritability may evolve'' \parencite{Lewontin:1970mc} and ``1. Different individuals in a population have different morphologies, physiologies, and behaviors (phenotypic variation). 2. Different phenotypes have different rates of survival and reproduction in different environments (differential fitness). 3. There is a correlation between parents and offspring in the contribution of each to future generations (fitness is heritable).'' \parencite{Lewontin:1970mc}
	\item ``...evolution will occur whenever and wherever three conditions are met: replication, variation (mutation), and differential fitness (competition)'' \parencite[quoting Daniel Dennett]{Ofria2004}
\end{enumerate}

\Textcite{Godfrey-Smith2007} concludes that the core requirement for \gls{ens} is some ``combination of variation, heredity, and fitness differences'', although he identified a number of differences between the summaries. For example, the most commonly cited summary is \textcite{Lewontin:1970mc}, but unusually that formulation states that ``fitness is heritable'' whereas typically phenotypic heredity (as appears in Lewontin's later 1980 summary) is stated as sufficient for a trait to evolve. 

These differences are also discussed in \textcite{Griesemer2001}, in particular with reference to the variations between the concept of inheritance in Darwin which includes both heritability (a capacity) and inheritance (a process carrying the capacity); Lewontin, which stresses the heritability while assuming inheritance, and Maynard Smith's multiplication which is actually about inheritance; his heredity is both \parencite{Griesemer2001}.

Quite apart from these differences in interpretation, the major theoretical difficulty in a literal application of \gls{ens} to artificial systems is captured succinctly by \textcite{Griesemer2005}: ``Darwin's theory of evolution by natural selection is restricted in scope. One sense in which it is restricted is that it refers to organisms.'' Organisms are not defined, but the context and scope is clearly biological. 

Although \gls{ens} is usually only discussed within the context of modern day biology, as we've seen when examining the definition of \gls{ens} there is nothing exclusively biological about the standard formulations. By abstracting the concepts of variation, selection, and in particular inheritance or heredity, a generalised form of \gls{ens} can be developed that goes beyond the usual biological readings. 

A specific example lies in the transition from non-living to living things. One of the main problems (of several) in extending \gls{ens} to prebiotic entities is that the meaning of the foundational elements--such as variation, selection, heredity, multiplication--are generally couched in biological terminology. For example, heredity is often discussed in terms of alleles, or traits, rather than in more general ways. However, there must have been some point at which the prebiotic processes transitioned to \gls{ens}, and it is clear that this transition did not happen abruptly in a population of fully-fledged modern organisms. The pathway to \gls{ens}, the evolution of evolution itself, is one of the main open problems in origins-of-life research, and it involves an extension of those foundational elements back into the prebiotic world.

This example of the transition to life is an example of \emph{extending} \gls{ens}; two further examples are now presented that are \emph{alternatives} to \gls{ens}: DaisyWorld, or regulation without selection, and evolution in an entirely non-living domain--the evolution of technology. 

\Textcite{Arthur2009} describes a mechanism for the evolution of technology, where evolution is used in the sense of ``all objects of some class are related by ties of common descent from the collection of earlier objects''. Evolution in technology occurs by using earlier technologies as building blocks in the composition of new technologies, and these new technologies then become building blocks for use in later technologies, and so on. Arthur calls this ``combinatorial evolution.'' But what is the starting point? How is this regression grounded? Arthur proposes that the capture and harnessing of natural phenomena starts each lineage and provides new raw components for inclusion in later technologies. \Textcite{Bourrat2015} comments that distributive evolution (where only the distribution of elements changes, as result of selection or drift) cannot result in novelties: Arthur's answer is that novelty comes from this incorporation of new phenomena from the source, the natural world.

Evolution is related to innovation: in fact, Arthur claims that by understanding the mechanism by which technologies evolve we will understand how innovations arise. In other words, innovations arise as the result of an evolutionary process, rather than \textit{de novo} from the brain of a designer. Darwinian evolution, or natural selection, is not appropriate for technology. Arthur quotes from Samuel Butler's essay ``Darwin Among the Machines'' to illustrate the impossibility of the slavish adoption of biological models: ``{[}t{]}here is nothing which our infatuated race would desire more than to see a fertile union between two steam engines\ldots{}''.

The first obstacle to a more general scope is the existence of innovations such as the jet engine, laser, railroad locomotive, or QuickSort computer algorithm (to name Arthur's examples.) Innovations seem to appear without obvious parentage; they do not appear to be the result of gradual changes or adaptations to earlier technologies. Arthur's answer is to look inside the innovation and to recognise that each is made up of recognisable components or modules; the key lies in the nature of heredity in technology. Technologies are formed by combining modules of earlier technologies. These groupings start as loose assemblages to meet some new function, but over time become fixed into a standard unit (for example, the change in DNA amplification mechanisms from assemblages of laboratory equipment to standard off-the-shelf products.)

However, even Arthur in his rejection of ``Darwinian evolution'', describes a process that still relies on selection, variation and inheritance. This is not the case in our final example, DaisyWorld.

\Textcite{LovelockMargulis2011} propose DaisyWorld as a example of an alternative to selection for regulation, based on two feedback loops. \Textcite{Saunders1994} explains how regulation can emerge in DaisyWorld without selection--the planet's temperature is adjusted to meet the conditions for maximum growth of the daisies without the daisies adapting to the planet. ``As a result, regulation, one of the most fundamental and necessary properties of organisms, appears without being selected for. What is more, it appears as a property not of the daisies, on which natural selection may have acted, but of the planet, on which, as Dawkins rightly points out, it could not'' \parencite{Saunders1994}.

The fundamental insight in DaisyWorld is that individuals modify environments (that is, niche construction): the daisies adapt the planet (specifically the temperature for maximum growth) to suit themselves, rather than adapting themselves to the planet; and in fact there's little benefit to adaptation by the daisies to the planetary conditions. As \textcite{Saunders1994} states, ``the ability to withstand a greater variability is not the result of Darwinian adaptation. On the contrary, it exists because of the absence of Darwinian adaptation.''

\section{Previous work}\label{previous-work-p2}

Over the years a significant body of literature has accumulated on models of biological inheritance. While standard population genetics describes the dynamics over time of genotype frequencies, it remains deeply rooted in the biology of genotypes and alleles. 

Different forms of inheritance, such as acquired characteristics (Lamarckian inheritance), have been compared and contrasted to the canonical non-acquired form in \gls{ens} by a number of authors (including for example \textcite{Jablonka1995, Paenke:2007ie,Gaucherel2012}), going back to the competing models for inheritance of Darwin and Lamarck. 

Early hypotheses in the metabolism-first or replicator-first debate led to the recognition of the significance of error threshold rates for mutations during copying (\cite{Eigen1971} in biochemistry and Muller's Ratchet \parencite{Muller1964} in population genetics, compared in \textcite{Wagner1993}), and later investigation of the interaction between inheritance (of genotypes, when non-acquired), selection (on the phenotype), and development (linking genotype to phenotype) inspired the theory of neutral landscapes \parencite{Kimura:1968uq}. 

By comparison, little explicit modelling of inheritance has been done in \gls{alife}. Most relevant work has already been reviewed in \cref{previous-work} where it generally forms one element of a wider investigation into \gls{oe}. In relation to \gls{ea}, many models for inheritance from variation and recombination have been proposed. These however are less relevant to our needs as they rely on exogenous, or external, predefined mechanisms rather than emerging from the properties of the genotypes. 

\Textcite{Paixao2015} sought common principles between population genetics and evolutionary computation with the goal of unifying the two fields, starting from a broad description of evolutionary processes as a ``population undergoing changes over time based on some set of transformations''. A transformation can be decomposed into a collection of (stochastic) operators, with an operator representing a probability distribution of potential outcomes; some operators act on phenotypes, others on genotypes. Various operators drawn from evolutionary computing are defined: for selection (uniform, proportional, tournament, truncation, cut, replace); variation from mutation (uniform, single-point), and variation from recombination (one-point crossover, k-point crossover, uniform crossover, unbiased variation).

Evolution is then a trajectory through a space of distributions; it can therefore be seen both as a sequence of population transformations and distribution transformations. When considered as a series of distribution transformations, \textcite{Paixao2015} sees a correspondence with the Estimation Distribution Algorithms (EDA): ``In an Estimation Distribution Algorithm (EDA), the algorithm tries to determine the distribution of the solution features, e.g. probability of having a 1-bit at a particular position, at the optimum. Some EDAs can be regarded abstractions of evolutionary processes: instead of generating new solutions through variation and then selecting from these, EDAs use a more direct approach to refine the underlying probability distribution. The perspective of updating a probability distribution is similar to the Wright--Fisher model.''

\Textcite{Paixao2015} concludes by demonstrating how existing ``classical models in theoretical population genetics and in the theory of evolutionary computation'' can be mapped into the framework of classified and categorised operators. Most of the various population genetics models can be represented, with the exception of some topic-specific \gls{ea} models, while genetic programming models are omitted for reasons of balance between ``simplicity and inclusiveness.'' However, \textcite{Paixao2015} is of limited relevance for this work as it is in effect a constitutive framework, providing a series of tests to classify existing models based on a general meta-model of evolution, rather than a causative model which is where our interest lies. Variation, in the form of reproduction and mutation, is only one element of the framework, and \textcite{Paixao2015} does not specifically address the emergence of heredity from variation.

More specific models of inheritance and heredity can be found in three works comparing the adaptive value of acquired and non-acquired characteristics. In the first two works, \textcite{Jablonka1995,Paenke:2007ie}, the comparison is made with respect to an environment that alternates between two states, $E_0$ and $E_1$. Each environment is associated with a corresponding adapted phenotype $P_0$ and $P_1$, respectively.  In both works variation only affects the ratio of each phenotype in the population (directly in \textcite{Jablonka1995}, indirectly via a ``predisposition'' or tendency in \textcite{Paenke:2007ie}); no new phenotypes are created, and as inheritance is modelled only at a population level, that is without reproduction, we cannot use these models to investigate the emergence of heredity.

Inheritance however is modelled in \textcite{Gaucherel2012}, the third work, in relationship to individuals. Two models are presented, the first and simplest describing a non-spatial scenario conceptually similar to that in \textcite{Jablonka1995} and \textcite{Paenke:2007ie}, while the second examines a spatial world based on DaisyWorld \parencite{LovelockMargulis2011}. Focussing on the first and most relevant of the two models, reproduction is the middle of three repeated stages--first ``annihilation'' where the population size is adjusted to some practical level, then reproduction, and finally development.

In the second stage (reproduction), individuals, represented by a single trait, or phenotype, value, are chosen for reproduction with some probability (based on the trait value), and, to model mutation, the child given a trait value that slightly varies from the parent's value: $trait_{child} = trait_{parent} + \delta$, where $\delta$ is described as taken from a uniform distribution of given range around the parent's trait value. \footnote{although this appears to be an error;  $\delta$ should instead be centred on $0$.})

The most relevant previous work is that of \textcite{Bourrat2015} who, in the course of examining the difference between evolution, natural selection and \gls{ens}, models the emergence of heredity in unchanging environments. \Textcite{Bourrat2015} demonstrates that imperfect inheritance is not compatible with \gls{ens} using an argument by contradiction \parencite[p.96]{Bourrat2015}: he lists the three conditions for a population to evolve solely by \gls{ens}, and then continues on to show that at least one of those conditions is incompatible with imperfect inheritance (as it happens, no production of new variants). The context is unequivocally related to biology; his examples involve genes, traits, phenotypes and drift.

\subsection{Bourrat's model of biased inheritance}

The six applicable models in \textcite[chap.3]{Bourrat2015} are designed to explore the implications of bias on inheritance, where \emph{unbiased} means a trait is uncorrelated between child and parent, in practice it means that trait is taken as a random choice between some lower and upper bounds \parencite[p.153]{Bourrat2015}. \emph{Biased} inheritance is naturally the opposite: there is some correlation between parent and child values for a trait, and so some prediction of traits is possible--a parent can ``pass it on''. Note that \textcite[p.173]{Bourrat2015} expressly notes that his ``biased inheritance'' is not the ``transmission bias'' of the second term in the Price equation: ``The bias in ‘biased inheritance’ is in reference to the type of the parent(s) (biased toward the type of the parent), while the bias in ‘transmission bias’ refers to a departure from an event of perfect transmission.'' Bourrat unfortunately doesn't formalise his model descriptions; instead there is a reference \parencite[p.129]{Bourrat2015} to a NETLOGO 5.02 implementation, and textual narrative descriptions of each model and the results.

%\Textcite{Bourrat2015}
%\begin{itemize}
%\item Persistors - unable to reproduce, selection only in ``weak'' sense of granite grains for hardness
%\item Procreators can reproduce but without inheritance of any property (including ability to procreate) except ``fact of coming into existence and membership of that class'' (class is class of parents defined by ``those properties that do not vary in the population''...{[}acknowledged as loose, but has benefit that no varying traits included{]}) p137. Procreator's offspring is persistors
%\item Minimal reproducers - indefinite procreation - where procreation can be transmitted from parent to offspring (with some low degree of fidelity)
%\item Unreliable reproducers (low bias for ability to procreate- ability to procreate randomly chosen between 0 and parent's ability), reliable reproducers (high bias - ability to procreate is same as parent's ability)
%\item Replicator - all traits (including procreation) can be inherited
%\end{itemize}

Model 1 begins with a population of 5000 ``persistors'' (that is, entities without the ability to reproduce), each of which has a survival rate (viability or the likelihood of surviving at each time step) between 0 and 0.99. Unsurprisingly, all eventually die. Model 2 introduces a single ``procreator'', capable of reproduction with both survival and fertility rates (offspring per unit time), into the population of persistors. The traits of the offspring of the procreator are uncorrelated (that is, unbiased inheritance) to those of the procreator, and the model now consists of \emph{selection} $\rightarrow$ \emph{reproduction} $\rightarrow$ \emph{check-for-overcrowding} \parencite[p.141]{Bourrat2015} at each timestep. Again, all entities eventually die.

Model 3 begins to get interesting: \textcite{Bourrat2015} replaces the procreator by a ``minimal reproducer'' which differs from a procreator in that the ability to procreate is itself a heritable trait, although as ``minimal'' it is an unbiased trait. As such, the offspring of the minimal reproducer may be either minimal reproducers or persistors without the ability to reproduce. Now the population size drops then increases to maximum size with about 10\% of the population being minimal reproducers. However, the proportion of high fitness (that is, high viability) entities doesn't increase beyond about 0.05, so there is no cumulative adaptation.

Biased (in fact, perfect) inheritance of viability is introduced in Model 4; the offspring inherit the viability of their parent. The proportion of high fitness entities rapidly approaches the upper limit of $1.0$, as expected as high viability entities live longer and so produce more offspring while low viability entities die sooner and so produce less-- fertility random, but viability is heritable.

The most significant model is Model 5 which adds a variable ability to procreate to Model 4, while viability remains inherited with perfect fidelity from the parent. The variation is provided by the addition of a \emph{mutation} stage so that the model now consists of these stages at each timestep: \emph{mutation} $\rightarrow$ \emph{selection} $\rightarrow$ \emph{reproduction} $\rightarrow$ \emph{check-for-overcrowding} \parencite[p.153]{Bourrat2015}. At each mutation stage, there can be an increase or decrease in both the ability to transmit the ability to procreate, and in degree of bias (that is, relationship to parent's ability) in the ability to procreate. The first defines the proportion of offspring of the parent who are themselves able to procreate; a low trait value for the parent means a low proportion of siblings can procreate, while bias represents the correlation between the parent and offspring's abilities to transmit procreation--low bias means the offspring's ability is only weakly correlated with parents ability. The initial population contains entities with viability in the full range $[0,1)$, an ability to procreate in $[0,0.2)$ and initial bias of $0$. Both the ability to procreate and the bias \emph{increase} over time in the population towards the upper limit of $1.0$. The model is asymmetric with respect to the change of inheritance of ability to procreate: reductions lead to extinction of a line, while increases lead to increased population. Bourrat's conclusion is that an initial population of unreliable reproducers (a low proportion of procreating offspring, no bias) will evolve into one of reliable reproducers--that is, inheritance can emerge.

This is extended in the final Model 6, where Bourrat demonstrates the combination of inheritance of procreation with that of another trait, viability. The model begins with a population produced by the end of Model 5 - a set of entities that can reliably transmit the ability to procreate to their offspring. Using the same structure for trait inheritance as in Model 5, Model 6 shows that inheritance of viability or fitness can also emerge. In total then, the entities at the conclusion of Model 6 have full inheritance of multiple traits.

\subsection{Limitations to the model presented by Bourrat}

There are some limitations however in these otherwise insightful models. First, heredity and fitness (viability) are treated as independent traits. But the mechanism for heredity is the thing that copies the information that generates an offspring's traits, so in practice they are not independent.

Second, while Model 5 has perfect inheritance on viability and demonstrates emergent inheritance on procreation, Model 6 shows emergent inheritance on viability while beginning with perfect inheritance on procreation (as it begins with a shortcut population of entities assumed to have been produced by Model 5.) Thus Bourrat does not show in either model the simultaneous emergence of inheritance of both procreation and viability.

Finally, the model assumes that the problem learnt by evolution is amenable to perfect understanding, and that there is one and only one optimal solution. This is a corollary of the model design where fitness is absolute and unchanging--as fitness represents an implicit relationship between an entity and its environment, then in Bourrat's model this relationship is also fixed and unchanging. Evolution is omniscient with full visibility into the world. However, in the real world and in the artificial domains of interest, the relationship between entity and environment is less sure. The environment itself may either change, or be uncertain. Under these conditions it is unlikely that values of $1.0$ would be possible, or indeed helpful, as a perfect bias value effectively is removing any source of variation from the population. This is unexplored by Bourrat.

The work of \textcite{Bourrat2015} therefore has the following limitations:
\begin{itemize}
	\item Heredity and fitness are unconnected in the model.
	\item The simultaneous emergence of inheritance of procreation and viability is not addressed.
	\item Most importantly, the effect of changing environments on inheritance is unaddressed.
\end{itemize}

\subsection{Hypothesis}\label{h2}

\Textcite{Bourrat2015} argues that heredity may in fact be a product of evolution rather than a precursor, or in other words, that the process of inheritance emerges from the action of selection and variation upon a population (\cref{previous-work-p2}.) In other words, \emph{variation and selection are sufficient for inheritance}.

The degree of variation between generations is important; if there is no correlation it effectively means evolution is operating as an unguided, random, search while complete correlation means that each generation is a copy of the previous one, and there are no novelties.

Our intuition is that the average heritability of the population will be inversely related to the degree of environmental change. More specifically, if we consider evolution to be a means by which a population learns how to adapt to an environment, the degree of environmental change can be described as the degree to which it's possible to learn the environment. We intuit that this is related to predictability, and also to complexity. 

This seems a reasonable supposition. \textcite{Adami2002} recasts population fitness in terms of complexity, specifically his physical complexity measure: ``It is probably more appropriate to say that evolution increases the amount of information a population harbours about its niche (and therefore, its physical complexity)'' \textcite{Adami2002}. \textcite{Prokopenko2009} discusses the information-theoretic view of the benefits of complexity.

The initial hypothesis of \textcite{Bourrat2015} assumes a perfectly predictable environment that can be exactly ``learned'' by an evolutionary algorithm. If we assume that learnabilty is related to predictability, then we can restate and expand the hypothesis in terms of predictability:

\begin{hypothesis}
Variation and selection are sufficient for inheritance, where the degree and variance of inheritance is proportional to the predictability of the environment.
\end{hypothesis}\label{hypothesis-2} 


\section{Predictions}\label{predictions}

In a stable or unchanging environment our prediction is that:

\begin{enumerate}
\item Average inheritance will tend towards perfect inheritance, confirming a result of \textcite{Bourrat2015}.
\item The population variance of inheritance will decrease more than would be expected by chance alone.
\end{enumerate}

Our predictions for changing environments are:

\begin{enumerate}
	\item Fidelity is proportional to the predictability of the environment, that is at a minimum in conditions of maximum unpredictability, and at a maximum in stable conditions.
	\item The higher the variability in the environment, the higher the $\sigma_{heritability}$. As a corollary, $\sigma_{heritability}$ under changing conditions will be greater than that under stable conditions.
\end{enumerate}
\subsection{Simulation model of variation and inheritance}\label{base-model}

We now introduce a general model of the relationship between \emph{Variation}, \emph{Selection} and \emph{Inheritance} in a population of evolving abstract entities (\cref{base-model-algorithm}) where the key elements, \emph{heritability} and \emph{fitness}, are represented as explicit parameters:

\begin{enumerate}
	\item \emph{Fidelity} is the likelihood that a child's value for a property will resemble that of its parent. The range is $[0,1]$, where a value of $0$ means that the value for a child's property has no inherent relationship to its parent's value, while $heritability = 1$ means the child's value will be identical to the parent's.
	\item \emph{Fitness} represents the probability that an entity will survive and possibly also reproduce, and has the usual range for a probability of $[0,1]$.
\end{enumerate}

This strategy of making otherwise derived variables explicit is also followed by Bourrat, who describes it as ``explaining variables which have previously been taken for granted in a model (such as reproduction and inheritance), by reference to other, more fundamental variables present in the model'' \parencite[p.129]{Bourrat2015}. Our model owes a direct debt to Bourrat in the representation of heritability as an explicit parameter (related to the two parameters ``heredity of the ability to procreate'' and ''transmission of the ability to procreate'' in \textcite{Bourrat2015}). 

In other respects, however, it resembles a standard \gls{ea}. The model consists of a population of abstract entities, and two simple functions--\emph{Selection} and \emph{Variation}--that each transform the population. Other elements of the model are as follows:

\begin{enumerate}
	\item \emph{Population} The population consists of $n$ entities, where $\lvert n\rvert\geq 0$.
	\item Each \emph{entity} is fully described by two properties -- \emph{fitness} and \emph{heritability}.
	\item \emph{Selection} forms a new population by selecting entities from the current population, as defined in \cref{model-functions}. The probability of an element being included in the new population ($p_{selection}$ in the algorithm) may be either a fixed value, or equal to its \emph{fitness}. 
	\item The new population created by \emph{Variation} consists of a number of new entities (``children'') for each entity (``parent'') in the current population (see \cref{model-functions}). Each entity has children with probability $p_{reproduction}$, and if it does, the number of children it has is some random number between $0$ and $n_{children}$. The properties of each child are related to the properties of its parent by a mapping, represented in the algorithm by the function $Derive$, which maps the parent value to a value in a range with an expected value equal to the parent's value and some degree of correlation captured by the value of the function $Range$.
\end{enumerate}

The model is parameterised to make it easy to describe different specific models within this general structure; these parameters are defined in \cref{tbl:parameter_definitions}.

\begin{table}
	\scriptsize
	\begin{center}
	\caption{Definitions for all parameters of the model.}\label{tbl:parameter_definitions}
	\begin{tabular}{@{}llp{8cm}@{}}
		\toprule
		Parameter          	& Value                                	& Description\\
		\midrule
		$p_{reproduction}$ 	& $[0,1]$                               & Probability of reproduction\\
		$p_{selection}$   	& $[0,1]$                               & Probability of selection\\
		$n_{children}$     	& $n_{children}\in \mathbb{Z}_{\ge 0}$ 	& Maximum number of children per parent\\
		Reproduce           & $entity\mapsto entity$       			& Function to create a new entity based on an existing one\\
		\bottomrule
	\end{tabular}
	\end{center}
\end{table}

\begin{algorithm}
\For{each generation $\in [1\dots$number of generations]}{
	$population\leftarrow Selection(population)$\;
	$population\leftarrow population \cup Variation(population)$\;
	\BlankLine
	\lIf{$population$ size is too small}{break}
}
\caption{Algorithm for the inheritance and variation model.}\label{base-model-algorithm}
\end{algorithm}

\begin{algorithm}
\Def{Selection(population)}{
	$population_{new}\leftarrow \{\}$\;
	\For{each $entity \in population$} {
		\Prob($p_{selection}$:){
			Add $entity$ to $population_{new}$\;
		}
	}
	\Return $population_{new}$\;
}
\BlankLine
\Def{Variation(population)}{
	$population_{new}\leftarrow \{\}$\;
	\For{each $entity$ with $fitness$ and $heritability$ in $population$}{
		\BlankLine
		\Prob($p_{reproduction}$:){
			\For{some number of children $\in \mathcal{U}[0,n_{children}]$} {
				$child\leftarrow$ Reproduce($entity$)\;
				Add $child$ to $population_{new}$\;
			}
		}
	}
	\Return $population_{new}$\;
}
\caption{Definitions for the functions \emph{Selection} and \emph{Variation}.}\label{model-functions}
\end{algorithm}

Each \gls{run} consists of a fixed number of time steps (generations), where at each step these two functions are applied in sequence to the current population to form a replacement population as documented in \cref{base-model-algorithm}. 

\section[Model behaviour in stable conditions]{Experimental test of hypothesis under stable conditions}\label{experimental-test-of-h2-under-fixed-conditions}

Returning to the overall goals for these experiments, \cref{hypothesis-2} makes two predictions for stable environments:

\begin{enumerate}
	\item Average inheritance will tend towards perfect inheritance.
	\item The population variance of inheritance will decrease more than would be expected by chance alone.
\end{enumerate}

The first test therefore is to examine if inheritance emerges from low-heritability and low-fitness initial conditions, and then the second test is whether the population variance for inheritance decreases as predicted. Remember that as discussed earlier, inheritance is the outcome of the relationship between parent and child traits, as represented by the measure of heritability, and that heritability ranges between $0$, for no correlation between parent and child, and $1.0$ for perfect correlation.

\subsection{Experimental design}\label{design}
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df_full <- subset(load.results('results/results-819350e-stable-environment.data'), truncate == 0 & correlation_correlation == 0) # correlation_correlation == 0 means TRUE in this dataset.
colnames(df_full)[13] <- "Distribution"
colnames(df_full)[14] <- "Reproduction"

df_full$n_offspring = factor(df_full$n_offspring, labels=c("2","5"))
df_full$p_reproduce = factor(df_full$p_reproduce, labels=c('Fitness','0.66'))
df_full$p_selection = factor(df_full$p_selection, labels=c('Fitness','0.66'))
df_full$Distribution = factor(df_full$Distribution, labels=c('Bounded','Sampled'))
#df_full$Reproduction <- factor(df_full$Reproduction, labels=c('Uncorrelated','Correlated'))

df <- subset(df_full, gen==500)
@

The factors and levels along with their mapping to model parameters are given in \cref{tbl:factor-levels-c7}.

\begin{table}
	\scriptsize
	\begin{center}
	\caption{Factors mapped to model parameters, plus factor levels, for the initial investigation into model sensitivity to parameter settings.}\label{tbl:factor-levels-c7}
	\begin{tabular}{@{}p{2.5cm}p{3cm}p{2cm}p{6cm}@{}}
		\toprule
		Factor                 &Model parameter		&Number of Levels 	& Levels\\
		\midrule
		$p_{reproduction}$     &$p_{reproduction}$	&2                	& 0.66 or $fitness$\\
		$p_{selection}$        &$p_{selection}$		&2                	& 0.66 or $fitness$\\
		$n_{children}$         &$n_{children}$ 		&2                	& 2 or 5\\
		\bottomrule
	\end{tabular}
	\end{center}
\end{table}

Each combination of factor levels has 10 replicates, to give a total of $2x2x2x1x2x10$ or 160 experiment runs.

\subsection{Sensitivity of the model to parameter values}\label{reducing-the-parameter-space-for-the-experiments}

In the absence of any restrictions on population size there is nothing to prevent a growing population eventually exceeding the capacity of the simulation system. This is unfortunately an unavoidable difficulty in experiments with exponential growth systems rather than a limitation of the theoretical model. 

The size of the population is determined by how population entities are introduced and removed. In standard Evolutionary Computation (\eg \textcite[50]{DeJong2006}) the choice of strategy is important to the performance and outcomes of the algorithm. New entities can be straight replacements, like-for-like, of their parent, or be placed in competition against entities in the parent population, or completely replace the parent population. Elements may be removed as a result of selection, or through fitness-independent sampling to maintain a particular population size, or through some end-of-life calculation. The population size limit may act as both upper and lower bound on population size to maintain a specific size, or solely as upper bound.

In \textcite{Gaucherel2012} the approach is to remove individuals from the population stochastically, with probability related to $e$ to the negative power of the population size multiplied by a configurable parameter, $\mu$. In the ``canonical'' Evolutionary Computation algorithm, a population limit results from selection where a set number of entities are extracted from the original population, chosen by one of a wide range of selection algorithms (among many sources, see overviews in \textcite[sect. 4.3.1]{DeJong2006} and \textcite[sect. 4.2]{Vose:1999di}.) Here though we separate the selection function from the population size limit in order to qualify the effect of the specific limiting mechanism used.

Translating model parameters into factors in the experiment design results in the first column of \cref{tbl:factor-levels-c7}. As is usual with exploratory experiments with a number of parameters, where each run of the model has some cost in time or other resources, the key problem is to understand the relationship between parameters and response variables at an acceptable cost. In this case, our main cost is time - each run of an evolutionary model is cheap in resources but takes a little time. Exhaustively sampling the entire parameter space is unrealistic. Therefore, we first reduce the search space by limiting the number of values that each parameter can take. By choosing these values appropriately, we can construct an analysis model from the results that is sufficiently accurate for our exploratory purposes at a greatly reduced cost in time.

From the results of earlier parameter selection experiments, we discovered that the model response is sensitive to the values of emph{Distribution}, \emph{Reproduction}, $p_{reproduction}$ and $p_{selection}$, but has little sensitivity to $n_{children}$.

\subsection{Does average inheritance approach perfect inheritance?}\label{does-average-inheritance-approach-perfect-inheritance}

We start with the following null and alternative hypotheses:

\begin{itemize}[label={}]
	\item H$_0$: heritability does not approach 1.0 during a run, irrespective of factor values, or \newline
 $\vert \overline{heritability}_{end}-\overline{heritability}_{start} \vert = 0$
	\item H$_1$: heritability increases to near 1.0 during a run, for some factor values, or \newline
 $\overline{heritability}_{end}-\overline{heritability}_{start} > 0$ and $1.0-\overline{heritability}_{end} < \delta$ for some $\delta$ and for some factor values.
\end{itemize}

\subsubsection{Response variables}

From the hypothesis the main property of interest is \emph{heritability}, or the correlation between parent and child property values. \emph{Fidelity} therefore is our response variable. Specifically we require $\overline{heritability}_{start}$ and $\overline{heritability}_{end}$, or the mean value for \emph{heritability} (across all replicates) at the beginning and end of each run.

\subsubsection{Results and discussion}

Of the \Sexpr{nrow(subset(df_full,gen==0))} experiment runs, all reached the experiment limit of \Sexpr{max(df_full['gen'])} generations. 

Our interest is in the final values for heritability under fixed-environment conditions. Therefore, the data in the following analysis is from the final generation of each run, unless otherwise noted.

<<lowstart, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.scap=NA, fig.cap='Summary results, showing on the left-hand side an overview for the final mean heritability ($\\overline{heritability}_{end}$) for each run and a density plot showing the distribution of $\\overline{heritability}_{end}$. The right-hand side shows the corresponding plots for final mean fitness ($\\overline{fitness}_{end}$)'>>=
ap <- qplot(row.names(df),ave_cor, geom="point", data=df, xlab='Experiment run',ylab="Final mean heritability") + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
bp <- qplot(row.names(df),ave_fit, geom="point", data=df, xlab='Experiment run',ylab="Final mean fitness") + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
cp <- qplot(ave_cor, geom="density", data=df, xlab="Final mean heritability",ylab="")
dp <- qplot(ave_fit, geom="density", data=df, xlab="Final mean fitness",ylab="")
grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
@

<<lowstartbyfactor, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.scap=NA, fig.cap='Summary results, this time averaged across all replicates of each unique combination of factor levels (that is, grouped by factor level). Dark-coloured points represent values at end of run; light-coloured points in a horizontal line are for initial values, reflecting averaged initial conditions.'>>=
ap <- ggplot(subset(df_full,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=ave_cor, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='Factor combination', y='Mean heritability') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
bp <- ggplot(subset(df_full,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=ave_fit, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='Factor combination', y='Mean fitness') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
cp <- ggplot(subset(df_full,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=sd_cor, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='', y=expression(paste(sigma," heritability"))) + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
dp <- ggplot(subset(df_full,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=sd_fit, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='', y=expression(paste(sigma," fitness"))) + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
@

<<mean-heritability-fitness, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.scap=NA, fig.cap='Ranges by generation of the mean heritability (top) and mean fitness (bottom) for all levels of all factors.'>>=
ap <- ggplot(df_full) + geom_line(aes(x=gen,y=ave_cor,group=run)) + labs(x="Generation", y="Mean heritability")
bp <- ggplot(df_full) + geom_line(aes(x=gen,y=ave_fit,group=run)) + labs(x="Generation", y="Mean fitness")
grid.arrange(ap,bp,nrow=2,ncol=1)
@

The first hypothesis prediction is that average heritability will tend towards exact inheritance, or $1.0$. The reasoning is as follows: heredity describes the correlation along a lineage, so a high fitness entity with high heredity is likely to have more high fitness descendants than low fitness descendents. Higher fitness entities will survive longer and reproduce more, and so high fitness/high heredity entities will slowly invade the population.

A simple visual inspection of this data in \cref{fig:lowstart} reveals that the heritability measure has a strong peak as predicted at $1.0$, with final mean heritability greater than approximately $0.96$ for all remaining runs. Fitness is bimodal, with peaks around final mean fitness values of approximately $0.45-0.55$ and $1.0$. From \cref{fig:lowstartbyfactor,fig:mean-heritability-fitness} it seems that the initial peak in fitness is associated with a subset of factor-levels. Closer examination shows that these runs, and only these runs, have the level $0.66$ for both factors $p_{reproduction}$ and $p_{selection}$. This supports the earlier observation in \cref{reducing-the-parameter-space-for-the-experiments} concerning the importance of the distinction between absolute and relative (\eg fitness-based) values for these two factors.

From inspection it seems clear that heritability does approach $1.0$ as predicted by the hypothesis. In conclusion, H$_0$ can be rejected, and H$_1$ accepted. Inheritance increases regardless of the model design.

\subsection{Does the standard deviation of inheritance decrease over time in the population?}\label{variance-of-inheritance-stable}

The second prediction of the hypothesis is that the population variance for inheritance ($\sigma_{heritability}$) should decrease over time towards a limit of $0$ in fixed environments.

\begin{itemize}[label={}]
	\item H$_0$: $\sigma_{heritability_{end}}-\sigma_{heritability_{start}} >= 0$, for all factor values.
	\item H$_1$: $\sigma_{heritability_{end}}-\sigma_{heritability_{start}} < 0$, for some factor values.
\end{itemize}

\subsubsection{Response variables}

Once again, the main property of interest, and so our response variable, is \emph{heritability}. From the hypothesis we require $\sigma_{heritability_{end}}$ and $\sigma_{heritability_{start}}$, the standard deviation of heritability at the beginning of each run, and at the end.

\subsubsection{Results and discussion}
<<sd-heritability-fitness, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.scap=NA, fig.cap='Ranges by generation of the standard deviation of heritability on the top row, and standard deviation of fitness below.'>>=
ap <- ggplot(df_full) + geom_line(aes(x=gen,y=sd_cor,group=run)) + labs(x="Generation", y=expression(paste(sigma," heritability")))
bp <- ggplot(df_full) + geom_line(aes(x=gen,y=sd_fit,group=run)) + labs(x="Generation", y=expression(paste(sigma," fitness")))
grid.arrange(ap,bp,nrow=2,ncol=1)
@

Data is all generations for all \Sexpr{nrow(unique(df['run']))} fixed-environment runs that reached completion at generation \Sexpr{max(df['gen'])}.

In a similar fashion to the procedure in \cref{does-average-inheritance-approach-perfect-inheritance}, by visual inspection of \cref{fig:sd-heritability-fitness}, the variance (square of the standard deviation) does decrease towards zero in all cases. Once again, the runs appear to fall into two distinct groups based on the speed of convergence towards zero, as seen in \cref{fig:sd-heritability-fitness}. The upper group is exclusively associated with runs where both factors $p_{reproduction}$ and $p_{selection}$ are set to $0.66$; these levels are also responsible for the two uppermost groups of final standard deviation of fitness in the lower section of \cref{fig:sd-heritability-fitness}. The distinction between these two uppermost groups however is due to a third factor, \emph{Distribution}: the uppermost group has level ``Bounded'' and the lowermost (or the middle group of the three) has level ``Sampled.''

In conclusion, from visual inspection, the variance of heritability does approach $0$, for all combinations of factor levels except where both $p_{reproduction}$ and $p_{selection}$ are set to $0.66$. Therefore H$_0$ is rejected and H$_1$ accepted.

\section{Model behaviour in changing environments}\label{model-behaviour-in-changing-environments}

In this \namecref{model-behaviour-in-changing-environments} we describe an appropriate experimental design and model to describe environmental changes; test the predictions of \cref{hypothesis-2} using the model; and finally discuss the results of the experiment. 

\subsection{Environmental model}\label{environmental-model}

Alongside the evolutionary model from \cref{base-model} we now add an environmental model to describe the changing fitness relationship between entities and the environment at each step of the evolutionary model. 

\subsubsection{Abrupt environmental change}\label{abrupt-environmental-change}

Although the generator described earlier produces a time series for environmental change with the property of stationarity, the $\delta$ term makes the evolutionary model of fitness non-stationary. However, any change is steady and gradual. An extension would be to co-opt the idea of concept drift from time series analysis to induce an abrupt change with probability $p$ at each generation. Each change would therefore form a new `concept`. Instead of the environment changing in a predictable and describable way from one generation to another, the change could not be predictable from the earlier history.

\subsubsection{Parameterised time-series model}

Environmental change may be represented as a parameterised time-series\footnote{A \emph{time series} is a set of observations $x_i$, each one being recorded at a specific time $t$ where an observation $x_i \in$ some set $\{X\}$, assumed to be $\mathbb{R}$ \parencite{Brockwell:2002dq}.} of particular form. 

Statistical techniques are commonplace for time-series predictions \parencite{Brockwell:2002dq}. ARMA models are used for \emph{stationary} series, that is a time-series whose joint probability distribution (and hence whose statistical properties such as mean and variance) do not change over time, equivalent to saying the series does not demonstrate concept drift. ARIMA models apply for non-stationary series where the difference between two sequential values of the original series can be shown to produce a stationary series--the I or ``Integrated'' component of the model provides the differencing.

Although time-series modelling provides techniques for describing time-series data in terms of an underlying model, the process can also be reversed to produce a time-series \textit{from} the model; in other words, if the variety of environmental change required to test our hypothesis can be described by a standard time-series model, the parameters that determine that model can also serve as our headline measure for environmental change.

Our environmental model provides an enhanced AR(1) or first-order autoregressive time-series, with each timestep corresponding to one evolutionary generation. Specifically, we describe the evolutionary change at each timestep as a function of the previous timestep:

$x_t = \Theta x_{t-1} + e_t + \delta$ \label{ar-1-time-series}

where $x_t$ is the change at timestep $t$, $\Theta$ is the AR coefficient, $e_t$ is a random, normally distributed, error component around a mean of $0$, where $e_t\stackrel{iid}{\sim}N(0,\sigma^{2}_e)$, and $\delta$ is a fixed bias value.

This series allows us to represent a broad range of environmental changes:

\begin{itemize}
	\item Each time-series is completely specified by three parameters, $\Theta$, $\sigma_e$ and $\delta$.
	\item $\Theta$ in an autoregressive time-series can be interpreted as specifying stability or smoothness, while $\delta$ is a fixed change. We use $\delta$ to model a fitness trend - environments with a positive $\delta$ will see the fitness of each entity improved at each generation, with the opposite of course true of negative $\delta$. Note that with this formulation we can model linear trends in fitness from a fixed bias in the environment produced by the $\delta$ term. This is not the same as a ARI model where the deltas of the environment time-series itself would follow a trend.
	\item An AR time-series has the property of stationarity, with the implication that the mean of the series is constant through time. However, as we apply the series values as deltas to element fitness, fitness can be non-stationary, and so may show a long term trend. This allows a simple non-differencing time-series to describe a steady change in fitness.
	\item As a corollary of stationarity, the range of the series is determined by the initial parameters. This is a useful property as it means that with appropriate parameter choices no scaling of the range is required.
	\item The time-series is defined by three independent elements, two predictable (driven by $\Theta$ and $\delta$) and the other ($\sigma_e$) random and unlearnable. By changing the ratio between the predictable and unpredictable we can examine the performance of the evolutionary algorithm on some continuum of predictability.
\end{itemize}

<<factorialthetasigma, pdfcrop=TRUE, echo=FALSE, warning=FALSE, cache=TRUE, fig.pos='ht', fig.scap=NA, fig.cap='Visualisation of the fitness changes at each time interval (top facet) and cumulative fitness change (bottom facet) that result from our environmental model for delta=0 with two values of sigma (left and right), and three values of theta (each facet row.)'>>=
library(reshape2)
library(ggplot2)
library(cowplot) # styling of plots, extension of ggplot2
library(gridExtra)

t1 <- read.csv('results/environments-factorial.csv', header=FALSE, colClasses=c("numeric","numeric","numeric"))

t1$run <- 1:nrow(t1)
names(t1)[1:3]<-c("theta", "sigma", "delta")
t2 <- melt(t1,id=c('run','theta','sigma','delta'))
t2$theta <- round(t2$theta,3)
t2$sigma <- round(t2$sigma,3)
t2$delta <- round(t2$delta,3)

for (r in unique(t2$run)) {
t2[t2$run==r,'t'] <- 1:50 # tag with timestamp

fitness <- 0.5
for (t in 1:50) {
fitness <- max(0,min(1,fitness + t2[t2$run==r & t2$t==t,'value']))
t2[t2$run==r & t2$t==t,'fitness'] = fitness
}
}
ap <- ggplot(subset(t2,delta==0)) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_grid(theta~sigma, labeller='label_both') + labs(x="t", y="Fitness change")  + theme(strip.text = element_text(size = 8))
bp <- ggplot(subset(t2,delta==0)) + geom_line(aes(x=t,y=fitness)) + facet_grid(theta~sigma, labeller='label_both') + labs(x="t", y="Fitness change")  + theme(strip.text = element_text(size = 8))
grid.arrange(ap,bp,nrow=2,ncol=1)
@

<<factorialsigmadelta, pdfcrop=TRUE, echo=FALSE, warning=FALSE, cache=TRUE, fig.pos='ht', fig.scap=NA, fig.cap='Visualisation of the fitness changes at each time interval (top facet) and cumulative fitness change (bottom facet) for theta=0 with three values of delta (columns), and two values of sigma (each facet row.)'>>=
ap <- ggplot(subset(t2,theta==0)) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_grid(sigma~delta, labeller='label_both') + labs(x="t", y="Fitness change")  + theme(strip.text = element_text(size = 8))
bp <- ggplot(subset(t2,theta==0)) + geom_line(aes(x=t,y=fitness)) + facet_grid(sigma~delta, labeller='label_both') + labs(x="t", y="Fitness change")  + theme(strip.text = element_text(size = 8))
grid.arrange(ap,bp,nrow=2,ncol=1)
@


<<sampleenvironment, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='ht', fig.scap=NA, fig.cap='Example time-series produced by our environmental model for a sample of theta, sigma and delta values. These are incremental fitness changes, rather than cumulative ones.'>>=
t1 <- read.csv('results/environments.csv', header=FALSE, colClasses=c("numeric","numeric","numeric"))
t1$run <- 1:nrow(t1)
names(t1)[1:3]<-c("theta", "sigma", "delta")
t2 <- melt(t1,id=c('run','theta','sigma','delta'))
t2$theta <- round(t2$theta,3)
t2$sigma <- round(t2$sigma,3)
t2$delta <- round(t2$delta,3)
ggplot(t2) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_wrap(~theta+sigma+delta, labeller='label_both') + labs(x="t", y="Fitness change")  + theme(strip.text = element_text(size = 8))
@


% Needs sampleenvironment
<<cumulativefitness, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.pos='ht', fig.scap=NA, fig.cap='Cumulative time-series examples for samples of theta, sigma and delta.'>>=
for (r in unique(t2$run)) {
t2[t2$run==r,'t'] <- 1:50 # tag with timestamp

fitness <- 0.5
for (t in 1:50) {
fitness <- max(0,min(1,fitness + t2[t2$run==r & t2$t==t,'value']))
t2[t2$run==r & t2$t==t,'fitness'] = fitness
}
}

ggplot(t2) + geom_line(aes(x=t,y=fitness)) + facet_wrap(~theta+sigma+delta, labeller='label_both') + labs(x="t", y="Fitness change") + theme(strip.text = element_text(size = 8))
@

\subsection{Experimental design}

\Cref{hypothesis-2} makes these predictions for changing environments:

\begin{enumerate}
	\item Fidelity is proportional to the predictability of the environment, that is at a minimum in conditions of maximum unpredictability, and at a maximum in stable conditions.
	\item The higher the variability in the environment, the higher the $\sigma_{heritability}$. As a corollary, $\sigma_{heritability}$ under changing conditions will be greater than that under stable conditions.
\end{enumerate}

As all three independent variables for our experiment, $\Theta$, $\sigma_e$ and $\delta$, are continuous in $\mathbb{R}$, and as we wish to test the specific relationship of heritability across a range of these variables without being restricted to the initial choice of fixed levels, we shift from the fixed-effect factorial designs used earlier to a random effects model \parencite[chap.13]{Montgomery2009}. The independent variables in each run are the parameters to the environmental model, with values taken from a uniform random sample from their range. Or in other words, a series of random samples with uniform probability from a cube formed by a parameter on each of the three axes (see \cref{fig:cube}.)

\begin{figure}
	\begin{center}
	\resizebox{0.35\textwidth}{!}{
		\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=3cm, thick,font=\LARGE]
		\draw (-4,3) rectangle (4,-5);
		\draw (-4,3) -- (-1,6) -- (7,6) -- (7,-2) -- (4,-5);
		\draw (7,6) -- (4,3) ;
		\draw [dashed] (-1,6) -- (-1,-2) -- (7,-2);
		\draw [dashed] (-1,-2) -- (-4,-5);
		\draw [densely dotted] (1,-5) -- (2,-4) -- (5,-4);
		\draw [densely dotted] (2,-4) -- (2,1.5) node[above] {$\Theta,\delta,\sigma_e$} ;
		\node at (-5,-1) {$\delta$};
		\node at (0,-6) {$\Theta$};
		\node at (6.5,-4) {$\sigma_e$};
		\end{tikzpicture}
	}
	\caption{The cube formed by the ranges of the three parameters to the environmental model, $\Theta$, $\sigma_e$ and $\delta$.}
	\label{fig:cube}
	\end{center}
\end{figure}

We create a set of independent datasets by sampling from four different parameter cubes, as defined in \cref{tbl:range-of-independent-variables}. The extremes of the ranges see a lower proportion of the runs (representing more significant environmental changes or more rapid changes) reach completion than was seen in the centre (where the environment was stable.) The range of each variable (the length of a cube side) is adjusted from dataset to dataset to balance an adequate density of coverage across the overall range against the total run time.

\begin{table}
	\scriptsize
	\begin{center}
	\caption{Range of independent variables $\Theta$, $\sigma_e$ and $\delta$.}
	\label{tbl:range-of-independent-variables}
	\begin{tabular}{@{}llll@{}}
		\toprule
		Dataset   	 	&  $\Theta$		& $\sigma_e$	& $\delta$\\
		\midrule
		Dataset no.1	& $[-0.4, 0.4]$	& $[0, 0.2]$ 	& $[-0.1, 0.1]$\\
		Dataset no.2	& $[-0.2, 0.2]$	& $[0, 0.1]$	& $[-0.05, 0.05]$\\
		Dataset no.3	& $[-0.2, 0.2]$	& $[0, 0.1]$	& $[-0.05, 0.05]$\\
		Dataset no.4	& $[-0.4, 0.4]$	& $[0, 0.4]$	& $[-0.04, 0.04]$\\
		\bottomrule
	\end{tabular}
	\end{center}
\end{table}

The other factors and levels along with their mapping to model parameters, are shown below:
\begin{itemize}
	\item$p_{reproduction}$ = fitness.
	\item $p_{selection}$ = fitness.
	\item $n_{children}=2$.
\end{itemize}

Finally, the dependent or response variables, driven by the hypothesis predictions, are mean population heritability and mean population fitness.

%\begin{table}\label{reproduction-distribution-factors-c8}
%	\caption{Levels for the factor \emph{Reproduction}}
%	\begin{tabular}{@{}p{2.5cm}p{3cm}p{7cm}@{}}
%		\toprule
%		Reproduction&Child entity parameter					&Distribution shape\\
%		\midrule
%		\multirow{2}{*}{Correlated}		&$fitness_{new}$	&$\mu=fitness$ and $\sigma=1-heritability$\\
%										&$heritability_{new}$	&$\mu=heritability$ and $\sigma=1-heritability$\\
%		
%		\multirow{2}{*}{Uncorrelated}	&$fitness_{new}$	&$\mu=fitness$ and $\sigma=1-heritability$\\
%										&$heritability_{new}$	&$\mu=heritability$ and $\sigma=\mathcal{U}[0, 1.0]$\\
%		\bottomrule
%	\end{tabular}
%\end{table}


\subsection{Results}

<<figure23, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.pos='htp', fig.scap=NA, fig.cap='Final mean heritability against the environmental model\'s sigma parameter (top), delta parameter (middle) and theta parameter (bottom) for all experimental runs.'>>=
# BY_LINEAGE	CORRELATED	N_OFFSPRING	P_REPRODUCE	P_SELECTION	RESTRICTION	delta	sigma	theta	ave_fid	ave_fit	experiment	gen	pop	run	sd_fid	sd_fit
colClasses <- c("factor","factor","factor","factor","factor","factor","numeric","numeric","numeric","numeric","numeric","factor","integer","numeric","integer","numeric","numeric")
r1 <- read.csv('results/results-0c267bcd2-a.csv', colClasses=colClasses)
r2 <- read.csv('results/results-0c267bcd2-b.csv', colClasses=colClasses)
r3 <- read.csv('results/results-cc648a0a9.csv', colClasses=colClasses)
r4 <- read.csv('results/results-996060f.csv', colClasses=colClasses)
results = rbind(r1,r2,r3,r4)
colnames(results)[c(7,8,9)] <- c("delta","sigma","theta")

results$BY_LINEAGE = factor(results$BY_LINEAGE, labels=c("By Population","By Lineage"))

ap <- ggplot(subset(results,gen==500)) + geom_point(aes(sigma,ave_fid)) + labs(x=expression(paste(sigma, ' parameter to environmental model')), y='Final mean heritability')
bp <- ggplot(subset(results,gen==500)) + geom_point(aes(delta,ave_fid)) + labs(x=expression(paste(delta, ' parameter to environmental model')), y='Final mean heritability') 
cp <- ggplot(subset(results,gen==500)) + geom_point(aes(theta,ave_fid)) + labs(x=expression(paste(theta, ' parameter to environmental model')), y='Final mean heritability')
grid.arrange(ap,bp,cp,nrow=3,ncol=1)
@

<<figure22, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.pos='htp', fig.scap=NA, fig.cap='Distribution of maximum generation reached for all runs for dataset no.1 (top) through dataset no.4 (bottom). Axes are to the same scale. Runs (along the x-axis) are ordered by final generation reached.'>>=
t1 <- aggregate(r1$gen,by=list(r1$run),max)
t2 <- aggregate(r2$gen,by=list(r2$run),max)
t3 <- aggregate(r3$gen,by=list(r3$run),max)
t4 <- aggregate(r4$gen,by=list(r4$run),max)
max_nrow <- max(nrow(t1),nrow(t2),nrow(t3),nrow(t4))
ap <- ggplot(t1,aes(1:nrow(t1),t1[order(t1[,2],decreasing=TRUE),2]))+geom_bar(stat="identity") + scale_x_continuous(limits=c(0,max_nrow)) + labs(x="", y="Final generation")
bp <- ggplot(t2,aes(1:nrow(t2),t2[order(t2[,2],decreasing=TRUE),2]))+geom_bar(stat="identity") + scale_x_continuous(limits=c(0,max_nrow)) + labs(x="", y="Final generation")
cp <- ggplot(t3,aes(1:nrow(t3),t3[order(t3[,2],decreasing=TRUE),2]))+geom_bar(stat="identity") + scale_x_continuous(limits=c(0,max_nrow)) + labs(x="", y="Final generation")
dp <- ggplot(t4,aes(1:nrow(t4),t4[order(t4[,2],decreasing=TRUE),2]))+geom_bar(stat="identity") + scale_x_continuous(limits=c(0,max_nrow)) + labs(x="", y="Final generation")
grid.arrange(ap,bp,cp,dp,nrow=4,ncol=1)
@

%<<figure25, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE,fig.pos='htp', fig.scap=NA, fig.cap='Final mean heritability against final mean fitness (top) and final mean heritability against final standard deviation of heritability (bottom).'>>=
%ap <- ggplot(subset(results,gen==500),aes(ave_fit,ave_fid)) + geom_point() + geom_smooth(method='lm') + labs(x='Final mean fitness',y='Final mean heritability')# and relationship between these two
%# Relationship between sd_fid and ave_fid!
%bp <- ggplot(subset(results,gen==500)) + geom_point(aes(sd_fid,ave_fid)) + labs(x='Final standard deviation of heritability',y='Final mean heritability')
%grid.arrange(ap,bp,nrow=2,ncol=1)
%#anova(lm(sd_fid~ave_fid, data=subset(results, gen==500))) # 0.001
%@

A summary of the results from each of the four datasets is given in \cref{tbl:summary-for-changing-datasets}, and a visualisation of all four datasets combined given in \cref{fig:figure23}. Note that the difference in the proportion of completed runs to total runs reflects the varying ranges of the independent variables in each dataset (as seen in \cref{tbl:range-of-independent-variables}), shown graphically in \cref{fig:figure22}.

\begin{table}
	\scriptsize
	\begin{center}
	\caption{Summary of results for changing environments.}
	\label{tbl:summary-for-changing-datasets}
	\begin{tabular}{@{}lll@{}}
		\toprule
		Dataset    		& $n$ (total runs) 				& Completed runs\\
		\midrule
		Dataset no.1	& \Sexpr{max(r1$run)+1}			& 	\Sexpr{nrow(r1[r1$gen==500,])+1}\\
		Dataset no.2	& \Sexpr{max(r2$run)+1}			& 	\Sexpr{nrow(r2[r2$gen==500,])+1}\\
		Dataset no.3	& \Sexpr{max(r3$run)+1}			& 	\Sexpr{nrow(r3[r3$gen==500,])+1}\\
		Dataset no.4	& \Sexpr{max(r4$run)+1}			& 	\Sexpr{nrow(r4[r4$gen==500,])+1}\\
		\bottomrule
	\end{tabular}
	\end{center}
\end{table}

\subsection{Is heritability proportional to the predictability of the environment?}\label{mean-heritability-predictability}

Remembering that a function of $\Theta$, $\sigma_e$ and $\delta$ provides a measure of environmental predictability, our null and alternate hypotheses are:

\begin{itemize}[label={}]
	\item H$_0$: $\overline{heritability}_{end}$ is not proportional to $f(\Theta,\sigma_e,\delta)$ for all functions $f()$.
	\item H$_1$: $\overline{heritability}_{end}$ is proportional to $f(\Theta,\sigma_e,\delta)$ for some function $f()$.
\end{itemize}

Because only $\Theta$ and $\delta$ are predictable, and hence learnable, we would not expect $\sigma_e$ as a parameter to any $f().$ 

By visual inspection of \cref{fig:figure23}, $\overline{heritability}_{end}$ is related to $f(\delta)$; this is partially supported by the results of an \gls{anova} analysis, which finds both $\delta$ and $\Theta$ to be highly significant (p\textless 0.001). Therefore $\overline{heritability}_{end}$ is proportional to either $f(\delta)$ or $f(\Theta,\delta)$ (and incidentally meets our expectation that $\sigma_e$ is not a potential parameter to $f()$ as it is not learnable by an evolutionary system.)

<<anova-mean-heritability, results='asis',echo=FALSE>>=
temp <- subset(results,gen==500)
colnames(temp)[10] <- "mean_heritability"

a <- with(temp,anova(lm(mean_heritability~sigma+delta+theta)))

library(xtable)

print(xtable(a, caption='ANOVA analysis for H$_1$: $\\overline{heritability}_{end}$ is proportional to $f(\\Theta,\\sigma_e,\\delta)$.', label='tbl:model-mean-anova'), booktabs=TRUE, include.rownames=TRUE, size="scriptsize", caption.placement="top")
@

As a result, we can reject H$_0$ and accept H$_1$: heritability is proportional to the predictability of the environment (as measured by some function $f(\Theta,\delta)$.)

%<<figure24, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE,fig.pos='htp', fig.scap=NA, fig.cap='Subset of results where $0.002 < \\lvert delta\\rvert$, again showing final mean heritability (top) and final standard deviation of heritability (bottom) against sigma parameter.'>>=
%results_low_bias <- subset(results, -0.002<delta & delta<0.002)
%ap <- ggplot(subset(results_low_bias,gen==500),aes(sigma,ave_fid)) + geom_point() + geom_smooth(method='lm') + labs(x='sigma parameter to environmental model', y='Final mean heritability')
%bp <- ggplot(subset(results_low_bias,gen==500),aes(theta,ave_fid)) + geom_point() + geom_smooth(method='lm') + labs(x='theta parameter to environmental model', y='Final mean heritability')
%grid.arrange(ap,bp,nrow=2,ncol=1)
%@
%
%<<figure24a, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE,fig.pos='htp', fig.scap=NA, fig.cap='Subset of results where $0.002 < \\lvert delta\\rvert$, again showing final mean heritability (top) and final standard deviation of heritability (bottom) against sigma parameter.'>>=
%results_low_bias <- subset(results, -0.002<delta & delta<0.002)
%ap <- ggplot(subset(results_low_bias,gen==500),aes(sigma,sd_fid)) + geom_point() + geom_smooth(method='lm') + labs(x='sigma parameter to environmental model', y='Final standard deviation of heritability')
%bp <- ggplot(subset(results_low_bias,gen==500),aes(theta,sd_fid)) + geom_point() + geom_smooth(method='lm') + labs(x='theta parameter to environmental model', y='Final standard deviation of heritability')
%grid.arrange(ap,bp,nrow=2,ncol=1)
%@

\subsection{Does the standard deviation of heritability vary in proportion to the variability in the environment?}\label{sd-heritability-predictability}

\begin{itemize}[label={}]
	\item H$_0$:  $\sigma_{heritability_{end}}$ is not proportional to $f(\Theta,\sigma_e,\delta)$ for all functions $f()$.
	\item H$_1$:  $\sigma_{heritability_{end}}$ is proportional to $f(\Theta,\sigma_e,\delta)$ for some function $f()$.
\end{itemize}

An \gls{anova} model evaluating a linear relationship between the standard deviation of heritability, and the parameters $\delta$, $\sigma_e$ and $\Theta$ to the environmental model suggests that all three parameters have some effect on the $\sigma_{heritability}$, with the influence of $\delta$ and $\Theta$ being highly significant (p\textless 0.001) and that of $\sigma_e$ significant (p\textless 0.01).  Unlike in the previous \namecref{mean-heritability-predictability}, here we are interested in the variability, or unpredictability, of the environment. As $\sigma_e$ is fundamentally unpredictable, it is not unexpected that it contributes to the relationship between the unpredictability of the environment and the variance of heritability.

<<anova-sd-heritability, results='asis',echo=FALSE>>=
temp <- subset(results,gen==500)
colnames(temp)[16] <- "sd_heritability"

a <- with(temp,anova(lm(sd_heritability~sigma+delta+theta)))

library(xtable)
print(xtable(a, caption='ANOVA analysis for H$_1$: $\\sigma_{heritability_{end}}$ is proportional to $f(\\Theta,\\sigma_e,\\delta)$.', label='tbl:model-sd-anova'), booktabs=TRUE, include.rownames=TRUE, size="scriptsize", caption.placement="top")
@

<<sd-heritability, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.pos='htp', fig.scap=NA, fig.cap='Final standard deviation of heritability against the environmental model\'s sigma parameter (top), delta parameter (middle) and theta parameter (bottom) for all experimental runs.'>>=
ap <- ggplot(subset(results,gen==500)) + geom_point(aes(sigma,sd_fid)) + labs(x=expression(paste(sigma, ' parameter to environmental model')), y=expression(sigma))
bp <- ggplot(subset(results,gen==500)) + geom_point(aes(delta,sd_fid)) + labs(x=expression(paste(delta, ' parameter to environmental model')), y=expression(sigma)) 
cp <- ggplot(subset(results,gen==500)) + geom_point(aes(theta,sd_fid)) + labs(x=expression(paste(theta, ' parameter to environmental model')), y=expression(sigma))
grid.arrange(ap,bp,cp,nrow=3,ncol=1)
@

The corollary of this prediction is that $\sigma_{heritability}$ will be greater in unpredictable environments than in stable environments (when all three parameters are near zero). As a first approach, we note that in \cref{variance-of-inheritance-stable} we showed that in stable environments the variance of inheritance tended towards zero. As $\sigma_e$ is by definition unpredictable (see \cref{environmental-model}) we can provide some support for the corollary if we can show that variance remains above zero for all $\sigma_e>0$ (incorporating all necessary uncertainties): 

\begin{itemize}[label={}]
	\item H$_0$:  $\sigma_{heritability_{end}}$ is close to $0$, for all clearly non-zero combinations of $\Theta$,$\delta$ and $\sigma_e$.
	\item H$_1$:  $\sigma_{heritability_{end}}$ is not close to $0$, for some clearly non-zero combinations of $\Theta$,$\delta$ and $\sigma_e$.
\end{itemize}

From a visual inspection of \cref{fig:sd-heritability} we can reject H$_0$ in favour of H$_1$: $\sigma_{heritability_{end}}$ is greater under unpredictable conditions than it is under stable environmental conditions.

%<<echo=FALSE>>=
%a<-with(temp,lm(sd_heritability~sigma*theta*delta))['coefficients']
%t1 <- read.csv('results/environments-factorial.csv', header=FALSE, colClasses=c("numeric","numeric","numeric"))
%
%t1$run <- 1:nrow(t1)
%names(t1)[1:3]<-c("theta", "sigma", "delta")
%t2 <- melt(t1,id=c('run','theta','sigma','delta'))
%t2$theta <- round(t2$theta,3)
%t2$sigma <- round(t2$sigma,3)
%t2$delta <- round(t2$delta,3)
%t3<-unique(t2[,c(2,3,4)])
%predictions <- cbind(t3,sd=predict(a,newdata=t3))
%predictions[order(predictions$sd),]
%
%with(temp,temp[order(sd_heritability),c("sd_heritability","delta","sigma","theta")])
%@

\section{Conclusions}\label{part2-future-work}

In this chapter we have shown by experiment that, as predicted, in stable environments:
\begin{itemize}
	\item Inheritance increases towards an upper limit of $1.0$, or perfect inheritance.
	\item The variance of inheritance decreases towards a lower limit of $0$.
\end{itemize}

This is encouraging, and the first point confirms the main result from the exploration by \textcite{Bourrat2015}. However, from the viewpoint of a creative open-ended evolutionary process, this is not in fact what is desired: perfect inheritance means an absence of novelty. As hypothesised earlier in \cref{h2} though, this should be different in systems where the environment changes, and in the following \namecref{model-behaviour-in-changing-environments} we shall put this to the test.

The previous \namecref{model-behaviour-in-changing-environments} has tested our hypothesis predictions from \cref{h2} for changing environments. First we defined what we mean by environmental change. In \cref{environmental-model} we described an environmental model based on a AR(1) timeseries defined by three independent parameters, $\Theta$, $\delta$ and $\sigma_e$. Specifically, the change in fitness at each timestep $t$ is given by the function $\Theta x_{t-1} + e_t + \delta$, where $e_t$ is an error term related to $\sigma_e$ by $e_t\stackrel{iid}{\sim}N(0,\sigma^{2}_e)$. From this description it is clear that of these three parameters, two--$\Theta$ and $\delta$--are more discoverable by an evolutionary learner than the other, $\sigma_e$.

\Cref{h2} made these predictions:

\begin{enumerate}
	\item Fidelity is proportional to the predictability of the environment, that is at a minimum in conditions of maximum unpredictability, and at a maximum in stable conditions.
	\item The higher the variability in the environment, the higher the $\sigma_{heritability}$. As a corollary, the $\sigma_{heritability}$ under changing conditions will be greater than that under stable conditions.
\end{enumerate}

In \cref{mean-heritability-predictability} we constructed a random-effects factorial experiment where the simulation model from \cref{base-model} was combined with the environmental model to confirm the first prediction: heritability in changing environments is indeed related to environmental predictability. However, this conclusion would be strengthened by an ordering of predictability based on $\Theta$, $\delta$ and $\sigma_e$, as suggested below in future work.

The second prediction was also confirmed in similar fashion in \cref{sd-heritability-predictability} with a similar caveat. Although we can show a clear difference between stable and changing environments, without an absolute ordering over $\Theta$, $\delta$ and $\sigma_e$ it is difficult to show a proportional relationship.

Overall this \namecref{model-behaviour-in-changing-environments} provides good support for our hypothesis that an effective inheritance mechanism can not only emerge from a low-heredity environment through evolution, but that it can be tuned and optimised by evolution to suit the population environment.

