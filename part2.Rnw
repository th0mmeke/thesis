<<setup, include=FALSE>>=
library(knitr)
library(cowplot) # styling of plots, extension of ggplot2
library(gridExtra) # grid layouts for ggplot2
library(lattice) # needed for bwplot etc
library(english) # convert numbers to words
opts_chunk$set(fig.path='generated_figures/')
knit_hooks$set(pdfcrop = hook_pdfcrop)

load.results.simple <- function(t) {
colClasses <- c("numeric","numeric","integer","integer","numeric","numeric","factor","factor","factor","factor","factor","factor","factor","factor")
read.csv(t, colClasses=colClasses)
}

load.results <- function(t) {
colClasses <- c("integer","integer","numeric","numeric","integer","integer","numeric","numeric","factor","factor","factor","factor","factor","factor","factor","factor")
df <- read.csv(t, colClasses=colClasses)
df$ecf <- factor(df$ecf,levels=c(0,1,5,10)) # place into natural order
df
}
@

% # Filter(Negate(is.factor),z[[gen]]) # quantile values for each combination of factor levels at gen
% # levels <- unique(interaction(Filter(is.factor,y))) # all unique combinations of factor levels in y

\chapter{Introduction}

%Models (not simulations) for the emergence of heredity, for the purpose of strengthening the theoretical basis for any simulation. Goal is a causal, rather than constitutive, model for the emergence of heredity from variation and inheritance.

%		\item Can evolution act on the inheritance mechanism to tune it for varying environmental conditions?
%		\item Does inheritance still emerge from variation when the fitness-environment relationship is not fixed?

\section{Previous work}

A significant body of literature has accumulated over the years on models of biological inheritance. While standard population genetics describes the dynamics over time of genotype frequencies, it remains deeply rooted in the biology of genotypes and alleles. Different forms of inheritance, such as acquired (or Lamarckian inheritance) characteristics have been compared and contrasted to the canonical non-acquired form in \gls{ens} by a number of authors (including for example \textcite{Jablonka1995, Paenke:2007ie,Gaucherel2012}), going back to the competing models for inheritance of Darwin and Lamarck themselves. Early hypotheses in the metabolism-first or replicator-first debate led to the recognition of the significance of error threshold rates for mutations during copying \parencite{Eigen1971}, and later investigation of the interaction between inheritance (of genotypes, when non-acquired), selection (on the phenotype), and development (linking genotype to phenotype) inspired the theory of neutral landscapes \parencite{Kimura:1968uq}. 

In comparison, little explicit modelling of inheritance has been done in \gls{alife}. Most relevant work has already been reviewed in \cref{previous-work} where it generally forms one element of a wider investigation into \gls{oe}. However, in relation to \gls{ea}, many models for inheritance from variation and recombination have been proposed, but are less relevant to our needs as they rely on exogenous, or external, predefined mechanisms rather than emerging from the properties of the genotypes. 

\Textcite{Paixao2015} sought common principles between population genetics and evolutionary computation with the goal of unifying the two fields, starting from a broad description of evolutionary processes as a ``population undergoing changes over time based on some set of transformations''. A transformation can be decomposed into a collection of (stochastic) operators, with an operator being a probability distribution of potential outcomes; some operators act on phenotypes, others on genotypes. Various operators drawn from evolutionary computing are defined: for selection (uniform, proportional, tournament, truncation, cut, replace); variation from mutation (uniform, single-point), and variation from recombination (one-point crossover, k-point crossover, uniform crossover, unbiased variation).

The evolutionary process is then a trajectory through a space of distributions; it can therefore be seen equally as a sequence of population transformations, and distribution transformations. When considered as a series of distribution transformations, \textcite{Paixao2015} sees a correspondence with Estimation Distribution Algorithms (EDA): ``In an Estimation Distribution Algorithm (EDA), the algorithm tries to determine the distribution of the solution features, e.g. probability of having a 1-bit at a particular position, at the optimum. Some EDAs can be regarded abstractions of evolutionary processes: instead of generating new solutions through variation and then selecting from these, EDAs use a more direct approach to refine the underlying probability distribution. The perspective of updating a probability distribution is similar to the Wright--Fisher model.''

\Textcite{Paixao2015} concludes by demonstrating how existing ``classical models in theoretical population genetics and in the theory of evolutionary computation'' can be mapped into the framework of classified and categorized operators. Most of the various population genetics models can be represented; some topic-specific \gls{ea} models could not be, while genetic programming models were omitted for reasons of balance between ``simplicity and inclusiveness.'' However, \textcite{Paixao2015} is of limited relevance for this work as it is in effect a constitutive framework, providing a series of tests to classify existing models based on a general meta-model of evolution, rather than a causative model which is where our interest lies. Variation, in the form of reproduction and mutation, is only one element of the framework, and \textcite{Paixao2015} does not specifically address the emergence of heredity from variation.

%Most evolutionary models are seen to satisfy five mathematical properties:
%\begin{itemize}
%\item V1--variation (either mutation or recombination) is uniformity preserving: no change to distribution if uniformly distributed.
%\item M1--mutation acts on individuals.
%\item M2--mutation can generate the whole search space \ie an ergodic operator (the defining characteristic of mutations).
%\item R1--recombination preserves allele frequencies (in expectation).
%\item S1--selection doesn't change individuals.
%\end{itemize}

More specific models of inheritance and heredity can be found in three works comparing the adaptive value of acquired and non-acquired characteristics. In the first two works, \textcite{Jablonka1995,Paenke:2007ie}, the comparison is made with respect to an environment that alternates between two states, $E_0$ and $E_1$. Each environment is associated with a corresponding adapted phenotype $P_0$ and $P_1$, respectively.  In both works variation only affects the ratio of each phenotype in the population (directly in \textcite{Jablonka1995}, indirectly via a ``predisposition'' or tendency in \textcite{Paenke:2007ie}); no new phenotypes are created, and as inheritance is modelled only at a population level, that is without reproduction, we cannot use these models to investigate the emergence of heredity.

Inheritance however is modelled in \textcite{Gaucherel2012}, the third work, in relationship to individuals. Two models are presented, the first and simplest describing a non-spatial scenario conceptually similar to that in \textcite{Jablonka1995} and \textcite{Paenke:2007ie}, while the second examines a spatial world based on DaisyWorld \parencite{LovelockMargulis2011}. Focussing on the first and most relevant of the two models, reproduction is the middle of three repeated stages--first ``annihilation'' where the population size is adjusted to some practical level, then reproduction, and finally development. Concentrating on the second, reproduction, stage, individuals, represented by a single trait, or phenotype, value, are chosen for reproduction with some probability (based on the trait value), and the child given a trait value that slightly varies from the parent's value to model mutation (paraphrasing \textcite{Gaucherel2012}, $trait_{child} = trait_{parent} + \delta$, where $\delta$ is described as taken from a uniform distribution of given range around the parent's trait value. \footnote{although this appears to be an error and instead it should centered on $0$.})

The most relevant previous work is that of \textcite{Bourrat2015} who, in the course of examining the difference between evolution, natural selection and \gls{ens}, models the emergence of heredity in unchanging environments. \Textcite{Bourrat2015} demonstrates that imperfect inheritance is not compatible with \gls{ens} using an argument by contradiction \parencite[p.96]{Bourrat2015}: he lists the three conditions for a population to evolve solely by \gls{ens}, and then continues on to show that at least one of those conditions is incompatible with imperfect inheritance (as it happens, no production of new variants). The context is explicity in relationship to biology; his examples involve genes, traits, phenotypes and drift.

The six applicable models in \textcite{Bourrat2015} are designed to explore the implications of bias on inheritance; in \textcite{Bourrat2015}, unbiased means a trait is uncorrelated with parent, in practice as a random choice between lower and upper bound \parencite[p.153]{Bourrat2015}. Biased inheritance is naturally the opposite: there is some correlation between parent and child values for a trait, and so some prediction of traits is possible--a parent can ``pass it on''. Note that \textcite[p.173]{Bourrat2015} expressly notes that his ``biased inheritance'' is not the `transmission bias` of the second term in the Price equation: ``The bias in ‘biased inheritance’ is in reference to the type of the parent(s) (biased toward the type of the parent), while the bias in ‘transmission bias’ refers to a departure from an event of perfect transmission.''

%\Textcite{Bourrat2015}
%\begin{itemize}
%\item Persistors - unable to reproduce, selection only in ``weak'' sense of granite grains for hardness
%\item Procreators can reproduce but without inheritance of any property (including ability to procreate) except ``fact of coming into existence and membership of that class'' (class is class of parents defined by ``those properties that do not vary in the population''...{[}acknowledged as loose, but has benefit that no varying traits included{]}) p137. Procreator's offspring is persistors
%\item Minimal reproducers - indefinite procreation - where procreation can be transmitted from parent to offspring (with some low degree of fidelity)
%\item Unreliable reproducers (low bias for ability to procreate- ability to procreate randomly chosen between 0 and parent's ability), reliable reproducers (high bias - ability to procreate is same as parent's ability)
%\item Replicator - all traits (including procreation) can be inherited
%\end{itemize}

Model 1 begins with a population of 5000 ``persistors'' (that is without reproduction), each of which has a survival rate (viability) between 0-0.99 (the likelihood of surviving at each time step). Unsurprisingly, all eventually die. Model 2 introduces a single ``procreator'', capable of reproduction with both survival and fertility rates (offspring per unit time), into the population of persistors. The traits of the offspring of the procreator are uncorrelated (that is, unbiased inheritance) to those of the procreator. Again, all eventually die.

Model 3 begins to get interesting: \textcite{Bourrat2015} replaces the procreator by a ``minimal reproducer'' which differs from a procreator in that the ability to procreate is itself a heritable trait, although as ``minimal'' it is an unbiased trait. As such, the offspring of the minimal reproducer may be either minimal reproducers or persistors without the ability to reproduce. Now the population size drops then increases to maximum size with about 10\% of the population being minimal reproducers. However, the proportion of high fitness (that is, high viability) entities doesn't increase beyond about 0.05, so there is no cumulative adaptation.

Biased (in fact, perfect) inheritance of viability is introduced in Model 4; the offspring inherit the viability of their parent. The proportion of high fitness entities rapidly approaches the upper limit of $1.0$, as expected as high viability entities live longer and so produce more offspring while low viability entities die sooner and so produce less-- fertility random, but viability is heritable.

The most significant model is Model 5 which adds a variable ability to procreate to Model 4. At each mutation stage, there can be an increase or decrease in both the ability to transmit the ability to procreate, and in degree of bias (that is, relationship to parent's ability) in the ability to procreate.
The first defines the proportion of offspring of the parent who are themselves able to procreate; a low trait value for the parent means a low proportion of siblings can procreate, while bias represents the correlation between the parent and offspring's abilities to transmit procreation--low bias means the offspring's ability is only weakly correlated with parents ability. The initial population contains entities with viability in the full range $[0,1)$, an ability to procreate in $[0,0.2)$ and initial bias of $0$. Both the ability to procreate and the bias \emph{increase} over time in the population towards the upper limit of $1.0$. The model is asymmetric in with respect to the change of inheritance of ability to procreate: reductions lead to extinction of a line, while increases lead to increased population. Bourrat's conclusion is that an initial population of unreliable reproducers (a low proportion of procreating offspring, no bias) will evolve into one of reliable reproducers--that is, inheritance can emerge. For completeness, in Model 6 Bourrat demonstrates the same behaviour for the inheritance of another trait, that of viability.

There are some limitations to these otherwise insightful models. First, as seen by the trend of trait values towards the limit of $1.0$, the model assumes that the problem learnt by evolution is capable of perfect understanding, and that there is one and only one optimal solution. This is a corollary of the model design where fitness is absolute and unchanging--if fitness represents (as it does) an implicit relationship between an entity and its environment, then in Bourrat's model this relationship is also fixed and unchanging: evolution is omniscient with full visibility into the world. However, in the real world and in the artificial domains of interest, the relationship between entity and environment is less sure. The environment itself may either change, or be uncertain. Under these conditions it is unlikely that values of $1.0$ would be possible, or indeed helpful, as a perfect bias value effectively is removing any source of variation from the population. This is unexplored by Bourrat. 

Second, heredity and fitness (viability) are treated as independent traits. But the mechanism for heredity is the thing that copies the information that generates an offspring's traits, so in practice they are not independent.

\section{Hypothesis: Correlated Variation and Selection are sufficient for Inheritance}\label{h2}



\begin{DRAFT}
Inheritance comes from a copy mechanism under evolutionary control, such that the fidelity of the copy may
be varied by interaction with the environment.

Inheritance describes the similarity of an offspring to its parent, and depending on the context, can refer to the correlation for either a single trait or to a group of traits shared between offspring and parent (perhaps all of them.)

For the single trait case, when the correlation between the value for a parent's trait and an offspring's trait approaches the upper limit of 1.0 we say we have complete or full inheritance. Conversely, if there is no correlation (near the lower limit of 0) there is no inheritance and the entities are unrelated. We extend the measure to a group of traits simply by taking the average correlation of the group.
\end{DRAFT}
\begin{hypothesis}
Variation, where there is some initial correlation between generations for a property, and Selection are sufficient for maximal Inheritance ($V'+S\rightarrow I$)
\end{hypothesis}\label{hypothesis-2}
\begin{DRAFT}
we mean inheritance increases to some optimal level
Inheritance is defined as any correlation better than 0.5, or in other words, as better than chance. Fidelity is defined as the degree of correlation between parent and offspring values for the same trait.

I iff V' (that is, with initial correlation)
Can this be strengthened to include V+S necessary for I, or I-\textgreater{}V+S?


V+S-\textgreater{}I
Argument that heredity may in fact be a product of evolution rather than a precursor \autocite{Bourrat2015}

Heredity seen as method to maintain low entropy over much longer time than possible with non-biological systems: \quote{ Living systems can stay away from maximum entropy for much longer, indeed arbitrarily long (the biotic time scale is, for all we know, only limited by the existence of the biosphere). It is then this ability: to persist in a state of reduced entropy for biotic as opposed to abiotic time scales, that defines a set of molecules as living, and this set of molecules must achieve that feat via the self-replication of information.}{\autocite{Adami2015}}

``Context determines fitness'', where the environment is stable and affects the development of the entities

Degree of variation between generations important (no correlation means effectively unguided search, complete correlation means no source of novelties)

Problem is how can the optimal degree of variation (that is, inheritance) be established endogenously rather than as a model parameter?

Inheritance related to variation (between generations). Low variation implies high inheritance

Selection strengthens degree of inheritance

high fitness lineages more successful. Inheritance increases
correlation along lineage, so high fitness more likely to be passed
down. (Also low fitness, but they will suffer). On average then high
inheritance increases average fitness over time



\end{DRAFT}

\section{Predictions}\label{predictions}

In an unchanging environment, where the relative fitness of an entity, with respect to the environment, does not change over time, our expectations are:

\begin{enumerate}
\item Average inheritance will tend towards perfect inheritance.
Higher fitness entities will survive longer and reproduce more; higher fidelity reduces variation in fitness and so results in higher fitness being preferred.
\item Population variance for inheritance will decrease more than chance.
\end{enumerate}

Under changing environmental conditions, where the fitness of an unchanging entity changes in response to environmental changes, we expect:

\begin{enumerate}
\item The \gls{sd} of final fidelity under changing conditions \textgreater{} that under fixed conditions.
\item The higher the variability in the environment, the higher the \gls{sd} of \emph{Fidelity}.
\item \emph{Fidelity} at the end of a run under changing conditions \textless{} that under fixed conditions.
\item The final \emph{Fitness} will be in the the range described by the distribution applied in the $tweakFitness$ function.
\end{enumerate}

\section{Alternative explanations}\label{alternative-explanations-1}

The three main alternatives, examined later in \cref{elimination-of-alternative-explanations} are:

\begin{enumerate}
\item Variation alone is sufficient for Inheritance ($V\rightarrow I$.)
\item Selection alone is sufficient for Inheritance ($S\rightarrow I$.)
\item Variation and Selection, without trait or property correlation, is sufficient for Inheritance.
\end{enumerate}

\section{Base model}\label{base-model}

\epigraph{%
An important ``Platonic'' conception is that abstraction is a mental process by which properties are thought of as entities distinct from the concrete objects in which they are instantiated. On an alternative, Aristotelian conception, abstraction is the mental process of subtracting certain accidental properties from concrete objects so as to regard objects in a manner closer to their essential natures.}%
{\textsc{\\\textcite{Griesemer2005}}}

To test \cref{hypothesis-2}, we turn to simulation and experiment. Simulation is a natural fit for the exploration of emergent systems where we expect complex behaviour from simple rules. If a simulation that accurately represents the system in the hypothesis behaves in a way that matches our predictions, the hypothesis is supported. On the other hand, if the behaviour doesn't align with the predictions, our hypothesis will be rejected.

Experimental tests are the strongest method we have to examine the claims made in \cref{hypothesis-2}. As explored earlier in \cref{methods}, logical argument or theorem-proving is difficult to apply to model-based systems; thought experiments lack the strength we hope for, while experimentation is both feasible and, assuming correct design, rigorous.

Note that for those familiar with the design of experiments in the physical world, there are some differences in simulations with the most significant being the sources and understanding of experimental errors. In simulation, experimental runs are exactly reproduceable, absent any dependency on factors external to the simulation. Variation is explictly introduced usually through a random number generator, which can be seeded to produce the same sequence of numbers again and again. This means that the practice in real-world experiments of ``blocking'' to control external variation is not required in simulation experiments. However, \gls{replicate}s where the same combination of factor values is run several times each with a different random seed value, remains valuable, but in this case less to control for experimental error and more to record the variation across a series of runs and the sensitivity of the model to parameter settings.

All experiments in this part of the work make use of some variant of the same base model (\cref{base-model-algorithm}) where the key elements of the hypothesis, such as inheritance fidelity, are represented as explicit parameters. The main elements will be quite familiar to anyone from Evolutionary Computation or Evolutionary Biology, although the introduction of fidelity is novel and important to the overall thesis.

We define a population of entities, each with two properties -- \emph{fitness} and \emph{fidelity} -- and a set of population-transforming functions.

\begin{itemize}
	\item \emph{Fitness} represents the probability that an entity will survive and possibly also reproduce, and has the usual range for a probability of $[0,1]$.
	\item \emph{Fidelity} measures the correlation between the child's value for a property and the same property's value in the parent. The range is $[0,1]$ where a value of $0$ means that the value for a child's property has no correlation with its parent's value for that property. High \emph{fidelity} values mean high correlation, and when $fidelity = 1.0$ the child's value is identical to the parent's.  There is a subtle difference between fidelity and inheritance: inheritance is the result, while fidelity is the mechanism, expressed as the degree of correlation between two generations. Inheritance can be thought of as emerging or resulting from fidelity.
\end{itemize}

The specific relationship between parent and child property values is given by a single mapping, represented in the algorithm by the function $Derive$.

The only transformation functions in the base model are for the two core elements from the hypothesis, \emph{Selection} and \emph{Variation}, although in later sections we will add others to examine the sensitivity of the model to various influences.

Each \gls{run} of the model consists of a fixed number of time steps (generations), where at each step the functions are applied in a defined order to the current population to form a replacement population. Colloquially, the replacement population is formed by a combination of parents from the current population, plus their children.

The model is parameterized (see \cref{tbl:parameter_definitions}) so that different combinations of factors can be investigated for their influence on the model's behaviour.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/model}
\end{figure}

\begin{algorithm}
	\For{each generation $\in [1\dots$number of generations]}{
		$population\leftarrow Selection(population)$\;
		$population\leftarrow population \cup Variation(population)$\;
		\BlankLine
		\lIf{$population$ size is too small}{break}
	}
	\BlankLine
	\BlankLine
	\Def{Selection(population)}{
		$population_{new}\leftarrow \{\}$\;
		\For{each $entity \in population$} {
			\lIf{$p_{selection} = 0$}{$p\leftarrow $parent's fitness}
			\lElse{$p\leftarrow p_{selection}$}
			\Prob($p$:){
				Add $entity$ to $population_{new}$\;
			}
		}
		\Return $population_{new}$\;
	}
	\BlankLine
	\Def{Variation(population)}{
		$children\leftarrow \{\}$\;
		\For{each $entity$ in $population$}{
			\lIf{$p_{reproduce} = 0$} {$p\leftarrow $parent's fitness}
			\lElse{$p\leftarrow p_{reproduce}$}
			\BlankLine
			\Prob($p$:){
				\For{some number of children $\in \mathcal{U}[0,n_{children}]$} {
					$fitness\leftarrow$ Derive(parent's $fitness$, parent's $fidelity$)\;
					\uIf{Correlate Fidelity} {
						$fidelity\leftarrow$ Derive(parent's $fidelity$, parent's $fidelity$)\;
					}
					\uElse{
						$range\leftarrow$ some $x \in \mathcal{U}[0,1]$\;
						$fidelity\leftarrow$ Derive(parent's $fidelity$, $range$)\;
					}
					Create new $child$ with $fitness$ and $fidelity$\;
					Add $child$ to $children$\;
				}
			}
		}
		\Return $children$\;
	}
	\caption{Algorithm for the base model}\label{base-model-algorithm}
\end{algorithm}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/correlation}
\end{figure}

\begin{table}
	\begin{center}
		\caption{Model parameters}\label{tbl:parameter_definitions}
		\begin{tabular}{@{}llp{8cm}@{}}
			\toprule
			Parameter          & Value                                & Description                                                                                                                   \\
			\midrule
			$p_{reproduction}$ & $0$                                  & Probability of reproduction is given by the parent's $fitness$                                                                \\
                & $(0,1]$                              & Fixed probability of reproduction. The probability of reproduction for any entity is given by $p_{reproduction}$              \\
			$p_{selection}$    & $0$                                  & Probability of selection is given by the parent's $fitness$                                                                   \\
                & $(0,1]$                              & Probability of selection is unrelated to parent's $fitness$ and given instead by $p_{selection}$ for all entities             \\
			$n_{children}$     & $n_{children}\in \mathbb{Z}_{\ge 0}$ & Maximum number of children per parent                                                                                         \\
			%Restriction&			  	$(population, n)\mapsto population$&	Function to take $n$ elements from $population$\\
			Derive             & $[0,1]\times[0,1]\mapsto[0,1]$       & Function for generating a sample value $x$ from a distribution described by some measure of expected value and range          \\
			Correlate Fidelity & $\{\mathrm{true}, \mathrm{false}\}$  & Child's fidelity is related to parent's fidelity by parent's fidelity, or if false, by a random value $x\in \mathcal{U}[0,1]$ \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\section{Initial conditions and settings}\label{initial-conditions}

At the beginning of each run we construct an initial population to some design, randomly chosen (so as not to introduce any bias that might result from a consistent starting point.) Entities begin with a low initial fidelity and fitness chosen randomly from a uniform distribution over a particular range (see \cref{tbl:ic}.) From the hypothesis \cref{hypothesis-2}, and driven by the overall goal of this thesis, we expect to see a population of low fidelity entities eventually replaced by one of high (or at least higher) fidelity ones. This is analogous to a key step on the path taken in biology from the abiotic world, where early copy mechanisms lacked the capabilities for high-fidelity copying.

\begin{table}[t]
	\begin{center}
		\caption{Initial conditions}\label{tbl:ic}
		\begin{tabular}{@{}ll@{}}
			\toprule
			Property & Initial range\\
			\midrule
			Fitness  & $\mathcal{U}[0, 0.3]$\\
            Fidelity & $\mathcal{U}[0, 0.3]$\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\chapter{Parameter selection}

Translating model parameters into factors in the experiment design results in the factors in the first column of \cref{tbl:factor-levels}. As is usual with exploratory experiments with a number of parameters, where each run of the model has some cost in time or other resources, the key problem is to understand the relationship between parameters and response variables at an acceptable cost. In this case, our main cost is time - each run of an evolutionary model is cheap in resources but takes a little time. Exhaustively sampling the entire parameter space is unrealistic. Therefore, we first reduce the search space by limiting the number of values that each parameter can take. By choosing these values appropriately, we can construct an analysis model from the results that is sufficiently accurate for our exploratory purposes at a greatly reduced cost in time.

There are many approaches to this, but they mostly fall into two standard groups. First are response-surface methods which sample from the parameter space in a particular fashion to effectively construct an analysable function, or response-surface, from parameter values to response-variable values that approximates to some degree the behaviour of the model. The emphasis is on the shape of the response-surface; the parameter values are randomly chosen.

The usual alternative is some variant of a factorial design, where each parameter of interest is represented by a factor taking some small number of values, or levels (two levels being most common) and the analysis model constructed from runs that systematically work through a series of combinations of factors at different levels. The emphasis here is on the response given particular factor, and hence parameter, values.

\section{Experimental design}

As our interest is in the behaviour of the model, both overall and under specific conditions (such as \emph{Correlate Fidelity}), a factorial design is preferred.

Now, a full factorial design with seven 2-level factors would require testing $2^{7}$ combinations of factor values, or 128 sets of replicated runs, while a $2^{(7-3)}$ fractional factorial design \footnote{\eg  \url{http://www.itl.nist.gov/div898/handbook/pri/section3/eqns/2to7m3.txt}} can reduce this to 16 sets of replicated runs without loss of validity on the assumption that 3-factor interactions and higher are not significant. In other words, a $2^{(7-3)}$ design is sufficient to separate the main effect from any 2-factor interactions.  This seems a reasonable tradeoff between discriminatory power and the total number of runs required, given our exploratory goals.

The design is complicated a little by the suspicion that two factors - $p_{reproduction}$ and $p_{selection}$ - require more than two levels. Fortunately a $2^n$ fractional design can be extended relatively simply to include 4-level factors (\textcite[368]{Montgomery2009}) either by replacement where each 4-level factor is mapped to two 2-level ones, or, as we choose to do, by a hybrid full-fractional design, where use combinations of our four-level factors,  $p_{reproduction}$ and $p_{selection}$, for two of the two-level factors, X1 and X2, of the standard $2^{(7-3)}$ design. This may not be quite as computationally efficient as a complete mixed-levels fractional factorial design but is efficient enough and meets our purposes.

We use a fractional factorial design to reduce the number of experimental runs required when compared to a full factorial design \autocite{Montgomery2009}. Although the number of runs is reduced, as each level of each factor occurs the same number of times in the results, the design remains ``balanced'' in the statistical sense, greatly easing analysis.

The number of \glspl{replicate} required for a particular statistical power is related to the \gls{sd} of the response variable.

\TODO{Based on preliminary runs, we begin with an initial estimate of 10 replicates for each combination of factor values given in the design; we confirm that this is sufficient through power calculations for specific tests where required in the later sections.}

\section{Factors and levels}\label{factors-and-levels}

Although several parameters in the model are continuous, at this stage the main ones can be reduced to a set of A/B alternatives, or two-level factors.

Of the others, $p_{reproduction}$ and $p_{selection}$ take four levels each to cover a reasonable range given the usual sensitivity of evolutionary models to these type of parameter; the value $0$ for the corresponding model parameter in each case means ``use parent's value'', and so is a special case. And as discussed in \cref{upper-size-bound}, \emph{Population Restriction} is held to Fitness-independent sampling.

The factors and levels used are given in \cref{tbl:factor-levels}.

\begin{table}
	\begin{center}
		\caption{Factor levels for investigation into inheritance}\label{tbl:factor-levels}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Factor                 & Number of Levels & Levels\\
			\midrule
			$p_{reproduction}$     & 4                & 0 or 0.33 or 0.66 or 1.0\\
			$p_{selection}$        & 4                & 0 or 0.33 or 0.66 or 1.0\\
			$n_{children}$         & 2                & 2 or 5\\
			Population Restriction & 1                & Fitness-independent sampling\\
			Distribution           & 2                & Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$%
														\footnote{But see discussion in \cref{screening-distribution} as to the implementation}\\
			Correlate Fidelity     & 2                & false or true\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\section{Probability of selection and probability of reproduction}

We take these together as they interact. Where one or both parameters equal zero (use the parent's fitness as the probability of selection or probability of reproduction), the model is similar to a standard EA; any other value means selection or reproduction occurs with a fixed probability irrespective of fitness.

Why would we be interested in runs under these conditions? Imagine a run where selection has the value $1.0$ while reproduction takes value $0$ - parents always survive to the next generation (subject to any population limits naturally), while producing children at each generation with a probability proportional to their fitness. Or reverse these values ($p_{selection} = 0$ while $p_{reproduction} = 1.0$), giving a run where parents always produce a fixed number of $n_{offspring}$ children at each generation, but their own survival rate is proportional to their fitness.

The combination where both $p_{selection}$ and $p_{reproduction}$ are non-zero leads to uninteresting behaviour as the major source of variation has been removed from the model.

\section[Upper size bound]{Effect of upper bound on population size}\label{upper-size-bound}

In the absence of any restrictions on population size, there is nothing to prevent a population growing beyond the capacity of the simulation system. This is a practical problem rather than a property of the theoretical model, and so to have faith in the simulation and model it's important we eliminate the possibility of introducing bias to the results from the mechanism used to control population size.

The size of the population is driven by how population elements are introduced and removed. In standard Evolutionary Computation (\eg \textcite[50]{DeJong2006}) the choice of strategy is important to the performance and outcomes of the algorithm. New elements can be straight replacements, like-for-like, of their parent, or be placed in competition against elements in the parent population, or completely replace the parent population. Elements may be removed as a result of selection, or through fitness-independent sampling to maintain a particular population size, or through some end-of-life calculation. The population size limit may act as both upper and lower bound on population size to maintain a specific size, or solely as upper bound.

Similar considerations apply to our model. Because we observe that the population size increases exponentially in many experimental runs (e.g., righthand side of \cref{fig:unboundedplot}), some restriction on population size is needed. In \textcite{Gaucherel2012} the approach is to remove individuals from the population stochastically, with probability related to $e$ to the negative power of the population size multiplied by a configurable parameter, $\mu$. In the ``canonical'' Evolutionary Computation algorithm, a population limit results from selection where a set number of elements is extracted from the original population, with elements chosen by one of a wide range of selection algorithms (among many sources, see overviews in \textcite[sect. 4.3.1]{DeJong2006} and \textcite[sect. 4.2]{Vose:1999di}.) Here though we break the selection function from the population size limit in order to qualify the effect of the specific limiting mechanism used.

To summarize then, the goal of this section is to:

\begin{enumerate}
	\item Confirm that an upper bound on population size is required,
	\item Decide if the choice of method for maintaining the bound might significantly affect any conclusions from the hypothesis tests, and if it might,
	\item Determine which method to use for the remaining experiments.
\end{enumerate}

If there is an affect on the hypothesis tests attributable to the bound mechanism, then the conclusions from the test become contingent on the method. This reduces the scope somewhat, but without a change of investigative approach seems unavoidable.

\subsection{Effect of population limits}

% EXPERIMENT 1 - UNBOUNDED POPULATIONS
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results.simple('results/results-d9f18e2.data'), environment_change_frequency==0)
@

The environment is considered ``fixed'' (that is, each element's fitness value does not change); the initial population consists of \Sexpr{df[1,]['pop']} elements, and the model run with a maximum of \Sexpr{max(df['gen'])} generations with factors as follows:

\begin{table} % d9f18e2
	\begin{center}
		\caption{Factor levels for investigation into the need for an upper-bound mechanism}
		\label{tbl:factors-levels-for-upper-bound-investigation}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter              & Number of Levels & Levels                                                       \\
			\midrule
			$p_{reproduction}$     & 2                & 0 or 1.0                                                     \\
			$p_{selection}$        & 2                & 0 or 1.0                                                     \\
			$n_{children}$         & 2                & 2 or 5                                                       \\
			Population Restriction & 2                & No upper-bound, or Fitness-independent sampling              \\
			Distribution           & 2                & Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$ \\
			Correlate Fitness      & 2                & false or true                                                \\
			Correlate Fidelity     & 2                & false or true                                                \\
			Starting population    & 1                & Low-start settings                                           \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<unboundedplot, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='On the left, a histogram of number of generations achieved by an unbounded algorithm before either i) the population size increases beyond 10x the original population size, or ii) the simulation succeeds in reaching the expected number of generations (which never occurred in an unbounded population). On the right, the growth of population size grouped by replicate (that is, by common factor levels) showing rapid population growth before reaching the 10x practicality limit.'>>=
ap <- ggplot(aggregate(df$gen,Filter(is.factor,df),max), aes(x))+geom_histogram(binwidth=1,boundary = .5, fill='gray70') + labs(x="Final generation", y="Count of Runs") + scale_x_continuous(breaks=1:12) + scale_y_continuous(limits=c(0,2),breaks=c(0,1,2))

bp <- ggplot(df,aes(x=gen,y=pop,group=interaction(p_reproduce,p_selection,fitness_correlation,correlation_correlation,truncate,distribution))) + geom_line() + geom_point() + scale_x_continuous(breaks=1:100) + labs(x="Generation", y="Population size")
grid.arrange(ap,bp,nrow=1,ncol=2)
@

Under these initial conditions, no experiment without an upper population bound continued for more than a handful of generations before the population size reached more than ten times the initial size. It is clear from \cref{fig:unboundedplot} that some form of upper bound is necessary.

\subsection[Choice of limit mechanism]{Is the choice of limit mechanism significant?}

Given then that an upper bound is needed for practicality, this section describes two possible approaches to the implementation of the \emph{Restriction} function in \cref{upper-bound-model-algorithm} and attempts to understand the form of the bias each introduces into the experimental results.

\begin{algorithm}
	\For{each generation $\in [1\dots$number of generations]}{
		$population\leftarrow Selection(population)$\;
		$population\leftarrow population \cup Variation(population)$\;
		$population\leftarrow Restriction(population)$\;
		\BlankLine
		\lIf{$population$ size is too small}{break}
	}
	\caption{Algorithm for the Model with an upper bound on population size. The \emph{Restriction} function is the only difference with the base model.}\label{upper-bound-model-algorithm}
\end{algorithm}

The first approach is to take a random sample of $n$ elements from the population, without regard for fitness (that is, fitness-independent sampling), while the other is to adopt the \emph{truncation} mechanism (described in \textcite[124]{DeJong2006} alongside others) as representative (although strongly elitist) of a fitness-based mechanism (henceforth called fitness-based selection).

It might be argued that \emph{truncation} is too extreme to make a fair comparison. However, without an accepted scheme to order selection algorithms along a dimension of interest, such as selection pressure, it's hard to justify oversetting it by an alternative. The key criteria is whether the conclusions drawn from any particular algorithm can be extended to a more general conclusion, independent of the specifics of the algorithm or algorithms used. The specificity of selection algorithms means this a difficult argument to make, and most likely only possible by examining a number of them, which is beyond the scope of this work.

Returning then to our examination of fitness-independent and fitness-based upper bounds against the unlimited control case, the null, H$_0$, and alternative hypothesis, H$_1$, are as follows (examined for both population mean fitness and fidelity) :

\begin{itemize}[label={}]
	\item H$_0$: the two samples are drawn from the same continuous distribution
	\item H$_1$: the two samples are not drawn from the same continuous distribution
\end{itemize}

\subsubsection{Fitness-independent sampling}\label{fitness-independent-sampling}

The bound is implemented by a fitness-independent sampling of $n$ elements from the population (as given in the $Restriction$ function in \cref{upper-bound-model-algorithm}), if and only if the pre-sampling population size is greater than $n$.

\begin{function}
	\SetKwProg{Def}{def}{:}{}
	\Def{Restriction(population,n)}{
		\Return $\text{random sample of }n\text{ elements from }population$\;
	}
	\caption{Restriction (Fitness-independent sampling)()}
\end{function}

%If we compare fitness-independent sampling to the control case without limits, we conclude that fitness independent sampling does indeed make a difference to the results, confirming the earlier visual assessment from the box and \gls{qq} plots (\cref{}.) This is somewhat unexpected as the average fitness before and after the application of the mechanism is, within sampling error, unchanged. It seems that the difference between limited and unlimited populations might come instead from the differences in population size in the selection and reproduction steps of the model rather than from any fitness-modifying actions of the restriction mechanism.

\subsubsection{Fitness-based selection}

The fitness-based population limit is based on \emph{truncation} from \textcite[124]{DeJong2006}, chosen as it is reasonably representative of methods used in Evolutionary Algorithms, and as a highly-elitist algorithm should provide useful contrast to the effectively uniform mechanism of \cref{fitness-independent-sampling}. If the two mechanisms produce similar results then it might be argued that other mechanisms are likely to be similar also.

It also recognizes that in both Evolutionary Computation and natural populations, the population limit is a function of the carrying capacity of the environment and fitness determines the selection.

\begin{function}
	\SetKwProg{Def}{def}{:}{}
	\Def{Restriction(population,n)}{
		$sortedPopulation\leftarrow$ sorted population by element fitness, in decreasing order \;
		\Return $\text{first }n\text{ elements from }sortedPopulation$\;
	}
	\caption{Restriction (Fitness-based selection)()}
\end{function}

\subsection{Experimental design}


\begin{table} % 53f6b74
	\begin{center}
		\caption{Factor levels for the choice of an upper-bound mechanism}
		\label{tbl:factor-levels-for-the-choice-of-an-upper-bound-mechanism}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter              & Number of Levels & Levels                                                       \\
			\midrule
			$p_{reproduction}$     & 4                & 0 or 0.33 or 0.66 or 1.0                                     \\
			$p_{selection}$        & 4                & 0 or 0.33 or 0.66 or 1.0                                     \\
			$n_{children}$         & 2                & 2 or 5                                                       \\
			Population Restriction & 2                & Fitness-independent sampling, or Fitness-based selection     \\
			Distribution           & 2                & Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$ \\
			Correlate Fitness      & 2                & false or true                                                \\
			Correlate Fidelity     & 2                & false or true                                                \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

% EXPERIMENT 2 - CHOICE OF LIMIT MECHANISM
<<echo=FALSE, cache=TRUE>>=
df <- subset(load.results.simple('results/results-53f6b74.data'), environment_change_frequency == 0)
runs <- nrow(subset(df,gen==0))
factors <- nrow(unique(Filter(is.factor,df)))
replicates <- runs / factors
df_completed <- subset(df, gen==500)
@

Data is from \Sexpr{nrow(subset(df,gen==0))} runs -- \Sexpr{factors} unique sets of factors, each with \Sexpr{replicates} replicates -- with settings given in \cref{tbl:factor-levels-for-the-choice-of-an-upper-bound-mechanism} under fixed conditions, of which we are exclusively concerned with the subset of \Sexpr{nrow(df_completed)} runs that reached completion at \Sexpr{max(df['gen'])} generations.

\subsection{Results and discussion}

The runs that reached completion are evenly distributed between the two methods, fitness-independent (\Sexpr{nrow(df_completed[df_completed['truncate']==0,])} runs) and fitness-based (\Sexpr{nrow(df_completed[df_completed['truncate']==1,])} runs), indicating that the two methods at least appear comparable with respect to their affect on population size.

<<limitcomparisonplots, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Summary plots for fitness-independent sampling ("Independent") and fitness-based or dependent selection ("Dependent"), where the top line contains density plots and the bottom line boxplots showing ranges for both final mean fidelity (on the left-hand side) and final mean fitness (on the right).'>>=
df_completed$truncate <- factor(df_completed$truncate, labels=c("Independent","Dependent"))
ap <- qplot(ave_cor, facets=truncate ~ ., geom="density", data=df_completed, xlab="Fidelity", ylab="Density")
bp <- qplot(ave_fit, facets=truncate ~ ., geom="density", data=df_completed, xlab="Fitness", ylab="Density")
cp <- qplot(truncate, ave_cor, geom="boxplot", data=df_completed, xlab="",ylab="Fidelity")
dp <- qplot(truncate, ave_fit, geom="boxplot", data=df_completed, xlab="",ylab="Fitness")
grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
@

<<limitqqplots, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='\\Gls{qq} plots comparing fitness-independent and fitness-based or dependent results for fidelity (left) and fitness (right).'>>=

myqqplot <- function(x,y,...) {
sx <- sort(x)
sy <- sort(y)
lenx <- length(sx)
leny <- length(sy)
if (leny < lenx)
sx <- approx(1L:lenx, sx, n = leny)$y
if (leny > lenx)
sy <- approx(1L:leny, sy, n = lenx)$y
qplot(sx,sy,...)
}

df_independent <- subset(df_completed,truncate=="Independent")
df_dependent <- subset(df_completed,truncate=="Dependent")
ap <- myqqplot(sort(df_independent$ave_cor), sort(df_dependent$ave_cor), main="Final mean fidelity", xlab="Independent",ylab="Dependent")
bp <- myqqplot(sort(df_independent$ave_fit), sort(df_dependent$ave_fit), main="Final mean fitness", xlab="Independent",ylab="Dependent")
grid.arrange(ap,bp,nrow=2,ncol=1)
@

%Standard non-parametric tests include Wilcoxon and Mann-Whitney for comparing two
%independent continuous random samples where the underlying distributions
%are known to have essentially the same shape, or a Friedman test where the samples might be related (\eg in block %experiment designs) and the objective is to distinguish differences between treatments.

<<kstest, warning=FALSE, pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
x_cor <- ks.test(df_independent$ave_cor, df_dependent$ave_cor)
x_fit <- ks.test(df_independent$ave_fit, df_dependent$ave_fit)
@

Applying a two-sample Kolmogorov-Smirnov test to determine if the results for each method are taken from the same distribution provides strong evidence (for population mean fidelity, approx. p-value=\Sexpr{round(x_cor$p.value,2)} and for pop. mean fitness, approx. p-value=\Sexpr{round(x_fit$p.value,2)}) to reject the null hypothesis, confirmed by visual inspection of \cref{fig:limitcomparisonplots} and \cref{fig:limitqqplots}. The distributions produced by the two methods are different for both average fidelity and average fitness.

From these tests it is clear that the two methods do not have statistically similar effects for both fidelity and fitness, and hence we cannot find support for the contention that our conclusions in the hypothesis tests can be made without extensive consideration of limit method. Method is important, and our conclusions are dependent on the method chosen.

This conclusion of course is based on a comparison of two particular methods; a third, fitness-based, method may produce distributions that are statistically similar to those of, say, sampling. In which case we might conclude that any conclusions drawn on the basis of sampling would also extend to this third method. This extension however is left for future work.

Returning to our goal, as the choice of method affects our results and for practical reasons we must make a choice, it seems reasonable to choose a method that has as small an influence as possible on the conclusions we draw from our hypothesis tests. As our tests involve fitness, a method that is by design orthogonal to fitness is the better choice. A sampling method is a better fit for this than a selection method, which biases on fitness, and therefore we adopt fitness-independent sampling rather than fitness-based restriction.

\section[Lower size bound]{Lower limit on population size}\label{lower-size-limit}

% EXPERIMENT - Lower size bound

<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results.simple('results/results-53f6b74.data'), environment_change_frequency == 0 & truncate==0)
@

\begin{table} % 53f6b74
	\begin{center}
		\caption{Factor levels for the choice of a lower-bound mechanism}
		\label{tbl:factor-levels-for-the-choice-of-a-lower-bound-mechanism}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter              & Number of Levels & Levels                                                       \\
			\midrule
			$p_{reproduction}$     & 4                & 0 or 0.33 or 0.66 or 1.0                                     \\
			$p_{selection}$        & 4                & 0 or 0.33 or 0.66 or 1.0                                     \\
			$n_{children}$         & 2                & 2 or 5                                                       \\
			Population Restriction & 2                & Fitness-independent sampling, or Fitness-based selection     \\
			Distribution           & 2                & Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$ \\
			Correlate Fitness      & 2                & false or true                                                \\
			Correlate Fidelity     & 2                & false or true                                                \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

We now turn to the other bound, and the impact of population extinctions. Runs in which the population dies out are incompatible with ongoing evolution, and so should be separated from other, sustainable, runs in the analysis. This situation does arise in our simulation: not all runs in the experiment described in \cref{upper-size-bound} completed the target of \Sexpr{max(df['gen'])} generations (\cref{fig:popoverall}), and of those that did, not all completed with a sustainable population size.

<<popoverall, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Not all runs in fixed conditions can maintain a substantial population size over time. The horizontal line at the top of the figure is from runs that were capped by the population upper-bound, while the great majority of unsustainable runs fall within the asymptotically decreasing band from top-left to bottom-right. Note the extreme case represented by the almost vertical set of points to the far left, discussed in the text.'>>=
ggplot(df) + geom_point(aes(x=gen,y=pop),size=0.25) + labs(x="Generation", y="Population size")
@

Of the \Sexpr{nrow(subset(df,gen==0))} experiment runs, \Sexpr{nrow(subset(df,gen==0))-nrow(subset(df, gen==500))} fail to reach completion, and \Sexpr{english(nrow(subset(df, gen==500 & pop < 4000)))} complete with a population size substantially smaller ($\leq$ \Sexpr{max(subset(df, gen==500 & pop < 4000)['pop'])}) than the population size in the other completed runs. In fact, all other runs reach completion with a population size at the upper bound limit of \Sexpr{min(subset(df, gen==500 & pop >=4000)['pop'])}.

From the clustering evident in \cref{fig:popoverall} we assert that the experiment runs can be separated into two disjoint sets - those that reach completion with a population size at or very near the upper-size limit, and those whose population is tending towards zero. Our interest lies only in sustainable populations and therefore from here on we exclude all runs where the population does not reach completion at or near the upper-size limit, without loss of generality.

%Inspecting the results by factor:
%\begin{enumerate}
%	\item All runs with $p_{reproduction}=0.33$ and $n_{children}=2$ fail to complete.
%	\item All runs that complete, but with an unsustainably small population, also include $p_{selection}=0.66$.
%\end{enumerate}
%
%To conclude this brief examination of lower-bounds, runs with factors associated with population extinction ($p_{reproduction}=0.33$, $n_{children}=2$ and $p_{selection}=0.66$) will be omitted from further analysis.

\TODO{explanation for extreme left case}

Note that these results also provide support for an experiment duration of \Sexpr{max(df['gen'])} generations - either a run maintains a stable population (and so the duration is unrelated to population size considerations), or if a population is to go extinct it does so by \Sexpr{max(df['gen'])} generations or soon after and so can easily be identified for special treatment.

\section{Number of offspring}
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results.simple('results/results-53f6b74.data'), environment_change_frequency == 0)
@

\begin{table} % 53f6b74
	\begin{center}
		\caption{Factor levels for the investigation into number of offspring}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter              & Number of Levels & Levels                                                       \\
			\midrule
			$p_{reproduction}$     & 4                & 0 or 0.33 or 0.66 or 1.0                                     \\
			$p_{selection}$        & 4                & 0 or 0.33 or 0.66 or 1.0                                     \\
			$n_{children}$         & 2                & 2 or 5                                                       \\
			Population Restriction & 2                & Fitness-independent sampling, or Fitness-based selection     \\
			Distribution           & 2                & Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$ \\
			Correlate Fitness      & 2                & false or true                                                \\
			Correlate Fidelity     & 2                & false or true                                                \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<popoffspring, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Population size over time for two values of parameter $n_{children}$ - 2 (on the left) and 5 (on the right) .'>>=
temp <- subset(df,n_offspring==1 & pop < 5000)
df$n_offspring = factor(df$n_offspring, labels=c("2","5"))
ggplot(df) + geom_point(aes(x=gen,y=pop), size=0.25) + labs(x="Generation", y="Population size") + facet_wrap(~n_offspring)
@

The vertical line on the extreme left of the right-hand side facet of \cref{fig:popoffspring} (for $n_{offspring} = 5$) consists of \Sexpr{nrow(subset(temp,gen==1))} runs from a total of nrow(subset(df,gen==0)) where the population dropped in the first generation to a mean size of \Sexpr{mean(subset(temp,gen==2)$pop)}, before recovering to the population upper limit by generation \Sexpr{max(temp['gen'])+1}. This is a characteristic of the initial conditions: runs where $n_{offspring} = 2$ and the probability of reproduction or selection is low, either because the parameter has a low, fixed, value or because they are derived from the parent's value, suffer high selection and low reproduction initially. Taking the case where both are given by the parent's value, $p_{reproduction} = p_{selection} = $\Sexpr{mean(subset(df,gen==0)$ave_fit)}. The difference between the two facets can be explained by the higher reproduction rate in the right-hand facet.

A similar effect can be seen in the left-hand side facet for $n_{offspring} = 2$ where some of the affected runs recovered while others went to extinction.

\section{Fidelity correlation}

\emph{Fidelity correlation} is parameterized in order to test the hypothesis that correlation is required for inheritance; the alternative hypothesis that inheritance can arise without correlation corresponds to \emph{Fidelity correlation} being false.

\section{Derive parameter}\label{screening-distribution}
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results.simple('results/results-53f6b74.data'), environment_change_frequency == 0)
@

\begin{table} % 53f6b74
	\begin{center}
		\caption{Factor levels for the investigation of Derive}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter              & Number of Levels & Levels                                                       \\
			\midrule
			$p_{reproduction}$     & 4                & 0 or 0.33 or 0.66 or 1.0                                     \\
			$p_{selection}$        & 4                & 0 or 0.33 or 0.66 or 1.0                                     \\
			$n_{children}$         & 2                & 2 or 5                                                       \\
			Population Restriction & 2                & Fitness-independent sampling, or Fitness-based selection     \\
			Distribution           & 2                & Gaussian dist., $\mathbb{N}$ or Uniform dist., $\mathcal{U}$ \\
			Correlate Fitness      & 2                & false or true                                                \\
			Correlate Fidelity     & 2                & false or true                                                \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<gaussian, pdfcrop=TRUE, echo=FALSE, cache=TRUE, out.width='0.45\\linewidth',fig.show='hold'>>=
df$distribution = factor(df$distribution, labels=c('Gaussian','Uniform'))
ggplot(df) + geom_boxplot(aes(x=distribution,y=ave_cor)) + labs(x="Distribution", y="Final mean fidelity")
ggplot(df) + geom_boxplot(aes(x=distribution,y=ave_fit)) + labs(x="Distribution", y="Final mean fitness")
@

The \emph{Derive} parameter describes a function to return a value in the range $[0,1]$ given two parameters, also in the range $[0,1]$. Obvious candidates for \emph{Derive} include probability distributions, where the function samples a value from a distribution. If the distribution is gaussian, the two parameters to \emph{Derive} would describe its mean and variance.  

From a visual inspection, we conclude that the results appear very similar for Uniform and Gaussian distributions.

\section{Conclusions}
\begin{DRAFT}
Independent variable is Fidelity correlation, the main factor of interest from hypothesis

Dependent variables, to qualify the sensitivity of the model:
\begin{enumerate}
	\item Distribution
	\item Number of offspring
\end{enumerate}

Set remaining parameters based on these screening experiments:
\begin{enumerate}
	\item Upper-size bound - sampling % truncate == 0
	\item Lower-size bound - sustainable populations % gen for run == max gen, and pop at max gen > 1000
	\item Probabilities - at least one of $p_{reproduction}$ and $p_{selection}$ is related to the parent's fitness (\ie the factor level has the special case value of 0.) % 
	      	
 % All runs that complete - that is, where gen == max(df$gen) & pop > 1000 or subset(df,gen==500 & pop>1000)
 % runs <- subset(df, gen==max(df$gen) & pop>1000)[,'run']
 % subset(df, truncate == 0 & run %in% runs & (p_reproduce==0 | p_selection == 0))
 % Use subset(df,correlation_correlation==1) by hypothesis, correlation_correlation==0 is control
	      	
\end{enumerate}
\end{DRAFT}
\chapter[Model behaviour in stable conditions]{Experimental test of hypothesis under stable conditions}\label{experimental-test-of-h2-under-fixed-conditions}

Returning to the overall goals for these experiments (to test the predictions of hypothesis \cref{hypothesis-2}, and to examine the impact of the factors on the results), hypothesis \cref{hypothesis-2} makes two predictions for fixed environments:

\begin{enumerate}
	\item Average inheritance will tend towards perfect inheritance, and
	\item Population variance for inheritance will decrease more than chance.
\end{enumerate}

The first test therefore is to examine if inheritance emerges from low-fidelity and low-fitness initial conditions, and then test whether the population variance for inheritance decreases as predicted. As discussed earlier, inheritance is the outcome of the relationship between parent and child traits, as represented by the measure of fidelity.

% EXPERIMENT 3 - Fidelity approaches 1.0

We start with the following null and alternative hypotheses:

\begin{itemize}[label={}]
	\item H$_0$: fidelity does not approach 1.0 during a run, irrespective of factor values, or \newline
 $\vert \overline{fidelity}_{end}-\overline{fidelity}_{start} \vert = 0$
	\item H$_1$: fidelity increases to near 1.0 during a run, for some factor values, or \newline
 $\overline{fidelity}_{end}-\overline{fidelity}_{start} > 0$ and $1.0-\overline{fidelity}_{end} < \delta$ for some $\delta$ and for some factor values.
\end{itemize}

\section{Response variables}\label{response-variables}

From the predictions of the hypothesis (\cref{predictions}), the main property of interest is \emph{fidelity}, or the correlation between parent and child property values. \emph{Fidelity} therefore is our response variable.
\\

Specifically we use $\overline{fidelity}_{end}$, or the mean value for \emph{fidelity} (across all replicates) at the end of a run, as under hypothesis \cref{hypothesis-2} we expect $\overline{fidelity}_{end}$ to approach 1.0 in an unchanging environment.

\section{Design}\label{design}
<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df_full <- load.results('results/results-819350e.data')
df <- subset(df_full, ecf==0 & truncate == 0 & (p_reproduce==0 || p_selection == 0))
@

\begin{table} % 5e33a27 and 819350e
	\begin{center}
		\caption{Factor levels for testing the hypothesis prediction of perfect inheritance in unchanging conditions}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter              & Number of Levels & Levels                                                   \\
			\midrule
			$p_{reproduction}$     & 2                & 0 or 0.66                                                \\
			$p_{selection}$        & 2                & 0 or 0.66                                                \\
			$n_{children}$         & 2                & 2 or 5                                                   \\
			Population Restriction & 2                & Fitness-independent sampling or fitness-based truncation \\
			Distribution           & 2                & Gaussian dist., $\mathbb{N}$ or folded Gaussian          \\
			Correlate Fidelity     & 2                & false or true                                            \\
			Shape of environment change		2&	Abrupt or continuous\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\section{Results and discussion}

Our interest is in the final values for fidelity under fixed-environment conditions. Therefore, data is from the final generation of those fixed-environment runs that reached completion at generation \Sexpr{max(df['gen'])} from the low-start dataset; a total of \Sexpr{nrow(unique(df['run']))} runs out of the full dataset of \Sexpr{nrow(unique(df_full['run']))} runs.

<<lowstart, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Summary results, showing on the left-hand side an overview for the final mean fidelity ($\\overline{fidelity}_{end}$) for each run and a density plot showing the distribution of $\\overline{fidelity}_{end}$. The right-hand side shows the corresponding plots for final mean fitness ($\\overline{fitness}_{end}$)'>>=
ap <- qplot(row.names(df),ave_cor, geom="point", data=df, xlab='Experiment run',ylab="Final ave. fidelity") + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
bp <- qplot(row.names(df),ave_fit, geom="point", data=df, xlab='Experiment run',ylab="Final ave. fitness") + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank())
cp <- qplot(ave_cor, geom="density", data=df, xlab="Final ave. fidelity",ylab="")
dp <- qplot(ave_fit, geom="density", data=df, xlab="Final ave. fitness",ylab="")
grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
@

<<lowstartbyfactor, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Summary results, this time averaged across all replicates of each factor combination (that is, grouped by replicate). Dark-coloured points represent values at end of run; light-coloured points are for initial values. Missing points are from those replicates that did not complete.'>>=
ap <- ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=ave_cor, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='Factor combination', y='Mean fidelity') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
bp <- ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=ave_fit, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='Factor combination', y='Mean fitness') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
cp <- ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=sd_cor, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='', y='SD fidelity') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
dp <- ggplot(subset(df,gen==0 | gen==500)) + geom_boxplot(aes(x=factor(exp),y=sd_fit, colour=factor(gen))) + scale_colour_manual(name="", values=c("grey80", "grey10")) + labs(x='', y='SD fitness') + theme(axis.ticks.x=element_blank(), axis.text.x = element_blank(), legend.position='none')
grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
@

<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df <- subset(load.results('results/results-5e33a27.data'), ecf==0 & truncate == 0 & (p_reproduce==0 || p_selection == 0))
@

The first hypothesis prediction is that average fidelity will tend towards exact inheritance, or $1.0$. A simple visual inspection of this data in \cref{fig:lowstart} reveals that the fidelity measure is distinctly bimodal, with peaks around final mean fidelity values 0.5--0.6 and 1.0. Fitness is also bimodal, but less so than fidelity. From inspection, it seems clear that fidelity does approach 1.0 for some combination of factor levels.

<<lowstartfactors, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Comparison between correlated and uncorrelated values for the factor \\textbf{Correlate Fidelity} showing results for final mean fidelity ($\\overline{fidelity}_{end}$, on the left) and final mean fitness ($\\overline{fitness}_{end}$, on right)'>>=
df$correlation_correlation <- factor(df$correlation_correlation, labels=c('false','true'))
ap <- ggplot(df) + geom_boxplot(aes(x=correlation_correlation,y=ave_cor)) + labs(x='Final mean fidelity',y='')
bp <- ggplot(df) + geom_boxplot(aes(x=correlation_correlation,y=ave_fit)) + labs(x='Final mean fitness',y='')
grid.arrange(ap,bp,nrow=1,ncol=2)
@

From \cref{fig:lowstartbyfactor}, all runs result in an increase in fidelity from the initial range of $[0, 0.3]$ but only some approach or reach 1.0. Those that do are uniformly associated with the \textbf{Correlate Fidelity} factor value of \emph{true} (see top-left plot in \cref{fig:lowstartfactors}), and those that did not had a \textbf{Correlate Fidelity} value of -1.

In conclusion, H$_0$ can be rejected, and H$_1$ accepted. Inheritance increases regardless of the model design, but is strongest when \textbf{Correlate Fidelity} is \emph{true}, that is, when the child's fidelity is correlated with that of its parent.

% EXPERIMENT 5 - SD of Fidelity approaches 0

The second prediction of the hypothesis is that population variance for inheritance ($\sigma_{fidelity}$) should decrease over time towards a limit of $0$ in fixed environments.

\begin{itemize}[label={}]
	\item H$_0$: $\sigma_{fidelity_{end}}-\sigma_{fidelity_{start}} >= 0$, for all factor values.
	\item H$_1$: $\sigma_{fidelity_{end}}-\sigma_{fidelity_{start}} < 0$, for some factor values.
\end{itemize}

We use the same experimental setup and data as for the first hypothesis test in \cref{tbl:factor-levels}.

<<pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
df_full <- load.results('results/results-819350e.data')
df <- subset(df_full, ecf==0 & truncate == 0 & (p_reproduce==0 || p_selection == 0))
@

\begin{table} % 5e33a27 and 819350e
	\begin{center}
		\caption{Factor levels for testing the hypothesis prediction that the population variance of inheritance should reduce to nothing in unchanging environments}
		\begin{tabular}{@{}llp{6cm}@{}}
			\toprule
			Parameter              & Number of Levels & Levels                                                   \\
			\midrule
			$p_{reproduction}$     & 2                & 0 or 0.66                                                \\
			$p_{selection}$        & 2                & 0 or 0.66                                                \\
			$n_{children}$         & 2                & 2 or 5                                                   \\
			Population Restriction & 2                & Fitness-independent sampling or fitness-based truncation \\
			Distribution           & 2                & Gaussian dist., $\mathbb{N}$ or folded Gaussian          \\
			Correlate Fidelity     & 2                & false or true                                            \\
			Shape of environment change		2&	Abrupt or continuous\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<lowstartranges1, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Smoothed ranges over time of the standard deviation of fidelity (top) and fitness (bottom) for all levels of all factors, broken out by level of \\emph{Correlate Fidelity}. Lines that drop below zero are an artifact of the line smoothing algorithm used (local polynomial regression fitting).'>>=
ap <- ggplot(df,aes(x=gen,y=sd_cor,group=run, colour=factor(correlation_correlation))) + scale_color_manual(name="Correlate\nFidelity", values=c("grey80", "grey10"), labels=c("false","true")) + stat_smooth() + labs(x="Generation", y="Standard Deviation of Fidelity") + theme(legend.key=element_rect(fill="white"))
bp <- ggplot(df,aes(x=gen,y=sd_fit,group=run,colour=factor(correlation_correlation))) + scale_color_manual(name="Correlate\nFidelity", values=c("grey80", "grey10"), labels=c("false","true")) + stat_smooth() + labs(x="Generation", y="Standard Deviation of Fitness") + theme(legend.key=element_rect(fill="white"))
grid.arrange(ap,bp,nrow=2,ncol=1)
@

<<lowstartranges2, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Smoothed ranges for the mean rather than the standard deviation - mean fidelity on the top row, mean fitness below - again by \\emph{Correlate Fidelity}.'>>=
cp <- ggplot(df,aes(x=gen,y=ave_cor,group=run, colour=factor(correlation_correlation))) + scale_color_manual(name="Correlate\nFidelity", values=c("grey80", "grey10"), labels=c("false","true")) + stat_smooth() + labs(x="Generation", y="Mean Fidelity") + theme(legend.key=element_rect(fill="white"))
dp <- ggplot(df,aes(x=gen,y=ave_fit,group=run, colour=factor(correlation_correlation))) + scale_color_manual(name="Correlate\nFidelity", values=c("grey80", "grey10"), labels=c("false","true")) + stat_smooth() + labs(x="Generation", y="Mean Fitness") + theme(legend.key=element_rect(fill="white"))
grid.arrange(cp,dp,nrow=2,ncol=1)
@

Data is all generations for those fixed-environment runs that reached completion with a 'sustainable' population (see \cref{lower-size-limit}) at generation \Sexpr{max(df['gen'])}; \Sexpr{nrow(unique(df['run']))} runs out of the full dataset of \Sexpr{nrow(unique(df_full['run']))} runs.

%<<lowstartranges2, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Range of 25\\% to 75\\% quartiles for \\gls{sd} for \\emph{true} setting for factor \\textbf{Correlate Fidelity}'>>=
%z<-by(df,df$gen,function(x) {aggregate(x$sd_cor ~ x$correlation_correlation, data=x, quantile)}) # quantiles aggregated by correlation_correlation
%z1<-apply(z,1,function(x){c(x[[1]][[2]][[2,2]],x[[1]][[2]][[2,3]],x[[1]][[2]][[2,4]])}) # correlation_correlation == -1
%z1<-apply(z,1,function(x){c(x[[1]][[2]][[1,2]],x[[1]][[2]][[1,3]],x[[1]][[2]][[1,4]])}) # correlation_correlation == 1
%z2<-z1[1:3,1:501]
%z3<-as.data.frame(cbind(1:501,z2[1,],z2[2,],z2[3,]))
%ggplot(z3,aes(V1,V2,V3)) + geom_ribbon(data=z3,aes(ymin=V2,ymax=V4), alpha = 0.2) + geom_line(aes(V1,V3))
%@



%<<ExperimentVarianceUnderHighStart, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='High start case: smoothed ranges for standard deviation of fidelity, as before grouped by \\emph{Correlate Fidelity}.'>>=
%ggplot(df,aes(x=gen,y=sd_cor,group=run, colour=factor(correlation_correlation))) + scale_color_manual(name="Correlate\nFidelity", values=c("grey80", "grey10"), labels=c("false","true")) + stat_smooth() + labs(x="Generation", y="Standard Deviation of Fidelity") + theme(legend.key=element_rect(fill="white"))
%@

%\chapter{Factor interactions}
%% EXPERIMENT 7 - Factor significance
%% Dataset: 2860d6fe
%
%<<ExperimentFactorInteractions, pdfcrop=TRUE, echo=FALSE, cache=TRUE>>=
%temp <- load.results('results/results-819350e.data')
%temp <- subset(temp, environment_change_frequency == 0 & truncate==0 & correlation_correlation==1)
%completing_factors <- unique(interaction(Filter(is.factor,temp[temp$gen==500,]))) # the factor levels that resulted in 500 generations
%df <- temp[interaction(Filter(is.factor,temp)) %in% completing_factors,] # raw data in long format - all low-start, fixed
%@
%
%<<lowstartfactorinfluence, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Box-plots for the effect of selected factors on final mean fidelity.'>>=
%#m0 <- lm(ave_cor~(p_reproduce+p_selection+n_offspring+distribution+fitness_correlation)^2, data=df)
%#x <- anova(m0)
%df$p_reproduce <- factor(df$p_reproduce,labels=c(0,0.33,0.66,1.0))
%df$p_selection <- factor(df$p_selection,labels=c(0,0.33,0.66,1.0))
%df$n_offspring <- factor(df$n_offspring,labels=c(2,5))
%df$distribution <- factor(df$distribution, labels=c("Gaussian","Uniform"))
%ap <- ggplot(df) + geom_boxplot(aes(x=p_reproduce,y=ave_cor)) + labs(x="Reproduction",y="Final mean fidelity")
%bp <- ggplot(df) + geom_boxplot(aes(x=p_selection,y=ave_cor)) + labs(x="Selection",y="Final mean fidelity")
%cp <- ggplot(df) + geom_boxplot(aes(x=n_offspring,y=ave_cor)) + labs(x="Offspring",y="Final mean fidelity")
%dp <- ggplot(df) + geom_boxplot(aes(x=distribution,y=ave_cor)) + labs(x="Distribution",y="Final mean fidelity")
%grid.arrange(ap,bp,cp,dp,nrow=2,ncol=2)
%@
%% significant with F = \Sexpr{round(x['F value'],2)} and p-value \textless{} \Sexpr{round(x['Pr(>F)'],2)}.
%
%Taking only runs where the factor \textbf{Correlate Fidelity} is \emph{true}, final mean fidelity is significantly influenced by all factors and by the first-level interactions between $p_{reproduction}$ and $n_{children}$, and $p_{reproduction}$ and $Distribution$.
%
%Examining this further, factor-by-factor, where \textbf{Correlate Fidelity} is \emph{true}, from \cref{fig:lowstartfactorinfluence} \TODO{complete}.
%
%\subsubsection{Is the choice of distribution function significant?}\label{distribution-function-1}
%
%gaussian implies a stronger relationship, uniform a broader range and less correlation. Considerations--connection to other fields -> gaussian. But uniform would be worst case--good to know conclusions hold even under these conditions.
%
%\begin{itemize}[label={}]
%	\item H$_0$: $\overline{gaussian} = \overline{uniform}$
%	\item H$_1$: $\overline{gaussian} \ne \overline{uniform}$
%\end{itemize}
%
%<<distributionfunction, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Comparison of Gaussian and Uniform distributions for Factor Distribution.'>>=
%df$distribution <- factor(df$distribution, labels=c("Gaussian","Uniform"))
%ap <- ggplot(df) + geom_boxplot(aes(x=distribution, y=ave_cor)) + labs(x="",y="Final mean fidelity")
%bp <- ggplot(df) + geom_boxplot(aes(x=distribution, y=ave_fit)) + labs(x="",y="Final mean fitness")
%grid.arrange(ap,bp,nrow=1,ncol=2,heights=unit(0.5, "npc"))
%@

\chapter{Model behaviour in changing environments}\label{model-behaviour-in-changing-environments}

Alongside our existing evolutionary model we now introduce an environmental model. This model describes the change of fitness resulting from enviromental change at each generation of the evolutionary model. 

Our intuition is that the average fidelity of the population will be inversely related to the degree of environmental change. More specifically, if we consider evolution to be a means by which a population learns how to adapt to an environment, the degree of environmental change can be described as the degree to which it's possible to learn the environment. We intuit that this is related to predictability, and hence to complexity. 

This seems a reasonable supposition. \textcite{Adami2002} recasts population fitness in terms of complexity, specifically his physical complexity measure: ``It is probably more appropriate to say that evolution increases the amount of information a population harbors about its niche (and therefore, its physical complexity)'' \textcite{Adami2002}. \textcite{Prokopenko2009} discusses the information-theoretic view of the benefits of complexity.

In this chapter we further develop this hypothesis to a testable form; propose it be tested by experiment on a simulation; describe an appropriate experimental design and model for a variety of environmental changes; and finally discuss the results of the experiment. The conceptual model underlying all of these steps is shown in \cref{fig:conceptual-model-for-environmental-change}. 

\begin{figure}
	\begin{center}
		\includegraphics[width=\linewidth]{figures/conceptual-model-for-environmental-change}
	\end{center}
\end{figure}\label{fig:conceptual-model-for-environmental-change}

First, we continue the discussion of learnability and predictability for two inter-related reasons: we need a testable measure for our hypothesis, and we need a generator for environmental changes that can be described in terms of the measure. \todo{first a time series for both, and then a separate measure...}

Now for some definitions:

A \emph{time series} is a set of observations $x_i$, each one being recorded at a specific time $t$ \textcite{Brockwell:2002dq} where an observation $x_i \in$ some set $\{X\}$, assumed to be $\mathbb{R}$.

\emph{Context} is any attribute whose values are largely independent but tend to be stable over contiguous intervals of another attribute known as the \emph{environmental attribute.} \textcite{Sammut:2010cr} The environmental attribute is typically, but not always, time; other attributes such as location are also possible. A sequence with stationary distribution is a context \textcite[p289]{Gama:2004ve}.

An instance $X_j$ is generated by a source, $S_j$. If every instance is sampled from the same source, that is $S_1 = S_2 = ... = S_{t+1} = S$ then the concept is \emph{stable}. If for any time points $i$ and $j$ $S_i \ne S_j$ then there is \emph{concept drift} \textcite{Zliobaite:2009cr}

According to Zliobaite \textcite{Bifet:2011uq,Zliobaite:2009cr}, the main types of concept drift can be classified using two attributes - speed, and reoccurrence - where speed can be either sudden or gradual, and reoccurrence either always new or reoccurring.  

These attributes when combined produce three independent categories - sudden, gradual and reoccurring.  In sudden drift there is a distinct break between concepts; gradual drift shows a period of mixed concepts, and reoccurring drift shows alternating or repeating concepts.  A problem may exhibit more than one source of concept drift, and in fact many real-world problems are of this form.

\section{Environmental model}\label{environmental-model}

\Textcite{Jablonka1995} and \textcite{Paenke:2007ie} share a simple model for environmental change, with abrupt switches between two defined environments, with the time between switches either being fixed or stochastic according to some probability. \Textcite{Gaucherel2012} also describes a single model for environmental change--a period of ``smooth'' change (either according to a form of $sin$ curve, or unchanging) followed by an abrupt change, repeated--with variation in the length and degree of each period. By contrast, \textcite[79]{Schuster2011}, when describing the relationship between fitness landscapes and error thresholds, details five distinct models of change: ``(i) the single-peak landscape corresponding to a mean field approximation, (ii) the hyperbolic landscape, (iii) the step-linear landscape, (iv) the multiplicative landscape, and (v) the additive or linear landscape.''.

Returning to first principles, any model must describe two particular elements of the environmental change: the scope of the change, and the shape of the change. First, we discuss the scope of change. Our evolutionary model allows us to group entities at three different levels:
\begin{enumerate}
	\item The group of all entities.
	\item A group for each set of ``related'' entities, where the most natural and obvious relation is that between parent and child; this is unambiguous and straightforward in our model where each entity has only one parent. We refer to a group of entities related by inheritance as a \emph{lineage}.
	\item A single-member group for each entity.
\end{enumerate}

In this work environmental changes may be applied to either of the first two of these three levels, with a consistent level applying throughout a run; the first level because it is the simplest application of environmental change, and the second as it represents the familiar scenario where we expect similar entities to react in similar ways to change, and where similarity is a result of descent: entities that share a common ancestor are more similar to each other than they are to other lineages. \todo{explanation for not addressing third level}

The shape of change is less constrained, and the space of all potential changes at any timestep effectively limitless: in fact, the potential change $\delta$ at timestep $t$ is $\delta_t\in R$. Simply taking a random sample from this space at each step is unlikely to result in enough resolution to test any relevant hypothesis. At the other extreme, taking only a small number of predetermined changes is likely to lead to a sampling fallacy where the choices bias the conclusions.

Instead, we need a way to parameterize the set of interesting enviromental changes so we can sample from a constrained but not predetermined parameter space. The range covered by the parameter space should include both predictable and unpredictable changes as the difference between the two is core to our hypothesis.

Our chosen method is to represent environmental change as a parameterized timeseries of particular form. Environmental change is modelled as an enhanced AR(1) or first-order autoregressive timeseries, with each timestep corresponding to one evolutionary generation. Specifically, we can describe the evolutionary change at each timestep as a function of the previous timestep:

$x_t = \Theta x_{t-1} + e_t + \delta$

where $x_t$ is the change at timestep $t$, $\Theta$ is the AR coefficient, $e_t$ is a random, normally distributed, error component around a mean of $0$, where $e_t\stackrel{iid}{\sim}N(0,\sigma^{2}_e)$, and $\delta$ is a fixed bias value.

This series allows us to represent a broad range of environmental changes:

\begin{itemize}
	\item Each timeseries is completely specified by three parameters, $\Theta$, $\sigma_e$ and $\delta$.
	\item $\Theta$ in an autoregressive timeseries can be interpreted as specifying stability or smoothness, while $\delta$ is a fixed change. We use $\delta$ to model a fitness trend - environments with a positive $\delta$ will see the fitness of each entity improved at each generation, with the opposite of course true of negative $\delta$. Note that with this formulation we can model linear trends in fitness from a fixed bias in the environment produced by the $\delta$ term. This is not the same as a ARI model where the environment timeseries itself would show a trend.
	\item The timeseries is defined by three independent elements, two predictable (driven by $\Theta$ and $\delta$) and the other ($sigma_e$) random and unlearnable. By changing the ratio between the two we can examine the performance of the evolutionary algorithm on some continuum of predictability.
	\item An AR timeseries has the property of stationarity, meaning that the mean of the series is constant through time. However, as we apply the series values as deltas to element fitness, fitness can be non-stationary, and so may show a long term trend. This allows a simple non-differencing timeseries to describe a steady improvement or worsening in fitness.
	\item As a corollary of stationarity, the range is strongly determined by the initial parameters. This is a useful property as it means that with appropriate parameter choices no scaling of the range is required. 
\end{itemize}

Statistical techniques are commonplace for time-series predictions \textcite{Brockwell:2002dq}. ARMA models are used for \emph{stationary} series, that is a series where \todo{Add definition}, equivalent to saying the series does not demonstrate concept drift. ARIMA models apply for non-stationary series where the difference between two sequential values of the original series can be shown to produce a stationary series. The I, or ``Integrated'' component, of the model provides the differencing. Seasonality, a particular form of re-occuring concept drift, may be modelled with both ARMA and ARIMA models by incorporating a seasonality component in the model \todo{reference}. If instead of mean value prediction we are interested in the mean variance (such as in financial markets) then ARCH and GARCH models are appropriate. ARMA or ARIMA combined with ARCH or GARCH can be used in iterative sequence to simultaneously predict both mean and variance for \todo{add defn - heteroscedastic} series. 

Time-series modeling provides techniques for describing time-series data in terms of an underlying model, summarizable in a few parameters. The process can also be reversed to produce a time-series from the model; in other words, if the variety of environmental change required to test our hypothesis can be described by a standard time-series model, the parameters that determine that model can also serve as our summary measure for environmental change. Note however that the change is described by a function or relationship between two or more parameters, depending on the model, rather than the one of a complexity-based measure.

\section{Hypothesis}

Our hypothesis, that fidelity is related to learnability, can now be refined in terms of the predictability of the environment, where predictability is proportional to some relation involving $\Theta$, $\sigma_e$ and $\delta$.

From the hypothesis we make these predictions:
\begin{enumerate}
	\item Fidelity is at a minimum in conditions of maximum unpredictability, that is for timeseries where $\Theta=0$, $\delta=0$ and $\sigma_e>0$.
	\item Fidelity is proportional to the predictability in the environment.
\end{enumerate}

\section{Experimental design}

As all three independent variables,  $\Theta$, $\sigma_e$ and $\delta$, are continuous in $\mathbb{R}$, and as we wish to test the specific relationship of fidelity across a range of these variables, our earlier factorial and fractional-factorial experiment designs are no longer appropriate. Instead we adopt a response surface design where each experiment run has initial values of the independent variables taken from a uniform random sample from their range, or in other words, a series of random samples with uniform probabilty from a space described as the hypercube with one variable on each of the three axes (see \cref{fig:hypercube}.)

\begin{figure}
	\begin{center}
		\includegraphics[width=\linewidth]{figures/hypercube}
	\end{center}
\end{figure}\label{fig:hypercube}

<<factorialthetasd, pdfcrop=TRUE, echo=FALSE, warning=FALSE, cache=TRUE, fig.pos='htp', fig.cap=''>>=
library(reshape2)
library(ggplot2)
library(cowplot) # styling of plots, extension of ggplot2
library(gridExtra)

t1 <- read.csv('results/environments-factorial.csv', header=FALSE, colClasses=c("numeric","numeric","numeric"))

t1$run <- 1:nrow(t1)
names(t1)[1:3]<-c("theta", "sd", "bias")
t2 <- melt(t1,id=c('run','theta','sd','bias'))
t2$theta <- round(t2$theta,3)
t2$sd <- round(t2$sd,3)
t2$bias <- round(t2$bias,3)

for (r in unique(t2$run)) {
	t2[t2$run==r,'t'] <- 1:50 # tag with timestamp
	
	fitness <- 0.5
	for (t in 1:50) {
		fitness <- max(0,min(1,fitness + t2[t2$run==r & t2$t==t,'value']))
		t2[t2$run==r & t2$t==t,'fitness'] = fitness
	}
}
ap <- ggplot(subset(t2,bias==0)) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_grid(theta~sd, labeller='label_both') + labs(x="t", y="Fitness change")
bp <- ggplot(subset(t2,bias==0)) + geom_line(aes(x=t,y=fitness)) + facet_grid(theta~sd, labeller='label_both') + labs(x="t", y="Fitness change")
grid.arrange(ap,bp,nrow=2,ncol=1)
@

<<factorialsdbias, pdfcrop=TRUE, echo=FALSE, warning=FALSE, cache=TRUE, fig.pos='htp', fig.cap=''>>=
ap <- ggplot(subset(t2,theta==0)) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_grid(sd~bias, labeller='label_both') + labs(x="t", y="Fitness change")
bp <- ggplot(subset(t2,theta==0)) + geom_line(aes(x=t,y=fitness)) + facet_grid(sd~bias, labeller='label_both') + labs(x="t", y="Fitness change")
grid.arrange(ap,bp,nrow=2,ncol=1)
@

<<sampleenvironment, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap=''>>=
library(reshape2)
library(ggplot2)
library(cowplot) # styling of plots, extension of ggplot2

t1 <- read.csv('results/environments.csv', header=FALSE, colClasses=c("numeric","numeric","numeric"))
t1$run <- 1:nrow(t1)
names(t1)[1:3]<-c("theta", "sd", "bias")
t2 <- melt(t1,id=c('run','theta','sd','bias'))
t2$theta <- round(t2$theta,3)
t2$sd <- round(t2$sd,3)
t2$bias <- round(t2$bias,3)
ggplot(t2) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_wrap(~theta+sd+bias, labeller='label_both') + labs(x="t", y="Fitness change")
@

% Needs sampleenvironment
<<cumulativefitness, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.pos='htp', fig.cap=''>>=
for (r in unique(t2$run)) {
	t2[t2$run==r,'t'] <- 1:50 # tag with timestamp
	
	fitness <- 0.5
	for (t in 1:50) {
		fitness <- max(0,min(1,fitness + t2[t2$run==r & t2$t==t,'value']))
		t2[t2$run==r & t2$t==t,'fitness'] = fitness
	}
}

ggplot(t2) + geom_line(aes(x=t,y=fitness)) + facet_wrap(~theta+sd+bias, labeller='label_both') + theme(strip.text.x = element_text(size = 6))
@

% Needs sampleenvironment
<<sampleentropy, pdfcrop=TRUE, echo=FALSE, warning=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Approximate entropy of each example environment.'>>=
library(pracma)
library(ggplot2)
library(cowplot) # styling of plots, extension of ggplot2

t2 <- t1[,1:3]
names(t2)[1]<-"theta"
names(t2)[2]<-"sd"
names(t2)[3]<-"bias"
for(n in 1:nrow(t1)) {t2[n,'sample_entropy'] <- sample_entropy(unlist(t1[n,4:52],use.names=FALSE))}
for(n in 1:nrow(t1)) {t2[n,'approx_entropy'] <- approx_entropy(unlist(t1[n,4:52],use.names=FALSE))}

#summary(lm(sample_entropy~theta+sd,data=t2)) # can error as some sample_entropy values may be Inf
ggplot(t2, aes(sd,approx_entropy)) + geom_point() + geom_smooth(method='lm') # formula not required

#summary(lm(approx_entropy~theta + sd, data = t2)) #  SIGNIFICANT
@

The range of each variable (the length of each side of the hypercube) is set from earlier experimentation. We created two independent datasets by sampling from two hypercubes, described in \cref{tbl:range-of-independent-variables}.

\begin{table}
	\begin{center}
		\caption{Range of independent variables $\Theta$, $\sigma_e$ and $\delta$}
		\label{tbl:range-of-independent-variables}
		\begin{tabular}{@{}llll@{}}
			\toprule
			Dataset   	 	&  $\Theta$		& $\sigma_e$	& $\delta$\\
			\midrule
			Dataset no.1	& $[-0.4, 0.4]$	& $[0, 0.2]$ 	& $[-0.1, 0.1]$\\
			Dataset no.2	& $[-0.2, 0.2]$	& $[0, 0.1]$	& $[-0.05, 0.05]$\\
			Dataset no.3	& $[-0.2, 0.2]$	& $[0, 0.1]$	& $[-0.05, 0.05]$\\
			Dataset no.4	& $[-0.4, 0.4]$	& $[0, 0.4]$	& $[-0.04, 0.04]$\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\section{Results and discussion}

<<figure22, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.pos='htp', fig.cap='Distribution of maximum generation reached for all runs of dataset no.1 (top), and dataset no.2 (bottom). Axes are to the same scale. Runs are ordered by final generation reached.'>>=
library(ggplot2)
library(gridExtra)
library(cowplot) # styling of plots, extension of ggplot2

# BY_LINEAGE	CORRELATED	N_OFFSPRING	P_REPRODUCE	P_SELECTION	RESTRICTION	ar_bias	ar_sd	ar_theta	ave_fid	ave_fit	experiment	gen	pop	run	sd_fid	sd_fit
colClasses <- c("factor","factor","factor","factor","factor","factor","numeric","numeric","numeric","numeric","numeric","factor","integer","numeric","integer","numeric","numeric")
r1 <- read.csv('results/results-0c267bcd2-a.csv', colClasses=colClasses)
r2 <- read.csv('results/results-0c267bcd2-b.csv', colClasses=colClasses)
r3 <- read.csv('results/results-cc648a0a9.csv', colClasses=colClasses)
r4 <- read.csv('results/results-996060f.csv', colClasses=colClasses)
results = rbind(r1,r2,r3,r4)

t1 <- aggregate(r1$gen,by=list(r1$run),max)
t2 <- aggregate(r2$gen,by=list(r2$run),max)
t3 <- aggregate(r3$gen,by=list(r3$run),max)
t4 <- aggregate(r4$gen,by=list(r4$run),max)
max_nrow <- max(nrow(t1),nrow(t2),nrow(t3),nrow(t4))
ap <- ggplot(t1,aes(1:nrow(t1),t1[order(t1[,2],decreasing=TRUE),2]))+geom_bar(stat="identity") + scale_x_continuous(limits=c(0,max_nrow)) + labs(x="", y="Final generation")
bp <- ggplot(t2,aes(1:nrow(t2),t2[order(t2[,2],decreasing=TRUE),2]))+geom_bar(stat="identity") + scale_x_continuous(limits=c(0,max_nrow)) + labs(x="", y="Final generation")
cp <- ggplot(t3,aes(1:nrow(t3),t3[order(t3[,2],decreasing=TRUE),2]))+geom_bar(stat="identity") + scale_x_continuous(limits=c(0,max_nrow)) + labs(x="", y="Final generation")
dp <- ggplot(t4,aes(1:nrow(t4),t4[order(t4[,2],decreasing=TRUE),2]))+geom_bar(stat="identity") + scale_x_continuous(limits=c(0,max_nrow)) + labs(x="", y="Final generation")
grid.arrange(ap,bp,cp,dp,nrow=4,ncol=1)
@

A summary of the results from each of the four datasets is given in \cref{tbl:summary-for-changing-datasets}. The difference in the proportion of  completed runs to total runs reflects the respective ranges of the independent variables (as seen in \cref{tbl:range-of-independent-variables}), as shown graphically in \cref{fig:figure22}.

\begin{table}
	\begin{center}
		\caption{Summary of results for changing environments}
		\label{tbl:summary-for-changing-datasets}
		\begin{tabular}{@{}lll@{}}
			\toprule
			Dataset    		& $n$ (total runs) 				& Completed runs\\
			\midrule
			Dataset no.1	& \Sexpr{max(r1$run)+1}			& 	\Sexpr{nrow(r1[r1$gen==500,])+1}\\
			Dataset no.1	& \Sexpr{max(r2$run)+1}			& 	\Sexpr{nrow(r2[r2$gen==500,])+1}\\
			Dataset no.1	& \Sexpr{max(r3$run)+1}			& 	\Sexpr{nrow(r3[r3$gen==500,])+1}\\
			Dataset no.1	& \Sexpr{max(r4$run)+1}			& 	\Sexpr{nrow(r4[r4$gen==500,])+1}\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

<<figure23, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.pos='htp', fig.cap='.'>>=
library(ggplot2)
library(gridExtra)
library(cowplot) # styling of plots, extension of ggplot2

ap <- ggplot(subset(results,gen==500),aes(ar_sd,ave_fid)) + geom_point() + geom_smooth(method='lm')
bp <- ggplot(subset(results,gen==500)) + geom_point(aes(ar_bias,ave_fid)) # fid ~ -bias when bias <0, otherwise fid can reach max
grid.arrange(ap,bp,nrow=2,ncol=1)

#anova(lm(ave_fid ~ ar_theta*ar_sd*ar_bias, data=subset(results,gen==500))) # theta not significant
#anova(lm(ave_fid ~ ar_sd*ar_bias, data=subset(results,gen==500))) # bias and sd:bias most important, then sd
#anova(lm(sd_fid ~ ar_sd*ar_bias, data=subset(results,gen==500 & -0.001<ar_bias & ar_bias<0.001))) # Remove effect of bias, then ar_sd 0.001
@
<<figure24, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE,fig.pos='htp', fig.cap='.'>>=
library(ggplot2)
library(gridExtra)
library(cowplot) # styling of plots, extension of ggplot2

ap <- ggplot(subset(results,gen==500 & -0.002<ar_bias & ar_bias<0.002),aes(ar_sd,ave_fid)) + geom_point()  + geom_smooth(method='lm')  # ave fid drops as sd increases
bp <- ggplot(subset(results,gen==500 & -0.002<ar_bias & ar_bias<0.002),aes(ar_sd,sd_fid)) + geom_point()  + geom_smooth(method='lm')  # sd fid increases as sd increases
grid.arrange(ap,bp,nrow=2,ncol=1)
@
<<figure25, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE,fig.pos='htp', fig.cap='.'>>=
library(ggplot2)
library(gridExtra)
library(cowplot) # styling of plots, extension of ggplot2

ap <- ggplot(subset(results,gen==500),aes(ave_fit,ave_fid)) + geom_point() + geom_smooth(method='lm') # and relationship between these two
# Relationship between sd_fid and ave_fid!
bp <- ggplot(subset(results,gen==500)) + geom_point(aes(sd_fid,ave_fid))
grid.arrange(ap,bp,nrow=2,ncol=1)
#anova(lm(sd_fid~ave_fid, data=subset(results, gen==500))) # 0.001
@

<<figure26, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE,fig.pos='htp', fig.cap='Effect of lineage upon results.'>>=
library(ggplot2)
library(gridExtra)
library(cowplot) # styling of plots, extension of ggplot2

ap <- ggplot(subset(results,gen==500 & -0.002<ar_bias & ar_bias<0.002),aes(ar_sd,ave_fid)) + geom_point()  + geom_smooth(method='lm')  + facet_wrap(~BY_LINEAGE)
bp <- ggplot(subset(results,gen==500 & -0.002<ar_bias & ar_bias<0.002),aes(ar_sd,sd_fid)) + geom_point()  + geom_smooth(method='lm')  + facet_wrap(~BY_LINEAGE)
grid.arrange(ap,bp,nrow=2,ncol=1)
@

%<<figure27, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE,fig.pos='htp', fig.cap='Sample entropy.'>>=
%library(pracma)
%library(ggplot2)
%library(cowplot) # styling of plots, extension of ggplot2
%
%t2 <- t1[,1:3]
%names(t2)[1]<-"theta"
%names(t2)[2]<-"sd"
%names(t2)[3]<-"bias"
%for(n in 1:nrow(t1)) {t2[n,'sample_entropy'] <- sample_entropy(unlist(t1[n,4:52],use.names=FALSE))}
%for(n in 1:nrow(t1)) {t2[n,'approx_entropy'] <- approx_entropy(unlist(t1[n,4:52],use.names=FALSE))}
%
%ggplot(t2, aes(sd,approx_entropy)) + geom_point() + geom_smooth(method='lm') # formula not required
%@

\begin{DRAFT}
The unbiased sample variance with Bessel's correction is \todo{reference}:
\[
s^2 = \frac {1}{n-1} \sum_{i=1}^n  \left(x_i - \overline{x} \right)^ 2 = \frac{\sum_{i=1}^n \left(x_i^2\right)}{n-1} - \frac{\left(\sum_{i=1}^n x_i\right)^2}{(n-1)n}.
\]

Let us assume that the upper bound on fidelity of 1.0 has no effect on $s^2$.
Then we can divide the fidelity values at time $t$ into those below the mean and those above; arrange the sample values so that the first $m$ fall below the mean and the remainder above. The sample variance can now be written as:  

\[
s^2 = \frac {1}{n-1} \sum_{i=1}^n  \left(x_i - \overline{x} \right)^ 2 = \frac {1}{n-1} \sum_{i=1}^m  \left( \left(x_i - \overline{x} \right)^ 2 + \sum_{i=m}^n  \left(x_i - \overline{x} \right)^ 2 \right)
\]

As $\overline{x}$ approaches 1.0 (the upper bound on fidelity) from below:

\[
s^2 = \frac {1}{n-1} \left( \sum_{i=1}^m   \left(x_i - 1 \right)^ 2 + \sum_{i=m}^n  \left(x_i - 1 \right)^ 2 \right)
\]

But by our initial statement, $s^2$ remains unchanged, therefore...

\[
\frac {1}{n-1} \left( \sum_{i=1}^m   \left(x_i - 1 \right)^ 2 + \sum_{i=m}^n  \left(x_i - 1 \right)^ 2 \right) = \frac{\sum_{i=1}^n \left(x_i^2\right)}{n-1} - \frac{\left(\sum_{i=1}^n x_i\right)^2}{(n-1)n}.
\]
\end{DRAFT}


\section{Future work}\label{part2-future-work}

There are some obvious extensions of this model that for reasons of scope have been left for future work. First, the model currently assumes only single-parent inheritance, whereas many biological species have two parents. Extending to two parents would be a useful enhancement to increase the model's scope. Second, the model does not include any influence from development (the production of the phenome from the genome). However, it is unclear at this stage what effect development would have on the model as its effects are bundled into the overall \emph{fitness} parameter. Finally, although outside of the overall scope of this work in evolutionary systems, the effect of acquired characteristics would be interesting to explore. Others (\eg \textcite{Gaucherel2012,Paenke:2007ie,Sasaki:2000dq}) have studied the differences between general models based on acquired and non-acquired characteristics, finding a difference between models in changing environments. This would be another area of exploration for the future.

Two other topics for possible future work are abrupt environmental change, and an information-based measure for change, as explained below.

\subsection{Abrupt environmental change}
	
Although the generator described earlier produces a time series for environmental change with the property of stationarity, the $\delta$ term makes the evolutionary model of fitness non-stationary. However, any change is steady and gradual. An extension would be to coopt the idea of concept drift from time series analysis to induce an abrupt change with probability $p$ at each generation. Each change would therefore form a new `concept`. Instead of the environment changing in a predictable and describable way from one generation to another, the change could not be predictable from the earlier history.

Stationarity is inherently a constraint for applying statistical methods to the analysis of evolutionary models, where almost by definition, the outcomes change over time. One common approach is be to assume approximate stationarity over short(ish) periods, along the lines of moving window or local kernel methods in time-series analysis. Alternatively, we could reformulate the problem so that out of all the possible message channels, we choose to investigate only those that meet the stationarity assumption. It's hard to see though how we can effectively examine an evolving system by discarding most of the relevant information.

Another broad approach might be to modify the method to reduce its dependence on stationarity. For example, taking one part of the problem, \textcite{LingFengLiu2014} develops a modification of the well-known Shannon entropy formula for non-stationary processes. The method though assumes that the process moves between a number of states with the series output following a known distribution in each state. Although this approach does provide an upper-bound to the information entropy of the process output, identifying the states and the accompanying distributions from a black-box process is likely to prove a challenge. Fundamentally, non-stationarity remains a problem for most methods.

\subsection{Information-based measure}

Predictability related to compression and information entropy. Compression is a mechanism for pattern-discovery.  \textcite{Shalizi2001} identifies patterns (pattern $P$ of an object $O$) with the ability to predict (given $P$, can infer $O$) or compress (given $O$, can compress to $P$). Compression doesn't imply Prediction. There is a related concept in algorithmic complexity theory--the difference between easily solvable (P) (prediction) and easily verifiable (NP) (compression). A problem may be decidable without being easily solvable. 

In a time-independent way we can identify patterns in an image, for example, that allows us to substitute the pattern for the raw data. It is thus related to algorithms - compression is the discovery of a specific algorithm to take raw data and produce patterned data, with the goal of increasing the information content and reducing the information entropy of the patterned data. The algorithm is the patterned data. Information entropy can then, in theory at least, be measured by the Kolmogorov entropy or algorithmic complexity - the length of the algorithm. In practice various other entropy measures, such as Shannon, sample, and approximate entropy, are commonly used for time-series characterization.

In a time-dependent way compression equals prediction - in the sense that we can produce an algorithm that improves our ability to predict the future output of a process. Prediction on one spatial dimension is called a time series. Most prediction requires consistent conditions or contexts - when these change our predictive ability is reduced, (in time-series this is called concept drift). 

Compression, as mechanism to identify patterns, will identify items spatially if we can map items to patterns. The choice of compression method will privilege different ways of defining a pattern - if we are to use pattern discovery we need to determine what are interesting patterns - which is hardly domain agnostic. 

\textcite[Appendix 1]{Edmonds1999} contains a thorough review of complexity measures. More recently see \textcite{Prokopenko2009}, \textcite{Ladyman2011} and \textcite{Lloyd2001}.

Statistical Complexity, or $C_\mu$, based on causality, is fundamentally time-dependent: one dimension plus time, although an extension to two spatial dimensions has been done by Shalizi with a more general extension to multi-dimensions still open although a mapping method for multi-dimensional data has been proposed by \textcite{Nerukh2002}, and a dimension reduction approach by fuzzy-clustering can be found in \textcite{Young2005}.

The benefits of $C_{\mu}$ \textcite{Crutchfield1989} are that it matches our intuition of complexity (better than most alternatives), it has strong theoretical support, and it makes no major assumptions about the underlying data other than stationarity - it is domain agnostic. \textcite{Shalizi2001} provides the best theoretical explanation of $C_{\mu}$ and epsilon-machines; Shalizi's PhD thesis \textcite{Shalizi2001a} provides further context. 

$C_{\mu}$ has been applied to a number of domains including random boolean networks \textcite{Gong2012}, spin \textcite{Vrabic2012,Shalizi2007,Nerukh2002,Feldman1998}, estimation of cortical thinning from brain MRI data \textcite{Young2008}, autonomy of protocells \textcite{Krakauer2008} and detection of anomalies (such as imminent crankshaft failure) directly from the causal states \textcite{Xiang2008}.
The canonical formulation of $C_{\mu}$ depends on two assumptions--discrete values and discrete time \textcite[p.24]{Shalizi2001}, and exact joint probabilities--which are unproblematic in our domain, and another, conditional stationarity, which poses a problem. Conditional stationarity, or time-invariant transition probabilities \textcite[p.25]{Shalizi2001} means $P(\overrightarrow{S}_t^L = s^L) = P(\overrightarrow{S}_0^L = s^L)$ for all $t \in \mathbb{Z}$ or ``the distribution of futures, conditional on histories, must be independent of when the history comes to an end'' \textcite[p.119]{Shalizi2001}.


\section{Elimination of alternative explanations}\label{elimination-of-alternative-explanations}
\begin{DRAFT}
\subsection{Variation alone is sufficient for Inheritance}
As an experimental test, we can discount this alternative hypothesis by showing an example of not(V-\textgreater{}I) or, V and not I.

\subsection{Selection alone is sufficient for Inheritance}
The test is to show an example of S and not I.

\subsection{Variation and Selection, without property correlation, is sufficient for Inheritance}
Test: already shown in earlier analysis, specifically in the Hypothesis analysis sections where the factor \textbf{Correlate Fidelity} is set to the low value ("false").
\end{DRAFT}

\chapter{Conclusions}
\begin{DRAFT}
Given:\newline
Hypothesis 1 (that Variation, Selection and Inheritance are sufficient for Evolution) and,\newline
hypothesis \autoref{hypothesis-2} (that Variation and Selection are sufficient for Inheritance),\newline
we suggest that:

\textit{Hypothesis 3}: Variation and Selection are sufficient for Evolution

\emph{erroneous copying} \parencite{Zachar2010} results in informational replicators.

The model as described is general, under the assumptions given below. It is also directly comparable to models from Evolutionary Computation and so we also expect any results and conclusions to be relevant to EC models.

\TODO{Map to standard EA models - mutation only etc}
\end{DRAFT}
