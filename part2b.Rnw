<<setup, include=FALSE>>=
library(knitr)
library(cowplot) # styling of plots, extension of ggplot2
library(gridExtra) # grid layouts for ggplot2
library(lattice) # needed for bwplot etc
library(english) # convert numbers to words
opts_chunk$set(fig.path='generated_figures/')
knit_hooks$set(pdfcrop = hook_pdfcrop)

sults.varying <- function(t) {
colClasses <- c("factor","factor","factor","factor","factor","factor","numeric","numeric","numeric","numeric","factor","integer","integer","integer","numeric","numeric")
read.csv(t, colClasses=colClasses)
}

@

\chapter{Changing conditions}\label{experimental-test-of-h2-under-changing-conditions}
\section{Experimental design}
Alongside our existing evolutionary model we now introduce an environmental model. This model describes the change of fitness resulting from enviromental change at each generation of the evolutionary model. Fitness is thus extended from purely a property of an entity to representing the relationship between entity and environment.

The model must describe two particular elements of this relationship: first,the scope of the change, and second, the shape of the change. In our evolutonary model we can only group entities in three non-trivial ways:
\begin{enumerate}
	\item The group of all entities.
	\item A single-member group for each entity.
	\item A group for each set of ``related'' entities, where the most natural and obvious relation is that between parent and child; this is unambiguous and straightforward in our model where each entity has only one parent. We refer to a group of entities related by inheritance as a \emph{lineage}.
\end{enumerate}

We can apply the scope of the change to each of these three groups.

The shape of change is less constrained, and the space of all potential changes at any timestep effectively limitless: in fact, the potential change $\delta$ at timestep $t$ is $\delta_t\in R$. Simply taking a random sample from this space at each step is unlikely to result in enough resolution to test any relevant hypothesis. At the other extreme, taking only a small number of predetermined changes is likely to lead to a sampling fallacy where the choices bias the conclusions.

Instead, we need a way to parameterize the set of interesting enviromental changes so we can instead sample from a constrained but not predetermined parameter space. The range covered by the parameter space should include both predictable and unpredictable changes as the difference between the two is core to our hypothesis.

Our chosen method is to represent environmental change as a parameterized timeseries of particular form, as follows. Environmental change is modelled as an AR(1) or first-order autoregressive timeseries, with each timestep corresponding to one evolutionary generation. Specifically, we can describe the evolutionary change at each timestep as a function of the previous timestep:

$x_t = \Theta x_{t-1} + e_t$

where $x_t$ is the change at timestep $t$, $\Theta$ is the AR coefficient, and $e_t$ is a random, normally distributed, error component around a mean of $0$, where $e_t\stackrel{iid}{\sim}N(0,\sigma^{2}_e)$

This series allows us to represent a broad range of environmental changes:

\begin{itemize}
	
	\item Each timeseries is completely specified by only two parameters, $\Theta$ and $\sigma_e$.
	\item $\Theta$ in an autoregressive timeseries can be interpreted as specifying stability or smoothness, allowing us to control the predictability of the change.
	\item The timeseries is composed of two independent elements, one predictable (driven by $\Theta$) and the other ($sigma_e$) random and unlearnable. By changing the ratio between the two we can examine the performance of the evolutionary algorithm on a continuum of predictability.
	\item An AR timeseries has the property of stationarity, meaning that the mean of the series is constant through time. However, as we apply the series values as deltas to element fitness, fitness can be non-stationary, or in other words show a long term trend. This allows a simple non-differencing timeseries to describe a steady improvement or worsening in fitness, something we'd like to include in our sample environments.
	\item As a corrollary of stationarity, the range is strongly determined by the initial parameters. This is a useful property as it means that with appropriate parameter choices no scaling of the range is required. 
\end{itemize}

Introduction of abrupt change to the environmental change model. A simple model would introduce an abrupt change with probability $p$ at each generation, where the change would form a new `concept` (in machine learning terminology, appriately as evolution can be seen as a learning system). Instead of the environment changing in a predictable and describable way from one generation to another, the change could not be predictable from the earlier history.

Our hypothesis, that fidelity is related to learnability, can now be refined in terms of the predictability of the environment, where predictability is proportional to the ratio of $\Theta$ to $\sigma_e$.

From the hypothesis we make these predictions:
\begin{enumerate}
	\item Fidelity will be at a minimum in conditions of maximum unpredictability, that is for timeseries where $\Theta=0$ and $\sigma_e>0$.
	\item Fidelity is proportional to the predictability in the environment.
\end{enumerate}

\chapter{Environmental model}
<<factorialenvironment, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap=''>>=
library(pracma)
library(reshape2)
library(ggplot2)

t1 <- read.csv('model/environments-factorial.csv', header=FALSE, colClasses=c("numeric","numeric","numeric"))

t1$run <- 1:nrow(t1)
names(t1)[1:3]<-c("theta", "sd", "bias")
t2 <- melt(t1,id=c('run','theta','sd','bias'))
t2$theta <- round(t2$theta,3)
t2$sd <- round(t2$sd,3)
t2$bias <- round(t2$bias,3)
ggplot(subset(t2,bias==0)) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_grid(theta~sd, labeller='label_both') + labs(x="t", y="Fitness change")
ggplot(subset(t2,sd!=0)) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_grid(theta~bias, labeller='label_both') + labs(x="t", y="Fitness change")
@

<<sampleenvironment, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap=''>>=
library(pracma)
library(reshape2)
library(ggplot2)

t1 <- read.csv('model/environments.csv', header=FALSE, colClasses=c("numeric","numeric","numeric"))
t1$run <- 1:nrow(t1)
names(t1)[1:3]<-c("theta", "sd", "bias")
t2 <- melt(t1,id=c('run','theta','sd','bias'))
t2$theta <- round(t2$theta,3)
t2$sd <- round(t2$sd,3)
t2$bias <- round(t2$bias,3)
ggplot(t2) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_wrap(~theta+sd+bias, labeller='label_both') + labs(x="t", y="Fitness change")
@

% Needs sampleenvironment
<<cumulativefitness, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap=''>>=
for (r in unique(t2$run)) {
	t2[t2$run==r,'t'] <- 1:50 # tag with timestamp
	
	fitness <- 0.5
	for (t in 1:50) {
		fitness <- max(0,min(1,fitness + t2[t2$run==r & t2$t==t,'value']))
		t2[t2$run==r & t2$t==t,'fitness'] = fitness
	}
}

t2$theta <- round(t2$theta,3)
t2$sd <- round(t2$sd,3)
t2$bias <- round(t2$bias,3)
ggplot(t2) + geom_line(aes(x=t,y=fitness)) + facet_wrap(~theta+sd+bias, labeller='label_both') + theme(strip.text.x = element_text(size = 8))
@

% Needs sampleenvironment
<<sampleentropy, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.cap='Sample entropy of each example environment.'>>=
t2 <- t1[,1:3]
names(t2)[1]<-"theta"
names(t2)[2]<-"sd"
names(t2)[3]<-"bias"
for(n in 1:nrow(t1)) {t2[n,'sample_entropy'] <- sample_entropy(unlist(t1[n,4:52],use.names=FALSE))}
for(n in 1:nrow(t1)) {t2[n,'approx_entropy'] <- approx_entropy(unlist(t1[n,4:52],use.names=FALSE))}

summary(lm(sample_entropy~theta+sd,data=t2)) # F-statistic: 2.359 on 2 and 497 DF,  p-value: 0.09561 - no real linear model
summary(lm(approx_entropy~theta + sd, data = t2)) # F-statistic:   1.9 on 2 and 497 DF,  p-value: 0.1506 - even less relationship...
ggplot(t2, aes(sd,sample_entropy)) + geom_point() + geom_smooth(method='lm') # formula not required
@

\chapter{Results and discussion}



\chapter{TODO-Elimination of alternative explanations}\label{elimination-of-alternative-explanations}

\section{Variation alone is sufficient for Inheritance}\label{variation-alone-sufficient-for-inheritance -- v-i}

As an experimental test, we can discount this alternative hypothesis by showing an example of not(V-\textgreater{}I) or, V and not I.

Models where p\_selection (factors{[}1{]}) == 1.0 effectively have no
selection (Pr(selected)=1.0), and under changing environment where
fitness changes each generation {[}Element(factors{[}4{]}(x.fitness-0.2,
0.95), x.correlation) for x in population{]}, fitness can and does drop
to zero without entities being removed from population

How to reproduce:

\begin{verbatim}
factor_values = [0, 1, 0, 1, 1, 0, 1] # factor_defns[for selection] = 1.0, meaning no selection! So zero fitness entities persist...
factors = [defn[value] for defn, value in zip(factor_defns, factor_values)]
(initial, final) = model.run(factors, population=init_population(5000, low_start=True), generations=250, population_limit=10, changing_environment=True)
self.assertEqual(5000, final.pop)
\end{verbatim}

Appears to get stuck in local maxima--when correlation goes to 1.0 then no scope for change of fitness \ldots{}

\section{Selection alone is sufficient for Inheritance}\label{selection-alone-sufficient-for-inheritance-s-i}

Test: show example of S and not I

\section{Variation and Selection, without property correlation, is sufficient for Inheritance}

Test: already shown in earlier analysis, specifically in the Hypothesis analysis sections where the factor \textbf{Correlate Fidelity} is set to the low value ("false").

\chapter{TODO-Conclusions}

Given:\newline
Hypothesis 1 (that Variation, Selection and Inheritance are sufficient for Evolution) and,\newline
hypothesis \autoref{hypothesis-2} (that Variation and Selection are sufficient for Inheritance),\newline
we suggest that:

\textit{Hypothesis 3}: Variation and Selection are sufficient for Evolution

Possible extensions

\begin{itemize}
	\item Trend to environmental change
	\item Experimental test: demonstrate Evolution given Variation and Selection
	\item Consistency and sufficiency--classification of existing systems
	\item Fitness independent of inheritance potential--bias applied only to bias value of offspring, not fitness. However, fitness dependent on inheritance is more likely. A mechanism that doesn't copy well unlikely to preserve information leading to high fitness\ldots{}--the parent's fidelity influences the offspring's fidelity, and to offspring's fitness
	\item Both \autocite{Bourrat2015} and this work introduce a check-for-overcrowding step; although motivated by practical considerations, under endogenous selection shouldn't overcrowding also be endogenous?
\end{itemize}
	

	
