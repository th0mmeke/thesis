<<setup, include=FALSE>>=
library(knitr)
library(cowplot) # styling of plots, extension of ggplot2
library(gridExtra) # grid layouts for ggplot2
library(lattice) # needed for bwplot etc
library(english) # convert numbers to words
library(xtable) # required for print.xtable.bootabs function
opts_chunk$set(fig.path='generated_figures/')
knit_hooks$set(pdfcrop = hook_pdfcrop)
@
\chapter{Environmental Effects on Replicator States}\label{toyworld2}

\section{Introduction}

\section{Environmental model}

Returning to first principles, any model must describe two particular elements of the environmental change: the \emph{scope} of the change, and the \emph{shape} of the change. First, we discuss the \emph{scope} of change. Our evolutionary model (\cref{base-model}) provides the scope to group entities at three different levels:
\begin{enumerate}
	\item The group of all entities.
	\item A group for each set of ``related'' entities, where the most natural and obvious relation is that between parent and child; this is unambiguous and straightforward in our model where each entity has only one parent. We refer to a group of entities related by inheritance as a \emph{lineage}.
	\item A single-member group for each entity.
\end{enumerate}

In this work environmental changes may be applied to either of the first two of these three levels, with a consistent level applying throughout a run; the first level because it is the simplest application of environmental change, and the second as it represents the familiar scenario where we expect similar entities to react in similar ways to change, and where similarity is a result of descent: entities that share a common ancestor are more similar to each other than they are to other lineages. We do not address the third possibility in this work as it implies that each entity has a unique response to environmental changes. This seems problematic; environmental response is a function of phenotypes, and we would expect related entities to have related phenotypes\footnote{In general, although biological mutations of the genome can sometimes cause significant phenotypical differences between related entities.}. Thus instead of single-member groups we would expect lineage-related groups, or in other words, the second level.

The \emph{shape} of change is less constrained, and the space of all potential changes at any timestep effectively limitless: in fact, the potential change $\delta$ at timestep $t$ is $\delta_t\in R$. Simply taking a random sample from this space at each step is unlikely to result in enough resolution to test any relevant hypothesis. At the other extreme, taking only a small number of predetermined changes is likely to lead to a sampling fallacy where the choices bias the conclusions.

Instead, we need a way to parameterise the set of interesting environmental changes so we can sample from a constrained but not predetermined parameter space. The range covered by the parameter space should include both predictable and unpredictable changes as the difference between the two is core to our hypothesis.

In previous work, \textcite{Jablonka1995} and \textcite{Paenke:2007ie} share a simple model for environmental change, with abrupt switches between two defined environments, with the time between switches either being fixed or stochastic according to some probability. \Textcite{Gaucherel2012} also describes a single model for environmental change--a period of ``smooth'' change (either according to a form of $sine$ curve or unchanging) followed by an abrupt change, repeated--with variation in the length and degree of each period. By contrast, \textcite[79]{Schuster2011}, when describing the relationship between fitness landscapes and error thresholds, details five distinct models of change: ``(i) the single-peak landscape corresponding to a mean field approximation, (ii) the hyperbolic landscape, (iii) the step-linear landscape, (iv) the multiplicative landscape, and (v) the additive or linear landscape.''

Our chosen method is to represent environmental change as a parameterised time-series\footnote{A \emph{time series} is a set of observations $x_i$, each one being recorded at a specific time $t$ \textcite{Brockwell:2002dq} where an observation $x_i \in$ some set $\{X\}$, assumed to be $\mathbb{R}$.} of particular form. Statistical techniques are commonplace for time-series predictions \textcite{Brockwell:2002dq}. ARMA models are used for \emph{stationary} series, that is a time-series whose joint probability distribution (and hence whose statistical properties such as mean and variance) do not change over time, equivalent to saying the series does not demonstrate concept drift. ARIMA models apply for non-stationary series where the difference between two sequential values of the original series can be shown to produce a stationary series. The I or ``Integrated'' component of the model provides the differencing. Seasonality, a particular form of recurring concept drift, may be modelled with both ARMA and ARIMA models by incorporating a seasonality component in the model (\eg \cite{Brockwell:2002dq}.)

Although time-series modelling provides techniques for describing time-series data in terms of an underlying model, the process can also be reversed to produce a time-series from the model; in other words, if the variety of environmental change required to test our hypothesis can be described by a standard time-series model, the parameters that determine that model can also serve as our summary measure for environmental change.

Environmental change is modelled as an enhanced AR(1) or first-order autoregressive time-series, with each timestep corresponding to one evolutionary generation. Specifically, we can describe the evolutionary change at each timestep as a function of the previous timestep:

$x_t = \Theta x_{t-1} + e_t + \delta$ \label{ar-1-time-series}

where $x_t$ is the change at timestep $t$, $\Theta$ is the AR coefficient, $e_t$ is a random, normally distributed, error component around a mean of $0$, where $e_t\stackrel{iid}{\sim}N(0,\sigma^{2}_e)$, and $\delta$ is a fixed bias value.

This series allows us to represent a broad range of environmental changes:

\begin{itemize}
	\item Each time-series is completely specified by three parameters, $\Theta$, $\sigma_e$ and $\delta$.
	\item $\Theta$ in an autoregressive time-series can be interpreted as specifying stability or smoothness, while $\delta$ is a fixed change. We use $\delta$ to model a fitness trend - environments with a positive $\delta$ will see the fitness of each entity improved at each generation, with the opposite of course true of negative $\delta$. Note that with this formulation we can model linear trends in fitness from a fixed bias in the environment produced by the $\delta$ term. This is not the same as a ARI model where the environment time-series itself would show a trend.
	\item The time-series is defined by three independent elements, two predictable (driven by $\Theta$ and $\delta$) and the other ($\sigma_e$) random and unlearnable. By changing the ratio between the predictable and unpredictable we can examine the performance of the evolutionary algorithm on some continuum of predictability.
	\item An AR time-series has the property of stationarity, with the implication that the mean of the series is constant through time. However, as we apply the series values as deltas to element fitness, fitness can be non-stationary, and so may show a long term trend. This allows a simple non-differencing time-series to describe a steady improvement, or worsening, in fitness.
	\item As a corollary of stationarity, the range of the series is determined by the initial parameters. This is a useful property as it means that with appropriate parameter choices no scaling of the range is required. 
\end{itemize}

We claim that, for an environmental model defined according to \cref{ar-1-time-series}, the combination of $\Theta$, $\sigma_e$ and $\delta$ provides one measure of \emph{environmental predictability}.

<<factorialthetasigma, pdfcrop=TRUE, echo=FALSE, warning=FALSE, cache=TRUE, fig.pos='htp', fig.scap=NA, fig.cap='Visualisation of the fitness changes at each time interval (top facet) and cumulative fitness change (bottom facet) that result from our environmental model for delta=0 with two values of sigma (left and right), and three values of theta (each facet row.)'>>=
library(reshape2)
library(ggplot2)
library(cowplot) # styling of plots, extension of ggplot2
library(gridExtra)

t1 <- read.csv('results/environments-factorial.csv', header=FALSE, colClasses=c("numeric","numeric","numeric"))

t1$run <- 1:nrow(t1)
names(t1)[1:3]<-c("theta", "sigma", "delta")
t2 <- melt(t1,id=c('run','theta','sigma','delta'))
t2$theta <- round(t2$theta,3)
t2$sigma <- round(t2$sigma,3)
t2$delta <- round(t2$delta,3)

for (r in unique(t2$run)) {
t2[t2$run==r,'t'] <- 1:50 # tag with timestamp

fitness <- 0.5
for (t in 1:50) {
fitness <- max(0,min(1,fitness + t2[t2$run==r & t2$t==t,'value']))
t2[t2$run==r & t2$t==t,'fitness'] = fitness
}
}
ap <- ggplot(subset(t2,delta==0)) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_grid(theta~sigma, labeller='label_both') + labs(x="t", y="Fitness change")  + theme(strip.text = element_text(size = 8))
bp <- ggplot(subset(t2,delta==0)) + geom_line(aes(x=t,y=fitness)) + facet_grid(theta~sigma, labeller='label_both') + labs(x="t", y="Fitness change")  + theme(strip.text = element_text(size = 8))
grid.arrange(ap,bp,nrow=2,ncol=1)
@

<<factorialsigmadelta, pdfcrop=TRUE, echo=FALSE, warning=FALSE, cache=TRUE, fig.pos='htp', fig.scap=NA, fig.cap='Visualisation of the fitness changes at each time interval (top facet) and cumulative fitness change (bottom facet) for theta=0 with three values of delta (columns), and two values of sigma (each facet row.)'>>=
ap <- ggplot(subset(t2,theta==0)) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_grid(sigma~delta, labeller='label_both') + labs(x="t", y="Fitness change")  + theme(strip.text = element_text(size = 8))
bp <- ggplot(subset(t2,theta==0)) + geom_line(aes(x=t,y=fitness)) + facet_grid(sigma~delta, labeller='label_both') + labs(x="t", y="Fitness change")  + theme(strip.text = element_text(size = 8))
grid.arrange(ap,bp,nrow=2,ncol=1)
@


<<sampleenvironment, pdfcrop=TRUE, echo=FALSE, cache=TRUE, fig.pos='htp', fig.scap=NA, fig.cap='Example time-series produced by our environmental model for a sample of theta, sigma and delta values. These are incremental fitness changes, rather than cumulative ones.'>>=
t1 <- read.csv('results/environments.csv', header=FALSE, colClasses=c("numeric","numeric","numeric"))
t1$run <- 1:nrow(t1)
names(t1)[1:3]<-c("theta", "sigma", "delta")
t2 <- melt(t1,id=c('run','theta','sigma','delta'))
t2$theta <- round(t2$theta,3)
t2$sigma <- round(t2$sigma,3)
t2$delta <- round(t2$delta,3)
ggplot(t2) + geom_line(aes(x=as.numeric(variable),y=value)) + facet_wrap(~theta+sigma+delta, labeller='label_both') + labs(x="t", y="Fitness change")  + theme(strip.text = element_text(size = 8))
@


% Needs sampleenvironment
<<cumulativefitness, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.pos='htp', fig.scap=NA, fig.cap='Cumulative time-series examples for samples of theta, sigma and delta.'>>=
for (r in unique(t2$run)) {
t2[t2$run==r,'t'] <- 1:50 # tag with timestamp

fitness <- 0.5
for (t in 1:50) {
fitness <- max(0,min(1,fitness + t2[t2$run==r & t2$t==t,'value']))
t2[t2$run==r & t2$t==t,'fitness'] = fitness
}
}

ggplot(t2) + geom_line(aes(x=t,y=fitness)) + facet_wrap(~theta+sigma+delta, labeller='label_both') + labs(x="t", y="Fitness change") + theme(strip.text = element_text(size = 8))
@

\subsection{An information-based measure}\label{information-based-measure}

Predictability is related to information entropy, and to compression; a mechanism for pattern-discovery.  \textcite{Shalizi2001} identifies patterns (pattern $P$ of an object $O$) with the ability to predict (given $P$, can infer $O$) or compress (given $O$, can compress to $P$). Compression doesn't imply Prediction. There is a related concept in algorithmic complexity theory--the difference between easily solvable (P) (prediction) and easily verifiable (NP) (compression). A problem may be decidable without being easily solvable. 

In a time-independent way we can identify patterns in an image, for example, that allows us to substitute the pattern for the raw data. It is thus related to algorithms - compression is the discovery of a specific algorithm to take raw data and produce patterned data, with the goal of increasing the information content and reducing the information entropy of the patterned data. The algorithm is the patterned data. Information entropy can then, in theory at least, be measured by the Kolmogorov entropy or algorithmic complexity--the length of the algorithm. In practice various other entropy measures, such as Shannon, sample, and approximate entropy, are commonly used for time-series characterisation.

\textcite[Appendix 1]{Edmonds1999} contains a thorough review of complexity measures. More recently see \textcite{Prokopenko2009}, \textcite{Ladyman2011} and \textcite{Lloyd2001}.

Statistical Complexity \parencite{Crutchfield1989}, or $C_\mu$, based on causality, is fundamentally time-dependent: one dimension plus time, although an extension to two spatial dimensions has been done by Shalizi with a more general extension to multi-dimensions still open although a mapping method for multi-dimensional data has been proposed by \textcite{Nerukh2002}, and a dimension reduction approach by fuzzy-clustering can be found in \textcite{Young2005}.

The benefits of $C_{\mu}$ are that it matches our intuition of complexity (better than most alternatives), it has strong theoretical support, and it makes no major assumptions about the underlying data other than stationarity - it is domain agnostic. \textcite{Shalizi2001} provides the best theoretical explanation of $C_{\mu}$ and epsilon-machines; Shalizi's Ph.D. thesis \textcite{Shalizi2001a} provides further context. 

$C_{\mu}$ has been applied to a number of domains including random boolean networks \parencite{Gong2012}, spin \parencite{Vrabic2012,Shalizi2007,Nerukh2002,Feldman1998}, estimation of cortical thinning from brain MRI data \parencite{Young2008}, autonomy of protocells \parencite{Krakauer2008} and detection of anomalies (such as imminent crankshaft failure) directly from the causal states \parencite{Xiang2008}.

The canonical formulation of $C_{\mu}$ depends on two assumptions--discrete values and discrete time \parencite[p.24]{Shalizi2001}, and exact joint probabilities--which are unproblematic in our domain, and another, conditional stationarity, which poses a problem. Conditional stationarity, or time-invariant transition probabilities \parencite[p.25]{Shalizi2001} means $P(\overrightarrow{S}_t^L = s^L) = P(\overrightarrow{S}_0^L = s^L)$ for all $t \in \mathbb{Z}$ or ``the distribution of futures, conditional on histories, must be independent of when the history comes to an end'' \parencite[p.119]{Shalizi2001}. Note this issue with stationarity is similar to the difficulty identified in \cref{abrupt-environmental-change}, and a solution to one might be applicable to the other.

\section{Hypothesis and predictions}

\begin{hypothesis}
That the number of stable cycle states increases in proportion to the degree of environmental variability up until some point of maximum variability.
\end{hypothesis}

\section{Experiment design}

\begin{algorithm}[ht]
	\BlankLine
	\While{generation $\leq$ generations}{
		\BlankLine
		\tcp{Selection of reactants}
		reactants $\leftarrow$ Reactor.GetReactants(ReactantSelectionMethod)\;
		\BlankLine
		\tcp{Construction of reaction}
		reactions $\leftarrow$ Chemistry.enumerate()\;
		reaction $\leftarrow$ WeightedSelection(reactions, ProductSelectionMethod)\;
		\BlankLine
		\uIf{reaction is not Null}{
			Reactor.PerformReaction(reaction)\;
		}
		\uElse{
			No reaction possible--just bounce the molecules off each other\;
		}
	}
	\caption{The main simulation loop in ToyWorld2}
\end{algorithm}

We test two forms of environmental variability: changes to the size of the foodset, and changes to the available energy for reactions.

\subsubsection{Product selection strategy}

Incorporating an environmental factor ($X_t$) into the least energy selection mechanism from \cref{weighting-calculation} is shown below:

\begin{displaymath}
	e_i=
	\begin{cases}
		\lvert E_i\rvert, & (1) \text{  if $E_i < 0$;}         \\
		0,                & (2) \text{  if $E_{avail} < E_i - X_t$;} \\
		E_{avail}-E_i+X_t,& (3) \text{  otherwise.}            
	\end{cases}
\end{displaymath}\label{environment-weighting-calculation}

\subsubsection{Foodset size function}

\begin{algorithm}[ht]
	increment $\leftarrow$ newsize - |food set|
	\uIf{increment $> 0$} {
		\tcp{Add molecules to food set}
		samplesize $\leftarrow$ increment, bounded to $[0,|food set|]$\;
		Extend $food set$ by $samplesize$ elements chosen with uniform probability from $food set$\;
	}
	\uElse{
		\tcp{Remove molecules from food set}
		new size $\leftarrow$ newsize, bounded to $[0,|food set|]$\;
		$food set \leftarrow new size$ elements chosen with uniform probability from $food set$\;
	}
\end{algorithm}

\subsection{Measures}

The primary metric in the hypothesis is \emph{stable cycle states}. That is, cycle forms that repeat some number of times, with a degree of continuity of components from cycle to successor cycle. 

If cycles are to hold genetic information in a replicator, they must be capable of maintaining information from generation to generation. The number of alternative stable cycle forms forms an upper bound on the total information held by a variable replicator.

\begin{algorithm}[ht]
	\tcp{Construct reaction graph from a list of $reactions$}
	\For{each $reaction$ in $reactions$}{
		Add a node for the reactant side of the reaction\;
		Add a node for the product side of the reaction\;
		Add an edge from reactant node to product node\;
		\For{each $reactant$ in $reaction$}{
			Add node for $reactant$, and edge from $reactant$ to node for reactant side of reaction\;
		}
		\For{each $product$ in $reaction$}{
			Add node for $product$, and edge from $product$ to node for product side of reaction\;
		}
	}
	\tcp{Get all cycles for each reactant in the graph}
	\For{each $seed$ in $reactants$}{
		\For{each $candidate$ in list of all molecules in the reaction graph of the same molecular species}{
			\For{each $path$ in all shortest paths from $candidate$ to $seed$}{
				\uIf{the stoichiometry for $seed$ in the cycle given by this $path\geq 2$}{
					Add $path$ to the list of cycles\;
				} 
			}
		}
	}
	\Return $cycles$
	\caption{Identifying all autocatalytic cycles}
\end{algorithm}

\begin{algorithm}[ht]
	\tcp{Identify stable cycles from a list of $cycles$}
	\tcp{Stable cycles are those where there are a chain of two or more cycles, that are linked by a product in one cycle being a reactant in another and where the cycles have the same "form", or sequence of molecule species (smiles) in the cycle}
	\BlankLine
	Sort $cycles$ into groups of equal length\;
	Group all cycles of same length into subgroups of the same form\;
	\BlankLine
	\tcp{Now attempt to connect cycles of the same form through shared molecules into one or more clusters of connected cycles}
	\For{each cycle form}{
		$clusters\leftarrow$ set of molecules in the first cycle of this form\;
		\For{all other $cycle$s of this form}{
			\For{each $cluster$ in $clusters$}{
				\uIf{$cycle$ shares molecules with $cluster$}{
					Add the molecules in this $cycle$ to $cluster$\;
				}
			}
		}
		\uIf{more than one cluster discovered}{
			$StableCycles[$ form of cycle $]\leftarrow$ size of longest cluster discovered\;
		}
	}
	\Return $StableCycles$
	\caption{Stable hypercycle state identification in ToyWorld2}
\end{algorithm}

\subsection{Local Reactant Selection}

Unless the total number of atoms in the reaction vessel can increase, to maintain a foodset level product molecules must be removed from the reactor at an equal rate. If there is a flow across the reaction vessel with new molecules (food set molecules) entering at one side and products be removed at the opposite side, then there is a maximum lifespan for any composome determined by how long it takes to traverse the vessel.

Anchoring the composome would allow this limit to be extended. Some form of membrane or boundary (e.g., Chemoton or autopoeitic entity), open to food set molecules entering across the boundary, and waste product molecules exiting would identify the reactions in the composome so they could be logically anchored.

Alternatively, the composome could be anchored by fixing product molecules in place while foodset molecules move freely in the vessel. This still leaves the problem of removing 'waste' (or else the vessel will be filled by product molecules). The LocalReactantSelection method models these fixed products.

However to mimic the effects of a membrane, all non-foodset reactants must be within a given distance of each other. We do this by first selecting two molecules from the combined foodset and reaction vessel population. For two molecules to react, if both are non-foodset molecules they must be physically co-located within the reaction vessel. Therefore if both of these chosen reactants is a non-foodset molecule, we must replace the second molecule by either a molecule from the same location, or by a foodset molecule. 

\begin{algorithm}[ht]

     $reactants\leftarrow$ two molecules taken with uniform probability from the combined foodset and reaction vessel population\;
	\BlankLine
     \uIf{both reactants are not in the food set}{
		$n\leftarrow |\text{food set}| / (|\text{food set}| + |\text{population}|)$\;
		Choose some $p \in \mathcal{U}[0,1]$\;
           \uIf{$p\leq n$}{
                sample population $\leftarrow$ food set\;
           }
		\uElse{
			sample population $\leftarrow$ all molecules within a given distance of the first reactant in the reaction vessel\;
			\uIf{the sample population is empty}{
				\tcp{Nothing in the same area...}
				sample population $\leftarrow$ food set\;
			}
		}
		Let the second reactant be a molecule chosen (with uniform probability) from the sample population\;
	}
	\Return reactants
	\caption{Local Reactant Selection}
\end{algorithm}

The foodset at the start of each experiment consisted of 10 x [H][H] molecules, 10 x FO, 20 x O, 10 x [O-][N+](=O)[N+]([O-])=O, 10 x N(=O)[O], and 20 x O=C=O.

\begin{table}
	\scriptsize
	\begin{center}
	\caption{Experiment runs}\label{tbl:toyworld2-experiments}
	\begin{tabular}{@{}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}@{}}
		\toprule
		Experiment                 &Form of variation		&Target of variation 	& No. environments	& No. repeats\\
		\midrule
		1480963448 & AR & Foodset & 50 & 1\\
		1481665906 & AR & N/A & 1 & 10\\
		1481670569 & AR & Energy & 10 & 1\\
		1481939843/0 & AR & Foodset&5 & 3\\
		1481939843/1 & AR & Energy &5 & 3\\
		1481952255 & Bistate & Energy & 5 & 2\\
		1481398302 & AR & Foodset &20 & 1\\
		\bottomrule
	\end{tabular}
	\end{center}
\end{table}

\section{Results}

<<summarytable, results='asis', echo=FALSE, cache=TRUE, warning=FALSE>>=
df <- read.csv('results/stablestates.csv')

df_len <- aggregate(min~datetime, data=df, FUN=length)
df_max <- aggregate(max~datetime, data=df, FUN=max)
df_min <- aggregate(min~datetime, data=df, FUN=min)
df_mean <- aggregate(mean~datetime, data=df, FUN=mean)
summary <- merge(merge(merge(df_len, df_min, by="datetime"),df_mean, by="datetime"), df_max, by="datetime")
names(summary)<-c("Experiment", "Total runs", "Min. States", "Mean States", "Max. States")

library(xtable)
print(xtable(summary, caption='Summary of experiment results', label='tbl:toyworld2-results-summary'), booktabs=TRUE, include.rownames=FALSE, size="scriptsize", caption.placement="top")
@

<<anova, echo=FALSE, cache=TRUE, warning=FALSE>>=
df_energy <- df[df$target=='ENERGY',]
df_food <- df[df$target=='FOOD',]


av_overall <- anova(lm(df$mean~df$dfa))
av_energy <- anova(lm(df_energy$mean~df_energy$dfa))
av_food <- anova(lm(df_food$mean~df_food$dfa))
pr_av_overall <- av_overall$"Pr(>F)"[1]
pr_av_energy <- av_energy$"Pr(>F)"[1]
pr_av_food <- av_food$"Pr(>F)"[1]
@

<<toyworld2-results, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.pos='htp', fig.scap=NA, fig.cap='Mean number of stable states for each environment against variability measured by the Hurst exponent (left panel) and for stable (Hurst=0) and varying (Hurst>0) environments (right).'>>=
df$state="Varying"
df$state[df$dfa==0] = "Stable"
df$state[df$datetime==1481952255] = "Bistate"
ap <- ggplot(df) + geom_point(aes(x=dfa, y=mean, color=state)) + labs(x='Hurst exponent', y='Mean number of stable states', color='Environment') + scale_colour_manual(name="",  values=c("grey80", "grey50", "black"))
bp <-ggplot(df) + geom_boxplot(aes(x=state, y=mean, group=state)) + labs(x='Environment state', y='Mean number of stable states')
grid.arrange(ap,bp,nrow=2,ncol=1)
@

<<toyworld2-results-by-target, pdfcrop=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.height=3.5, fig.pos='htp', fig.scap=NA, fig.cap='Mean number of stable states for each environment for variation applied to the food set population (left) and to the product selection strategy (right).'>>=
ggplot(df) + geom_boxplot(aes(x=target, y=mean, group=target)) + labs(x='Target of variation', y='Mean number of stable states') + scale_x_discrete(labels=c("Product selection strategy", "Food set"))
@

% States are by seed molecule

\section{Discussion}

\section{Conclusions}




